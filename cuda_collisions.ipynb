{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"3\"   # turn all kernels synchronous\n",
    "torch.cuda.empty_cache()                  # start with clean slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patching the snippet loop to use torch-based GPU helpers from collision_cuda.py\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "import importlib\n",
    "import pandas as pd\n",
    "\n",
    "import collision_cuda_clean\n",
    "importlib.reload(collision_cuda_clean)\n",
    "import collision_utils_new\n",
    "importlib.reload(collision_utils_new)\n",
    "\n",
    "from collision_cuda_clean import (\n",
    "    quick_unit_filter_gpu, scan_unit_lags_t, score_active_set_t,\n",
    "    build_rolled_bank\n",
    ")\n",
    "\n",
    "import collision_utils\n",
    "importlib.reload(collision_utils)\n",
    "\n",
    "from collision_utils import (\n",
    "    accumulate_unit_stats, accept_units,\n",
    "    micro_align_units, build_channel_index,\n",
    ")\n",
    "\n",
    "from collision_utils_new import resolve_snippet_t\n",
    "\n",
    "# ---------------- user knobs ----------------\n",
    "AMP_THR   = 25.0\n",
    "MASK_THR  = 5.0        # ignored; kept for API parity\n",
    "MAX_LAG   = 20\n",
    "BETA      = 0.5\n",
    "SNIP_LEN  = 121\n",
    "OVERLAP   = 80\n",
    "STEP      = SNIP_LEN - OVERLAP\n",
    "DISCOVERY_THR = 200.0\n",
    "MAX_SAMPLES   = 200_000\n",
    "USE_CUDA  = True\n",
    "\n",
    "# ---------- one-time GPU preparation ----------\n",
    "unit_ids  = sorted(unit_info.keys())\n",
    "uid2row   = {u:i for i,u in enumerate(unit_ids)}\n",
    "row2uid   = {i:u for u,i in uid2row.items()}\n",
    "\n",
    "raw_chunk = raw_data[:MAX_SAMPLES, :512].astype(np.float32).T\n",
    "if USE_CUDA:\n",
    "    raw_chunk_t = torch.from_numpy(raw_chunk).to('cuda', torch.float32)\n",
    "    EIs_t  = torch.from_numpy(\n",
    "                np.stack([unit_info[u]['ei'] for u in unit_ids])\n",
    "             ).to('cuda')\n",
    "    rolled_t = build_rolled_bank(EIs_t)               # [U,41,C,T]\n",
    "    peak_ch_t = torch.tensor(\n",
    "                [np.argmax(unit_info[u]['ei'].ptp(axis=1)) for u in unit_ids],\n",
    "                device='cuda', dtype=torch.int64)\n",
    "\n",
    "sel_mask = np.zeros((len(unit_ids), 512), bool)\n",
    "for i,u in enumerate(unit_ids):\n",
    "    sel_mask[i, unit_info[u]['selected_channels']] = True\n",
    "sel_mask_t = torch.from_numpy(sel_mask).to('cuda')\n",
    "\n",
    "p2p_all = {u: unit_info[u]['ei'].ptp(axis=1) for u in unit_info}\n",
    "\n",
    "prev_tail_t = None \n",
    "prev_tail = None \n",
    "lag_history_t = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- main snippet loop --------\n",
    "start = 1845\n",
    "while start + SNIP_LEN <= 1845+121:\n",
    "    print(f\"\\n--- snippet @ {start} ---\")\n",
    "\n",
    "\n",
    "    unit_log     = defaultdict(lambda: {'deltas': [], 'lags': []})\n",
    "    \n",
    "    raw_snip   = raw_chunk[:, start:start+SNIP_LEN]\n",
    "    raw_snip_t = raw_chunk_t[:, start:start+SNIP_LEN] if USE_CUDA else None\n",
    "\n",
    "    if prev_tail is not None:\n",
    "        raw_snip[:, :tail_len] -= prev_tail              # CPU\n",
    "        if USE_CUDA:\n",
    "            raw_snip_t[:, :tail_len] -= prev_tail_t      # CUDA\n",
    "\n",
    "    # 1) quick filter\n",
    "    if USE_CUDA:\n",
    "        good_rows, best_lags = quick_unit_filter_gpu(\n",
    "                raw_snip_t,          # [C,T] cuda\n",
    "                EIs_t,               # [U,C,T]\n",
    "                peak_ch_t,           # [U]\n",
    "                sel_mask_t)          # build once:  bool[U,C] True on selected channels\n",
    "\n",
    "        good_units = [row2uid[r] for r in good_rows]\n",
    "        lag_dict   = {row2uid[r]: [int(l.item()) for l in best_lags[r]]\n",
    "                    for r in good_rows}\n",
    "\n",
    "\n",
    "        # print(good_units)\n",
    "    else:\n",
    "        raise NotImplementedError(\"CPU path not shown\")\n",
    "\n",
    "    if not good_units:\n",
    "        print(\"no units â†’ skip\")\n",
    "        prev_tail_t = None \n",
    "        prev_tail = None \n",
    "        start += STEP\n",
    "        continue\n",
    "\n",
    "    print(f\"Considering {len(good_units)} units\")\n",
    "\n",
    "    # 2) lag scan\n",
    "    # lags = scan_unit_lags_t(raw_snip_t, rolled_t)          # [U,3]\n",
    "    # lag_dict = {u: lags[uid2row[u]].tolist() for u in good_units}\n",
    "    # print(lag_dict)\n",
    "\n",
    "    # 3) resolve snippet (uses CUDA scorer internally)\n",
    "    best_combo, per_unit_delta, _ = resolve_snippet_t(\n",
    "        raw_snippet=raw_snip,\n",
    "        good_units=good_units,\n",
    "        channel_to_units=build_channel_index(good_units, unit_info),\n",
    "        lag_dict=lag_dict,\n",
    "        unit_info=unit_info,\n",
    "        p2p_all=p2p_all,\n",
    "        amp_thr=AMP_THR, beta=BETA,\n",
    "        use_cuda=USE_CUDA,\n",
    "        rolled_t=rolled_t,\n",
    "        raw_snip_t=raw_snip_t,\n",
    "        uid2row=uid2row\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # 5) update rolling evidence\n",
    "    for uid, d in per_unit_delta.items():\n",
    "        unit_log[uid]['deltas'].append(d)\n",
    "        unit_log[uid]['lags'].append(best_combo['lags'].get(uid, np.nan))\n",
    "\n",
    "    # 6) global decision + micro-alignment\n",
    "    stats = accumulate_unit_stats(unit_log)\n",
    "\n",
    "    accepted, rejected = accept_units(stats)\n",
    "    \n",
    "    final_lags, final_deltas = micro_align_units(accepted, stats, unit_info, raw_snip, p2p_all)\n",
    "\n",
    "\n",
    "    print(\"final lags:\", best_combo[\"lags\"])\n",
    "\n",
    "    # ----------- build combined_final (still on CPU) -----------------\n",
    "    combined_final = np.zeros_like(raw_snip, dtype=np.float32)\n",
    "    for uid, lag in final_lags.items():\n",
    "        combined_final += np.roll(unit_info[uid]['ei'], lag, axis=1)\n",
    "\n",
    "    # ----------- store the tail for the NEXT iteration ---------------\n",
    "    tail_start = STEP             # 41\n",
    "    tail_len   = OVERLAP          # 80\n",
    "    prev_tail  = combined_final[:, tail_start:].copy()      # (C,80)\n",
    "    if USE_CUDA:\n",
    "        prev_tail_t = torch.from_numpy(prev_tail).to('cuda')\n",
    "\n",
    "    adjusted_lags = {\n",
    "        uid: start + (SNIP_LEN // 2) - MAX_LAG + lag\n",
    "        for uid, lag in final_lags.items()\n",
    "    }\n",
    "    lag_history_t.append(adjusted_lags)   \n",
    "    \n",
    "    start += STEP\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
