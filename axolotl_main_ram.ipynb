{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "from compare_eis import compare_eis\n",
    "\n",
    "\n",
    "# --- Path and recording setup ---\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat\"\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Get total number of samples ---\n",
    "file_size_bytes = os.path.getsize(dat_path)\n",
    "total_samples = file_size_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "\n",
    "# --- Load entire file into RAM as int16 ---\n",
    "raw_data = np.fromfile(dat_path, dtype=dtype, count=total_samples * n_channels)\n",
    "raw_data = raw_data.reshape((total_samples, n_channels))  # shape: [T, C]\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 1500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "do_pursuit = 0\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0528.h5' # where to save data\n",
    "\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug\"\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "    ks_vision_ids = f['/vision_ids'][:]  # shape: (N_units,)\n",
    "\n",
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "save_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_and_artifacts.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(save_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines = axolotl_utils_ram.compute_baselines_int16(raw_data, segment_len=100_000) # shape (512, 360)\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "        }, f)\n",
    "\n",
    "\n",
    "# get KS EIs\n",
    "ks_ei_path = '/Volumes/Lab/Users/alexth/axolotl/ks_eis_subset.h5'\n",
    "ks_templates = {}\n",
    "ks_n_spikes = {}\n",
    "\n",
    "with h5py.File(ks_ei_path, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        unit_id = int(k.split('_')[1])-1\n",
    "        ks_templates[unit_id] = f[k][:]\n",
    "        ks_n_spikes[unit_id] = f[k].attrs.get('n_spikes', -1)  # fallback if missing\n",
    "\n",
    "ks_unit_ids = list(ks_templates.keys())\n",
    "ks_ei_stack = np.stack([ks_templates[k] for k in ks_unit_ids], axis=0)  # [N x 512 x 81]\n",
    "\n",
    "unit_id = 0\n",
    "\n",
    "print(f\"\\n=== Starting unit {unit_id} ===\")\n",
    "\n",
    "while True:\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # could cache scores on channels to pre-identify next one\n",
    "    ref_channel = axolotl_utils_ram.find_dominant_channel_ram(\n",
    "            raw_data = raw_data,\n",
    "            segment_len = 100_000,\n",
    "            n_segments = 10,\n",
    "            peak_window = 30,\n",
    "            top_k_neg = 20,\n",
    "            top_k_events = 5,\n",
    "            seed = 42\n",
    "        )\n",
    "\n",
    "    threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "        raw_data=raw_data,\n",
    "        ref_channel=ref_channel,\n",
    "        window = 30,\n",
    "        total_samples_to_read = total_samples,\n",
    "        refractory = 30,\n",
    "        top_n = 100\n",
    "    )\n",
    "\n",
    "    print(f\"Channel: {ref_channel}, Threshold: {-threshold:.1f}, Initial spikes: {len(spike_times)}\")\n",
    "\n",
    "    snips, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spike_times,\n",
    "        window=window,\n",
    "        selected_channels=np.arange(n_channels)\n",
    "    )\n",
    "\n",
    "    ei = np.mean(snips, axis=2)\n",
    "    ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "    spikes_for_plot_pre = valid_spike_times\n",
    "\n",
    "    # Step 6–7: Cluster and select dominant unit\n",
    "    clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips, ei, k_start=3,return_debug=True)\n",
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.66× of mean waveform peak\n",
    "    threshold_ampl = 0.66 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "        dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        ref_channel=ref_channel,\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype='int16'\n",
    "    )\n",
    "\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    if unit_id >= max_units:\n",
    "        print(\"Reached unit limit.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25, 4))\n",
    "plt.plot(raw_data[:5000, 39])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Channel 39: First 5,000 samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parameters ---\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat'  # ← replace with actual path\n",
    "n_channels = 512\n",
    "channel = 39\n",
    "n_samples = 5000\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Read from disk ---\n",
    "with open(dat_path, 'rb') as f:\n",
    "    # Read first 5000 timepoints (i.e., 5000 * n_channels values)\n",
    "    raw = np.fromfile(f, dtype=dtype, count=n_samples * n_channels)\n",
    "    raw = raw.reshape((n_samples, n_channels))  # [time, channel]\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(25, 4))\n",
    "plt.plot(raw[:5000, 39])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Channel 39: First 5,000 samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "from compare_eis import compare_eis\n",
    "\n",
    "\n",
    "# --- Path and recording setup ---\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat\"\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Get total number of samples ---\n",
    "file_size_bytes = os.path.getsize(dat_path)\n",
    "total_samples = file_size_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "\n",
    "# --- Load entire file into RAM as int16 ---\n",
    "raw_data = np.fromfile(dat_path, dtype=dtype, count=total_samples * n_channels)\n",
    "raw_data = raw_data.reshape((total_samples, n_channels))  # shape: [T, C]\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 1500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "do_pursuit = 0\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0528.h5' # where to save data\n",
    "\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug/0607\"\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "    ks_vision_ids = f['/vision_ids'][:]  # shape: (N_units,)\n",
    "\n",
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "save_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_derivative.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(save_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines = axolotl_utils_ram.compute_baselines_int16_deriv_robust(raw_data, segment_len=100_000, diff_thresh=10, trim_fraction=0.15) # shape (512, 360)\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "        }, f)\n",
    "\n",
    "\n",
    "# get KS EIs\n",
    "ks_ei_path = '/Volumes/Lab/Users/alexth/axolotl/ks_eis_subset.h5'\n",
    "ks_templates = {}\n",
    "ks_n_spikes = {}\n",
    "\n",
    "with h5py.File(ks_ei_path, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        unit_id = int(k.split('_')[1])-1\n",
    "        ks_templates[unit_id] = f[k][:]\n",
    "        ks_n_spikes[unit_id] = f[k].attrs.get('n_spikes', -1)  # fallback if missing\n",
    "\n",
    "ks_unit_ids = list(ks_templates.keys())\n",
    "ks_ei_stack = np.stack([ks_templates[k] for k in ks_unit_ids], axis=0)  # [N x 512 x 81]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id = 0\n",
    "print(unit_id)\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug/0615\"\n",
    "\n",
    "import sys\n",
    "\n",
    "class Tee:\n",
    "    def __init__(self, *files):\n",
    "        self.files = files\n",
    "    def write(self, obj):\n",
    "        for f in self.files:\n",
    "            f.write(obj)\n",
    "    def flush(self):\n",
    "        for f in self.files:\n",
    "            f.flush()\n",
    "\n",
    "# Create the log file\n",
    "log_file = open(\"/Volumes/Lab/Users/alexth/axolotl/debug/0615/processing_log.txt\", \"w\")\n",
    "\n",
    "# Redirect stdout to both notebook and file\n",
    "sys.stdout = Tee(sys.__stdout__, log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "from diptest import diptest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# unit_id = 1004\n",
    "\n",
    "\n",
    "last_ref_channel = None\n",
    "remaining_candidates = []\n",
    "\n",
    "do_second_clustering = False\n",
    "\n",
    "# ================================================================\n",
    "# ---- 0.  One-time initialisation\n",
    "# ================================================================\n",
    "if 'rejected_spike_log' not in globals():\n",
    "    rejected_spike_log = {\n",
    "        'times'   : [],   # absolute sample indices\n",
    "        'channel' : [],   # ref_channel for each rejected snip\n",
    "        'unit_id' : []    # which unit’s refinement produced the reject\n",
    "    }\n",
    "\n",
    "# ref_channel = 51\n",
    "\n",
    "# if 1:\n",
    "while True:\n",
    "\n",
    "    print(f\"\\n=== Starting unit {unit_id} ===\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not remaining_candidates:\n",
    "        seed=int(time.time())\n",
    "\n",
    "        remaining_candidates,top_amplitudes = axolotl_utils_ram.find_dominant_channel_ram(\n",
    "            raw_data=raw_data,\n",
    "            positions=ei_positions,\n",
    "            segment_len=200_000, # at 20k samples per s, this is 10s snippet, and we have 10 of them\n",
    "            n_segments=20,\n",
    "            peak_window=30,\n",
    "            top_k_neg=40,   # number of negative peaks to keep per channel per segment\n",
    "            top_k_events=100, # number of top amplitudes to average per channel\n",
    "            seed=seed,\n",
    "            use_negative_peak=True,\n",
    "            top_n = 10,\n",
    "            min_spacing = 150\n",
    "        )\n",
    "        channel_source='fresh'\n",
    "        last_ref_channel = None\n",
    "        formatted_amps = \", \".join(f\"{amp:.1f}\" for amp in top_amplitudes)\n",
    "        print(f\"New amplitudes: {formatted_amps}, on channels {remaining_candidates}\")\n",
    "\n",
    "        \n",
    "\n",
    "    candidate = remaining_candidates.pop(0)\n",
    "    ref_channel = candidate\n",
    "\n",
    "    if last_ref_channel is not None: # From cache\n",
    "        channel_source='cache'\n",
    "\n",
    "    # Update tracker\n",
    "    # ref_channel = 29\n",
    "    # channel_source = 'fresh'\n",
    "    last_ref_channel = ref_channel\n",
    "\n",
    "\n",
    "\n",
    "    threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "        raw_data=raw_data,\n",
    "        ref_channel=ref_channel,\n",
    "        window = 30,\n",
    "        total_samples_to_read = total_samples,\n",
    "        refractory = 10,\n",
    "        top_n = 100,\n",
    "        threshold_scale = 0.5\n",
    "    )\n",
    "\n",
    "    elapsed_thresholding = time.time() - start_time \n",
    "    print(f\"Threshold: {elapsed_thresholding:.1f} s\")\n",
    "\n",
    "    # spike_times = unmatched_spikes_2\n",
    "\n",
    "    print(f\"Channel: {ref_channel} (from {channel_source}), Threshold: {-threshold:.1f}, Initial spikes: {len(spike_times)}\")\n",
    "\n",
    "\n",
    "    snips, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spike_times,\n",
    "        window=window,\n",
    "        selected_channels=np.arange(n_channels)\n",
    "    )\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = valid_spike_times // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    ei = np.mean(snips_baselined, axis=2)\n",
    "    #ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "    spikes_for_plot_pre = valid_spike_times\n",
    "\n",
    "    # Step 6–7: Cluster and select dominant unit\n",
    "    k_start = min(8, 3 + (len(spike_times) - 1) // 3000)\n",
    "\n",
    "    clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, n_bad_channels_pre, cluster_eis_pre, cluster_to_merged_group_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips_baselined, ei, k_start=k_start,return_debug=True)\n",
    "\n",
    "\n",
    "    elapsed_clustering = time.time() - start_time \n",
    "    print(f\"Clustering: {elapsed_clustering-elapsed_thresholding:.1f} s, with k={k_start}\")\n",
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    contributing_original_ids_pre = [\n",
    "        orig_id for orig_id, merged_idx in cluster_to_merged_group_pre.items()\n",
    "        if merged_idx == selected_cluster_index_pre\n",
    "    ]\n",
    "    contributing_original_ids_pre = np.array(contributing_original_ids_pre)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    elapsed_lags = time.time() - start_time \n",
    "    print(f\"Lags: {elapsed_lags-elapsed_clustering:.1f} s\")\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    elapsed_snippet_extraction = time.time() - start_time \n",
    "    print(f\"Snippet: {elapsed_snippet_extraction-elapsed_lags:.1f} s\")\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 0.  Gather ref-channel waveforms (baseline-subtracted)\n",
    "    # --------------------------------------------------------------------\n",
    "    ref_snips     = snips_baselined[ref_channel, :, :].copy()\n",
    "    ref_snips     = ref_snips.T          # [N × 81]\n",
    "    wave_window   = slice(15, 40)                                 # focus on peak\n",
    "    waveforms     = ref_snips[:, wave_window]                     # [N × 25]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 1.  PCA(2)  (no z-score yet)\n",
    "    # --------------------------------------------------------------------\n",
    "    pcs_raw = PCA(n_components=2, svd_solver='full').fit_transform(waveforms)  # [N × 2]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 2.  Hartigan’s dip test on rotated projections\n",
    "    # --------------------------------------------------------------------\n",
    "    angle_step = 10\n",
    "    angles = np.deg2rad(np.arange(0, 180, angle_step))\n",
    "\n",
    "    best_p = 1.0\n",
    "    best_proj = None\n",
    "    for theta in angles:\n",
    "        proj = pcs_raw[:, 0] * np.cos(theta) + pcs_raw[:, 1] * np.sin(theta)\n",
    "        _, p = diptest(proj)\n",
    "        if p < best_p:\n",
    "            best_p, best_proj = p, proj\n",
    "\n",
    "    discard_inds_bimodal = np.empty(0, dtype=int)\n",
    "\n",
    "    if best_p < 0.05:                                   # suspected bimodality\n",
    "        # histogram → first two peaks as initial centroids\n",
    "        hist, bin_edges = np.histogram(best_proj, bins=30)\n",
    "        peaks, _ = find_peaks(hist)\n",
    "        if len(peaks) >= 2:\n",
    "            centroids = bin_edges[peaks[:2]].reshape(-1, 1)\n",
    "            km = KMeans(n_clusters=2, init=centroids, n_init=1, random_state=42)\n",
    "            labels = km.fit_predict(best_proj[:, None])\n",
    "\n",
    "            # keep larger lobe, discard smaller one\n",
    "            counts = np.bincount(labels)\n",
    "            keep_label = counts.argmax()\n",
    "            discard_inds_bimodal = np.where(labels != keep_label)[0]\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 3.  Re-project kept spikes with a fresh PCA if split clusters\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    if discard_inds_bimodal.size:\n",
    "        keep_mask  = np.ones(len(pcs_raw), dtype=bool)\n",
    "        keep_mask[discard_inds_bimodal] = False\n",
    "        # Apply to real data and snips_baselined\n",
    "        snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "        snips_full = snips_full[:, :, keep_mask]\n",
    "        valid_spike_times = valid_spike_times[keep_mask]\n",
    "        spikes = spikes[keep_mask]\n",
    "        waveforms     = waveforms[keep_mask, :]\n",
    "        pcs_raw   = PCA(n_components=2, svd_solver='full').fit_transform(waveforms)\n",
    "        print(f\"   Split the final cluster according to Hartigan test! Started with {len(keep_mask)} spikes, ended with {len(spikes)}\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 4.  Scale, Mahalanobis\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    pcs_z      = StandardScaler().fit_transform(pcs_raw)\n",
    "    pcs = pcs_z\n",
    "    \n",
    "    d2 = np.sum(pcs_z**2, axis=1)\n",
    "\n",
    "    from scipy.stats import chi2\n",
    "    thr_vis  = chi2.ppf(0.999,  df=2)     # illustration\n",
    "    thr_cut  = chi2.ppf(0.9999, df=2)     # discard\n",
    "\n",
    "    final_outlier_inds      = np.where(d2 > thr_vis)[0]\n",
    "    final_outlier_inds_max  = np.where(d2 > thr_cut)[0]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 5.  Local Outlier Factor\n",
    "    # --------------------------------------------------------------------\n",
    "        # ----------------- build robust inlier core -------------------------\n",
    "    core_mask = d2 < chi2.ppf(0.999, df=2)       # Mahalanobis core (~0.1 % trimmed)\n",
    "\n",
    "    # ----------------- semi-supervised LOF on the core ------------------\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, novelty=True)\n",
    "    lof.fit(pcs[core_mask])                    # train ONLY on good spikes\n",
    "    lof_pred   = lof.predict(pcs)              # −1 = outlier w.r.t. core\n",
    "    lof_scores = lof.negative_outlier_factor_      # more negative = more outlying\n",
    "    lof_inds   = np.where(lof_pred == -1)[0]\n",
    "\n",
    "    final_outlier_inds_max  = np.union1d(final_outlier_inds_max, lof_inds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 1.  Build mean waveform from *accepted* spikes on ref channel\n",
    "    # ------------------------------------------------------------------\n",
    "    pre, post        = window                     # snippet definition\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "    accepted_mask    = np.ones(n_spikes, dtype=bool)\n",
    "    accepted_mask[final_outlier_inds_max] = False\n",
    "    accepted_inds    = np.where(accepted_mask)[0]\n",
    "\n",
    "\n",
    "    if accepted_inds.size:                         # normal case\n",
    "        mean_ref = snips_baselined[ref_channel, :, accepted_inds].copy()\n",
    "    else:                                          # degenerate (all rejected)\n",
    "        mean_ref = snips_baselined[ref_channel, :, :].copy()\n",
    "\n",
    "    mean_ref = mean_ref.T\n",
    "    mean_ref = np.mean(mean_ref, axis=1)\n",
    "    mean_ref = mean_ref-mean_ref[0]\n",
    "\n",
    "    abs_ref = np.abs(mean_ref)\n",
    "\n",
    "    # --- thresholds in μV ------------------------------------------------\n",
    "    max_abs = abs_ref.max()\n",
    "    low_thr = 0.01 * max_abs\n",
    "    high_thr = 0.1 * max_abs\n",
    "\n",
    "    # first index where |WF| > 5 µV  (fall back to 0)\n",
    "    try:\n",
    "        idx_start = int(np.where(abs_ref > low_thr)[0][0])\n",
    "    except IndexError:\n",
    "        idx_start = 0\n",
    "\n",
    "    # last index where |WF| > 20 µV  (fall back to end)\n",
    "    cands = np.where(abs_ref > high_thr)[0]\n",
    "    idx_end = int(cands[-1]) if cands.size else snippet_len - 1\n",
    "\n",
    "    print(f\"Template masking for reject spikes: {idx_start} to {idx_end}\")\n",
    "\n",
    "\n",
    "    # convert waveform indices → sample offsets relative to spike centre\n",
    "    # global_sample = spike_time + (wave_idx + pre)\n",
    "    offset_start = idx_start + pre            # pre negative\n",
    "    offset_end   = idx_end   + pre            # inclusive\n",
    "    if offset_start > offset_end:             # safety\n",
    "        offset_start, offset_end = pre, post\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 2.  Stash rejects & apply baseline on ref channel\n",
    "    # ------------------------------------------------------------------\n",
    "    segment_len = 100_000                      # already used elsewhere\n",
    "\n",
    "    reject_times = spikes[final_outlier_inds_max]\n",
    "    n_rej        = len(reject_times)\n",
    "\n",
    "    for t in reject_times:\n",
    "        # ---- (a)  log entry ------------------------------------------\n",
    "        rejected_spike_log['times']  .append(int(t))\n",
    "        rejected_spike_log['channel'].append(ref_channel)\n",
    "        rejected_spike_log['unit_id'].append(unit_id)\n",
    "\n",
    "        # # ---- (b)  baseline value for this spike ----------------------\n",
    "        # seg_id        = int(t // segment_len)\n",
    "        # baseline_val  = baselines[ref_channel, seg_id]\n",
    "\n",
    "        # # ---- (c)  overwrite ref-channel samples with baseline --------\n",
    "        # t0 = int(t + offset_start)\n",
    "        # t1 = int(t + offset_end + 1)           # slice end is non-inclusive\n",
    "        # # bounds check\n",
    "        # if t0 < 0:            t0 = 0\n",
    "        # if t1 > raw_data.shape[0]:\n",
    "        #     t1 = raw_data.shape[0]\n",
    "        # raw_data[t0:t1, ref_channel] = baseline_val\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 2.  Mahalanobis distance  (robust covariance)\n",
    "    # --------------------------------------------------------------------\n",
    "    # from sklearn.covariance import MinCovDet\n",
    "    # from scipy.stats import chi2\n",
    "\n",
    "    # mcd      = MinCovDet().fit(pcs_z)\n",
    "    # d2       = mcd.mahalanobis(pcs_z)                           # squared distance\n",
    "    # df       = pcs_z.shape[1]                                   # degrees of freedom\n",
    "    # thr_maha = chi2.ppf(0.999999, df=df)    # Illustration only\n",
    "    # final_outlier_inds = np.where(d2 > thr_maha)[0]\n",
    "\n",
    "    # thr_maha = chi2.ppf(0.9999999, df=df)    # real discards: 0.1 % most extreme  → tweak if desired\n",
    "    # final_outlier_inds_max = np.where(d2 > thr_maha)[0]\n",
    "\n",
    "    # print(f\"Maha rejects: {len(maha_inds)}\")\n",
    "\n",
    "    # # --------------------------------------------------------------------\n",
    "    # # 3.  Local Outlier Factor\n",
    "    # # --------------------------------------------------------------------\n",
    "    #     # ----------------- build robust inlier core -------------------------\n",
    "    # core_mask = d2 < chi2.ppf(0.999999, df=df)       # Mahalanobis core (~0.1 % trimmed)\n",
    "\n",
    "    # # ----------------- semi-supervised LOF on the core ------------------\n",
    "    # from sklearn.neighbors import LocalOutlierFactor\n",
    "    # lof = LocalOutlierFactor(n_neighbors=len(maha_inds), novelty=True)\n",
    "    # lof.fit(pcs_all[core_mask])                    # train ONLY on good spikes\n",
    "    # lof_pred   = lof.predict(pcs_all)              # −1 = outlier w.r.t. core\n",
    "    # lof_scores = lof.negative_outlier_factor_      # more negative = more outlying\n",
    "    # lof_inds   = np.where(lof_pred == -1)[0]\n",
    "\n",
    "\n",
    "    # print(f\"LOF rejects: {len(lof_inds)}\")\n",
    "    \n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 4.  Assemble the two masks you asked for\n",
    "    # --------------------------------------------------------------------\n",
    "    # (A) illustration set – only one metric (Mahalanobis here)\n",
    "    # final_outlier_inds      = maha_inds\n",
    "\n",
    "    # (B) aggressive discard – any metric\n",
    "    # final_outlier_inds_max  = np.union1d(maha_inds, lof_inds)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 5.  Optional: visualisation helpers\n",
    "    # --------------------------------------------------------------------\n",
    "    # pcs_plot = np.column_stack((pc1_z, pc2_z))          # for scatter plots\n",
    "    # Now you can colour by           : \n",
    "    #    * black  = inliers\n",
    "    #    * red    = final_outlier_inds\n",
    "    #    * green  = extra LOF-only outliers (final_outlier_inds_max \\ final_outlier_inds)\n",
    "\n",
    "\n",
    "    bad_inds = final_outlier_inds_max #outlier_inds\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spikes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces_easy = snips_baselined[ref_channel, :, final_outlier_inds]  # shape: (n_bad, T)\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    if do_second_clustering:\n",
    "        # Get original traces for bad_spike_traces\n",
    "        snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "            dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "            spike_times=spikes[bad_inds],\n",
    "            ref_channel=ref_channel,\n",
    "            window=window,\n",
    "            n_channels=512,\n",
    "            dtype='int16'\n",
    "        )\n",
    "\n",
    "        segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "        snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "    final_spike_inds = np.where(keep_mask)[0]\n",
    "\n",
    "\n",
    "    elapsed_bad = time.time() - start_time \n",
    "    print(f\"Distortion handling: {elapsed_bad - elapsed_snippet_extraction:.1f} s, stashed {len(bad_inds)} spikes\")\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "    if do_second_clustering:\n",
    "\n",
    "        if len(spikes)<100:\n",
    "            pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "            labels_post = np.array([0])                    # just one fake cluster label\n",
    "            sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "            ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "            selected_index_post = 0                        # only one cluster, so index is 0\n",
    "            cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "            spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "            spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "            matches = []                                # no matches\n",
    "            # `snips_baselined` is [C x T x N]\n",
    "            # We only subtract on the referent channel to avoid distortion\n",
    "            template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "            residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "            # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "            # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "            # force key and lookup to match normal case: np.int64\n",
    "            ref_channel = np.int64(ref_channel)\n",
    "            selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "            residuals_per_channel = {\n",
    "                ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            clusters_post, pcs_post, labels_post, sim_matrix_post, n_bad_channels_post, cluster_eis_post, cluster_to_merged_group_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "            # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "            ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "            \n",
    "            contributing_original_ids_post = [\n",
    "                orig_id for orig_id, merged_idx in cluster_to_merged_group_post.items()\n",
    "                if merged_idx == selected_cluster_index_post\n",
    "            ]\n",
    "            contributing_original_ids_post = np.array(contributing_original_ids_post)\n",
    "\n",
    "\n",
    "            spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "            snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "            p2p_threshold = 30\n",
    "            ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "            selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "            selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "            snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "            #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "            # check for matching KS units\n",
    "            results = []\n",
    "            lag = 20\n",
    "            ks_sim_threshold = 0.75\n",
    "\n",
    "            # Run comparison\n",
    "            sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "            matches = [\n",
    "                {\n",
    "                    \"unit_id\": ks_unit_ids[i],\n",
    "                    \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                    \"similarity\": float(sim[i]),\n",
    "                    \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "                }\n",
    "                for i in np.where(sim > ks_sim_threshold)[0]\n",
    "            ]\n",
    "        elapsed_post_clustering= time.time() - start_time \n",
    "        print(f\"Post-clustering: {elapsed_post_clustering-elapsed_bad:.1f} s\")\n",
    "    else:\n",
    "        final_ei = np.mean(snips_baselined, axis=2)\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = final_ei.max(axis=1) - final_ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "        snips_full = snips_full[selected_channels, :, :]\n",
    "        \n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, final_ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n",
    "        elapsed_post_clustering= time.time() - start_time \n",
    "        print(f\"Matches: {elapsed_post_clustering-elapsed_bad:.1f} s\")\n",
    "\n",
    "\n",
    "    \n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    import axolotl_utils_ram\n",
    "    import importlib\n",
    "    importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "    if do_second_clustering:\n",
    "\n",
    "        axolotl_utils_ram.plot_unit_diagnostics(\n",
    "            output_path=debug_folder,\n",
    "            unit_id=unit_id,\n",
    "\n",
    "            # --- From first call to cluster_spike_waveforms\n",
    "            pcs_pre=pcs_pre,\n",
    "            labels_pre=labels_pre,\n",
    "            sim_matrix_pre=sim_matrix_pre,\n",
    "            cluster_eis_pre = cluster_eis_pre,\n",
    "            spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "            n_bad_channels_pre = n_bad_channels_pre,\n",
    "            contributing_original_ids_pre = contributing_original_ids_pre,\n",
    "\n",
    "            # --- From ei_pursuit\n",
    "            mean_score=mean_score,\n",
    "            valid_score=valid_score,\n",
    "            mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "            valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "            mean_thresh=mean_thresh,\n",
    "            valid_thresh=valid_thresh,\n",
    "\n",
    "            # --- Lag estimation and bad spike filtering\n",
    "            lags=lags,\n",
    "            bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "            good_mean_trace=good_mean_trace,\n",
    "            threshold_ampl=-threshold_ampl,\n",
    "            ref_channel=ref_channel,\n",
    "            snips_bad=snips_bad,\n",
    "\n",
    "            # --- From second clustering\n",
    "            pcs_post=pcs_post,\n",
    "            labels_post=labels_post,\n",
    "            sim_matrix_post=sim_matrix_post,\n",
    "            cluster_eis_post = cluster_eis_post,\n",
    "            spikes_for_plot_post = spikes_for_plot_post,\n",
    "            n_bad_channels_post = n_bad_channels_post,\n",
    "            contributing_original_ids_post = contributing_original_ids_post,\n",
    "\n",
    "            # --- For axis labels etc.\n",
    "            window=(-20, 60),\n",
    "\n",
    "            ei_positions=ei_positions,\n",
    "            selected_channels_count=len(selected_channels),\n",
    "\n",
    "            spikes = spikes, \n",
    "            orig_threshold = threshold,\n",
    "            ks_matches = matches\n",
    "        )\n",
    "    else:\n",
    "        axolotl_utils_ram.plot_unit_diagnostics_single_cluster(\n",
    "            output_path=debug_folder,\n",
    "            unit_id=unit_id,\n",
    "\n",
    "            # --- From first call to cluster_spike_waveforms\n",
    "            pcs_pre=pcs_pre,\n",
    "            labels_pre=labels_pre,\n",
    "            sim_matrix_pre=sim_matrix_pre,\n",
    "            cluster_eis_pre = cluster_eis_pre,\n",
    "            spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "            n_bad_channels_pre = n_bad_channels_pre,\n",
    "            contributing_original_ids_pre = contributing_original_ids_pre,\n",
    "\n",
    "            # --- Lag estimation and bad spike filtering\n",
    "            lags=lags,\n",
    "            bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "            bad_spike_traces_easy=bad_spike_traces_easy,  # shape: (n_bad, T)\n",
    "            pcs = pcs,\n",
    "            outlier_inds_easy = final_outlier_inds,\n",
    "            outlier_inds = final_outlier_inds_max,\n",
    "            good_mean_trace=good_mean_trace,\n",
    "            ref_channel=ref_channel,\n",
    "\n",
    "            final_ei = final_ei,\n",
    "\n",
    "            # --- For axis labels etc.\n",
    "\n",
    "            ei_positions=ei_positions,\n",
    "\n",
    "            spikes = spikes, \n",
    "            orig_threshold = threshold,\n",
    "            ks_matches = matches\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    elapsed_diagnostic= time.time() - start_time \n",
    "    print(f\"Diagnostics: {elapsed_diagnostic-elapsed_post_clustering:.1f} s\")\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n",
    "\n",
    "    elapsed_saving = time.time() - start_time \n",
    "    print(f\"Saving: {elapsed_saving-elapsed_diagnostic:.1f} s\")\n",
    "\n",
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        \n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    elapsed_residual = time.time() - start_time \n",
    "    print(f\"Residual: {elapsed_residual-elapsed_saving:.1f} s\")\n",
    "\n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "\n",
    "    for t in reject_times:\n",
    "        # ---- (b)  baseline value for this spike ----------------------\n",
    "        seg_id        = int(t // segment_len)\n",
    "        baseline_val  = baselines[ref_channel, seg_id]\n",
    "\n",
    "        # ---- (c)  overwrite ref-channel samples with baseline --------\n",
    "        t0 = int(t + offset_start)\n",
    "        t1 = int(t + offset_end + 1)           # slice end is non-inclusive\n",
    "        # bounds check\n",
    "        if t0 < 0:            t0 = 0\n",
    "        if t1 > raw_data.shape[0]:\n",
    "            t1 = raw_data.shape[0]\n",
    "        raw_data[t0:t1, ref_channel] = baseline_val\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "\n",
    "    print(f\"Subtraction: {elapsed -elapsed_residual:.1f} s\")\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "    # print(f\"timing. threshold: {elapsed_thresholding:.1f}, cluster: {elapsed_clustering-elapsed_thresholding:.1f},lags: {elapsed_lags-elapsed_thresholding:.1f},post: {elapsed_post_clustering-elapsed_lags:.1f},diag: {elapsed_diagnostic-elapsed_post_clustering:.1f},.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    # if unit_id >= max_units:\n",
    "    #     print(\"Reached unit limit.\")\n",
    "    #     break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import chi2\n",
    "thr_vis  = chi2.ppf(0.9,  df=2)     # illustration\n",
    "thr_cut  = chi2.ppf(0.9, df=2)     # discard\n",
    "\n",
    "final_outlier_inds      = np.where(d2 > thr_vis)[0]\n",
    "final_outlier_inds_max  = np.where(d2 > thr_cut)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"LOF rejects: {len(lof_inds)}\")\n",
    "\n",
    "final_outlier_inds      = lof_inds\n",
    "\n",
    "\n",
    "# Assume labels_kmeans contains the 0/1 cluster assignments from k-means on best_proj\n",
    "# And pcs is your [N, 2] PC1/PC2 array (z-scored)\n",
    "pcs = pcs_z\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(pcs[:, 0], pcs[:, 1], s=5, alpha=0.7)\n",
    "plt.scatter(pcs[final_outlier_inds_max, 0], pcs[final_outlier_inds_max, 1], s=15, color=\"green\",alpha=1)\n",
    "plt.scatter(pcs[final_outlier_inds, 0], pcs[final_outlier_inds, 1], s=3, color=\"red\",alpha=1)\n",
    "plt.xlabel(\"PC1 (z-score)\")\n",
    "plt.ylabel(\"PC2 (z-score)\")\n",
    "plt.title(f\"PC1 vs PC2 scatter\\Hartigan p = {p:.3f}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(pc1_z, pc2_z, s=2, alpha=0.5)\n",
    "plt.title(\"PC1 z-scores for Ref Channel Waveforms\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "thresh_pc = 6\n",
    "final_outlier_mask_max = (np.abs(pc1_z) > thresh_pc) | (np.abs(pc2_z) > thresh_pc) # or -2.5, tune this\n",
    "final_outlier_inds_max = np.where(final_outlier_mask_max)[0]\n",
    "\n",
    "print(len(final_outlier_inds_max))\n",
    "# print(final_outlier_inds_max)\n",
    "\n",
    "if final_outlier_inds_max.size > 0:\n",
    "    from scipy.spatial.distance import cdist\n",
    "\n",
    "    # Combine PC1 and PC2 z-scores\n",
    "    pcs_all = np.column_stack((pc1_z, pc2_z))\n",
    "\n",
    "    # Split into bad and good\n",
    "    bad_pcs = pcs_all[final_outlier_inds_max]\n",
    "    good_mask = np.ones(len(pcs_all), dtype=bool)\n",
    "    good_mask[final_outlier_inds_max] = False\n",
    "    good_pcs = pcs_all[good_mask]\n",
    "\n",
    "    N = min(20, len(good_pcs))\n",
    "\n",
    "    bad_good_means = []\n",
    "    good_good_means = []\n",
    "    ratios = []\n",
    "\n",
    "    for bad in bad_pcs:\n",
    "        # Distance to all good spikes\n",
    "        dists = np.linalg.norm(good_pcs - bad, axis=1)\n",
    "        nearest_inds = np.argsort(dists)[:N]\n",
    "        nearest_good = good_pcs[nearest_inds]\n",
    "\n",
    "        # Mean distance bad -> nearest N good\n",
    "        mean_bad_good = np.mean(dists[nearest_inds])\n",
    "        bad_good_means.append(mean_bad_good)\n",
    "\n",
    "        # Mean pairwise distance among those N good spikes\n",
    "        gg_dists = cdist(nearest_good, nearest_good)\n",
    "        mean_good_good = np.mean(gg_dists[np.triu_indices_from(gg_dists, k=1)])\n",
    "        good_good_means.append(mean_good_good)\n",
    "\n",
    "        # Ratio\n",
    "        ratios.append(mean_bad_good / (mean_good_good + 1e-8))\n",
    "\n",
    "    bad_good_means = np.array(bad_good_means)\n",
    "    good_good_means = np.array(good_good_means)\n",
    "    ratios = np.array(ratios)\n",
    "    # Summary\n",
    "    print(f\"Mean bad-good distance: {np.mean(bad_good_means):.4f}\")\n",
    "    print(f\"Mean good-good distance: {np.mean(good_good_means):.4f}\")\n",
    "    print(f\"Mean ratio (bad-good / good-good): {np.mean(ratios):.4f}\")\n",
    "\n",
    "    # Optional: print distribution\n",
    "    # for i, (bg, gg, r) in enumerate(zip(bad_good_means, good_good_means, ratios)):\n",
    "    #     print(f\"Bad spike {i}: bad-good = {bg:.4f}, good-good = {gg:.4f}, ratio = {r:.2f}\")\n",
    "    # final_outlier_inds_max = final_outlier_inds_max[ratios>3.5]\n",
    "\n",
    "N = snips_baselined.shape[2] # or however you define total spike count\n",
    "labels = np.zeros(N, dtype=int)\n",
    "thresh_pc = 2\n",
    "final_outlier_mask = (np.abs(pc1_z) > thresh_pc) | (np.abs(pc2_z) > thresh_pc) # or -2.5, tune this\n",
    "final_outlier_inds = np.where(final_outlier_mask)[0]\n",
    "\n",
    "labels[final_outlier_inds] = 1\n",
    "\n",
    "thresh_pc = 6\n",
    "final_outlier_mask_max = (np.abs(pc1_z) > thresh_pc) | (np.abs(pc2_z) > thresh_pc) # or -2.5, tune this\n",
    "final_outlier_inds_max = np.where(final_outlier_mask_max)[0]\n",
    "\n",
    "labels[final_outlier_inds_max] = 2\n",
    "\n",
    "merged_clusters, sim, n_bad_channels = axolotl_utils_ram.merge_similar_clusters(snips_baselined, labels, max_lag=3, p2p_thresh=30.0, amp_thresh=-20, cos_thresh=0.9)\n",
    "\n",
    "print(sim)\n",
    "print(n_bad_channels)\n",
    "print(N)\n",
    "print(len(merged_clusters))\n",
    "for i, arr in enumerate(merged_clusters):\n",
    "    print(f\"Array {i} length: {len(arr)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verify_cluster import verify_cluster\n",
    "\n",
    "# parameters for verify_cluster\n",
    "params = {\n",
    "    'window': (-20, 60),\n",
    "    'min_spikes': 100,\n",
    "    'ei_sim_threshold': 0.75,\n",
    "    'k_start': 10,\n",
    "    'k_refine': 4\n",
    "}\n",
    "\n",
    "# choose cell ID (this is index of the cell, not vision ID)\n",
    "spike_times = spikes\n",
    "\n",
    "# run recursive clustering\n",
    "clusters = verify_cluster(\n",
    "    spike_times=spike_times,\n",
    "    dat_path=dat_path,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "print(f\"Returned {len(clusters)} clean subclusters\")\n",
    "for i, cl in enumerate(clusters):\n",
    "    print(f\"  Cluster {i}: {len(cl['inds'])} spikes\")\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze_clusters\n",
    "import importlib\n",
    "importlib.reload(analyze_clusters)\n",
    "\n",
    "# plot EI (Vision style), ISI, firing rate, time course (from one pixel) and STA (single frame with strongest pixel)\n",
    "analyze_clusters.analyze_clusters(clusters,\n",
    "                 spike_times=spike_times,\n",
    "                 sampling_rate=20000,\n",
    "                 dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "                 h5_path='/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5',\n",
    "                 triggers_mat_path='/Volumes/Lab/Users/alexth/axolotl/trigger_in_samples_201703151.mat',\n",
    "                 cluster_ids=None,\n",
    "                 lut=None,\n",
    "                 sta_depth=30,\n",
    "                 sta_offset=0,\n",
    "                 sta_chunk_size=1000,\n",
    "                 sta_refresh=2,\n",
    "                 ei_scale=3,\n",
    "                 ei_cutoff=0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END of main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "\n",
    "# --- PC1 vs PC2 scatter plot ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "unique_labels = np.unique(labels_pre)\n",
    "print(unique_labels)\n",
    "colors = plt.cm.tab10.colors  # or any colormap you like\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    mask = labels_pre == label\n",
    "    color = colors[i % len(colors)]\n",
    "    plt.scatter(pcs_pre[mask, 0], pcs_pre[mask, 1], s=10, color=color, alpha=0.7, label=f\"Cluster {label}\")\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Cluster PCA Scatter\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot EIs for each cluster ---\n",
    "for i, ei in enumerate(cluster_eis_pre):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plot_ei_waveforms(\n",
    "        ei=ei,                 # list of EIs\n",
    "        positions=ei_positions,\n",
    "        ref_channel=ref_channel,\n",
    "        scale=70.0,\n",
    "        box_height=1.5,\n",
    "        box_width=50,\n",
    "        linewidth=0.5,\n",
    "        alpha=0.9,\n",
    "        colors='black'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 20000\n",
    "\n",
    "# --- Load spike times and electrode positions from HDF5 ---\n",
    "all_spikes = {}\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    unit_ids = sorted(f['/spikes'].keys(), key=lambda x: int(x.split('_')[1]))\n",
    "    for uid in unit_ids:\n",
    "        unit_index = int(uid.split('_')[1])\n",
    "        raw = f[f'/spikes/{uid}'][:]\n",
    "        if raw.ndim == 1 and raw.shape[0] == 1:\n",
    "            spikes_sec = np.array(raw[0]).flatten()\n",
    "        else:\n",
    "            spikes_sec = np.array(raw).flatten()\n",
    "        spikes_samples = np.round(spikes_sec * sampling_rate).astype(np.int32)\n",
    "        all_spikes[unit_index] = spikes_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_spikes = all_spikes[56]\n",
    "print(len(ks_spikes))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ref_channel = 51\n",
    "\n",
    "# Define window around each spike\n",
    "pre, post = 20, 60\n",
    "snip_len = pre + post + 1\n",
    "\n",
    "# Allocate array to hold all snippets\n",
    "snippets = []\n",
    "\n",
    "for s in ss:#ks_spikes[:1000]:\n",
    "    if s - pre >= 0 and s + post < raw_data.shape[0]:\n",
    "        snippet = raw_data[s - pre : s + post + 1, ref_channel]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot all snippets\n",
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='black', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "plt.title(f\"Ref channel {ref_channel} waveforms at {len(snippets)} spikes\")\n",
    "plt.xlabel(\"Sample index (relative to spike)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_spikes = all_spikes[56]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ref_channel = 51\n",
    "amps = raw_data[ks_spikes, ref_channel]\n",
    "plt.hist(amps, bins=100)\n",
    "plt.axvline(-50, color='red', linestyle='--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_spikes = all_spikes[56]\n",
    "# ks_spikes = ks_spikes[amps > -50]\n",
    "\n",
    "print(len(ks_spikes))\n",
    "\n",
    "ref_channel = 51\n",
    "fs = 20000  # sampling rate in Hz\n",
    "segment_len = fs  # 1 second = 20,000 samples\n",
    "\n",
    "trace = raw_data[:, ref_channel].astype(np.float32)\n",
    "n_samples = len(trace)\n",
    "\n",
    "# Create baseline-corrected version of the trace\n",
    "trace_corrected = np.empty_like(trace)\n",
    "\n",
    "n_segments = (n_samples + segment_len - 1) // segment_len  # ceil division\n",
    "for i in range(n_segments):\n",
    "    start = i * segment_len\n",
    "    end = min(start + segment_len, n_samples)\n",
    "    segment = trace[start:end]\n",
    "    trace_corrected[start:end] = segment - np.mean(segment)\n",
    "\n",
    "# Now extract snippets from corrected trace\n",
    "pre, post = 20, 60\n",
    "snip_len = pre + post + 1\n",
    "snippets = []\n",
    "\n",
    "for s in ks_spikes[:1000]:\n",
    "    if s - pre >= 0 and s + post < trace_corrected.shape[0]:\n",
    "        snippet = trace_corrected[s - pre : s + post + 1]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='black', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "plt.title(f\"Ref channel {ref_channel} waveforms at {len(snippets)} spikes\\n(1s segment-wise baseline subtraction)\")\n",
    "plt.xlabel(\"Sample index (relative to spike)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amps = trace_corrected[ks_spikes]\n",
    "plt.hist(amps, bins=100)\n",
    "plt.axvline(-50, color='red', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "unit_id_1 = 1011\n",
    "max_diff = 10 # in samples\n",
    "\n",
    "# --- Load spike times ---\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    spikes_1 = h5[f'unit_{unit_id_1}']['spike_times'][:]\n",
    "\n",
    "\n",
    "spikes_2 = ks_spikes\n",
    "# --- Sort spike times (for efficiency) ---\n",
    "spikes_1 = np.sort(spikes_1)\n",
    "spikes_2 = np.sort(spikes_2)\n",
    "\n",
    "# --- Match spikes within max_diff ---\n",
    "matches = []\n",
    "j_start = 0\n",
    "\n",
    "for i, t1 in enumerate(spikes_1):\n",
    "    while j_start < len(spikes_2) and spikes_2[j_start] < t1 - max_diff:\n",
    "        j_start += 1\n",
    "\n",
    "    j = j_start\n",
    "    while j < len(spikes_2) and spikes_2[j] <= t1 + max_diff:\n",
    "        if abs(t1 - spikes_2[j]) <= max_diff:\n",
    "            matches.append((i, j))\n",
    "        j += 1\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"Found {len(matches)} matched spikes between unit {unit_id_1} and ks\")\n",
    "for i, j in matches:\n",
    "    print(f\"Spike1: {spikes_1[i]}, Spike2: {spikes_2[j]}\")\n",
    "\n",
    "# --- Build set of matched indices in spikes_2 ---\n",
    "matched_j = set(j for _, j in matches)\n",
    "\n",
    "# --- Extract unmatched spike times from spikes_2 ---\n",
    "unmatched_spikes_2 = [spikes_2[j] for j in range(len(spikes_2)) if j not in matched_j]\n",
    "\n",
    "print(f\"Found {len(unmatched_spikes_2)} unmatched spikes in unit ks\")\n",
    "unmatched_spikes_2 = np.array(unmatched_spikes_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from h5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "unit_id = 40 # or whatever unit you're checking\n",
    "\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    group = h5[f'unit_{unit_id}']\n",
    "    \n",
    "    spike_times = group['spike_times'][:]\n",
    "    ei = group['ei'][:]\n",
    "    selected_channels = group['selected_channels'][:]\n",
    "    \n",
    "    peak_channel = group.attrs['peak_channel']\n",
    "\n",
    "# Check shapes or values\n",
    "print(\"Spike times:\", spike_times.shape)\n",
    "#print(\"EI shape:\", ei.shape)\n",
    "# print(\"Selected channels:\", selected_channels)\n",
    "# print(179 in selected_channels)\n",
    "#print(\"Peak channel:\", peak_channel)\n",
    "print(spike_times)\n",
    "\n",
    "\n",
    "unit_id = 11 # or whatever unit you're checking\n",
    "\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    group = h5[f'unit_{unit_id}']\n",
    "    \n",
    "    spike_times1 = group['spike_times'][:]\n",
    "    ei = group['ei'][:]\n",
    "    selected_channels = group['selected_channels'][:]\n",
    "    \n",
    "    peak_channel = group.attrs['peak_channel']\n",
    "\n",
    "# Check shapes or values\n",
    "print(\"Spike times:\", spike_times1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# ei = np.mean(snips_baselined, axis=2)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(ei[125,:], color='black', linewidth=1)\n",
    "plt.plot(ei1[125,:], color='red', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ref_channel = 51\n",
    "\n",
    "# Define window around each spike\n",
    "pre, post = 20, 60\n",
    "snip_len = pre + post + 1\n",
    "\n",
    "# Allocate array to hold all snippets\n",
    "snippets = []\n",
    "\n",
    "for s in spikes_1[:50]:\n",
    "    if s - pre >= 0 and s + post < raw_data.shape[0]:\n",
    "        snippet = raw_data[s - pre : s + post + 1, ref_channel]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot all snippets\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='black', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "\n",
    "snippets = []\n",
    "\n",
    "for s in unmatched_spikes_2[:50]:\n",
    "    if s - pre >= 0 and s + post < raw_data.shape[0]:\n",
    "        snippet = raw_data[s - pre : s + post + 1, ref_channel]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot all snippets\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='red', alpha=0.1, linewidth=1)\n",
    "\n",
    "plt.title(f\"Ref channel {ref_channel} waveforms at {len(snippets)} spikes\")\n",
    "plt.xlabel(\"Sample index (relative to spike)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snips, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "    raw_data=raw_data,\n",
    "    spike_times=ks_spikes[mean_scores_at_spikes<20000],\n",
    "    window=window,\n",
    "    selected_channels=np.arange(512)\n",
    ")\n",
    "\n",
    "segment_len = 100_000\n",
    "snips_baselined = snips.copy()  # shape (n_channels, 81, N)\n",
    "n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "# Determine segment index for each spike\n",
    "segment_indices = valid_spike_times // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "# Loop through channels and subtract baseline per spike\n",
    "for ch in range(n_channels):\n",
    "    snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "ei = np.mean(snips_baselined, axis=2)\n",
    "#ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei,\n",
    "    positions=ei_positions,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,\n",
    "    box_width=50,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"EI\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "(\n",
    "spikes,\n",
    "mean_score,\n",
    "valid_score,\n",
    "mean_scores_at_spikes,\n",
    "valid_scores_at_spikes,\n",
    "mean_thresh,\n",
    "valid_thresh\n",
    ") = axolotl_utils_ram.ei_pursuit_ram(\n",
    "    raw_data=raw_data,\n",
    "    spikes=ks_spikes,                     # absolute sample times\n",
    "    ei_template=ei,                    # EI from selected cluster\n",
    "    save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "    alignment_offset = -window[0],\n",
    "    fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "    sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "    return_debug=True, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(spikes))\n",
    "ss=ks_spikes[mean_scores_at_spikes<20000]\n",
    "print(len(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(mean_scores_at_spikes, bins=200, alpha=0.5, label='KS spike scores', color='red')\n",
    "plt.xlabel(\"Mean EI Match Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Mean EI Scores: Global vs. KS-aligned\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NaNs in scores: {np.isnan(scores).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "fit_percentile = 40\n",
    "sigma_thresh = 5.0\n",
    "\n",
    "scores = mean_scores_at_spikes  # KS spike scores\n",
    "\n",
    "clean_scores = mean_scores_at_spikes[~np.isnan(mean_scores_at_spikes)]\n",
    "\n",
    "# 1. Determine percentile cutoff\n",
    "cutoff = np.percentile(clean_scores, fit_percentile, method='nearest')\n",
    "print(cutoff)\n",
    "\n",
    "# 2. Select left tail\n",
    "left_tail = clean_scores[clean_scores <= cutoff]\n",
    "\n",
    "# 3. Fit normal distribution to tail\n",
    "mu, sigma = norm.fit(left_tail)\n",
    "\n",
    "# 4. Compute final threshold\n",
    "threshold = mu - sigma_thresh * sigma\n",
    "\n",
    "print(f\"Fitted mu = {mu:.3f}, sigma = {sigma:.3f}\")\n",
    "print(f\"Computed threshold = {threshold:.3f}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(left_tail, bins=100, density=True, alpha=0.5, label=\"Left tail\")\n",
    "\n",
    "# Overlay fitted Gaussian\n",
    "x = np.linspace(left_tail.min(), left_tail.max(), 200)\n",
    "pdf = norm.pdf(x, mu, sigma)\n",
    "plt.plot(x, pdf, 'r-', label=f\"Fit: μ={mu:.2f}, σ={sigma:.2f}\")\n",
    "\n",
    "plt.axvline(threshold, color='red', linestyle='--', label=f\"Threshold = {threshold:.2f}\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Fit to Left Tail of Scores (KS Spikes)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "unit_id_1 = 1000\n",
    "unit_id_2 = 11\n",
    "max_diff = 10 # in samples\n",
    "\n",
    "# --- Load spike times ---\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    spikes_1 = h5[f'unit_{unit_id_1}']['spike_times'][:]\n",
    "    spikes_2 = h5[f'unit_{unit_id_2}']['spike_times'][:]\n",
    "\n",
    "# --- Sort spike times (for efficiency) ---\n",
    "spikes_1 = np.sort(spikes_1)\n",
    "spikes_2 = np.sort(spikes_2)\n",
    "\n",
    "# --- Match spikes within max_diff ---\n",
    "matches = []\n",
    "j_start = 0\n",
    "\n",
    "for i, t1 in enumerate(spikes_1):\n",
    "    while j_start < len(spikes_2) and spikes_2[j_start] < t1 - max_diff:\n",
    "        j_start += 1\n",
    "\n",
    "    j = j_start\n",
    "    while j < len(spikes_2) and spikes_2[j] <= t1 + max_diff:\n",
    "        if abs(t1 - spikes_2[j]) <= max_diff:\n",
    "            matches.append((i, j))\n",
    "        j += 1\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"Found {len(matches)} matched spikes between unit {unit_id_1} and {unit_id_2}\")\n",
    "for i, j in matches:\n",
    "    print(f\"Spike1: {spikes_1[i]}, Spike2: {spikes_2[j]}\")\n",
    "\n",
    "# --- Build set of matched indices in spikes_2 ---\n",
    "matched_j = set(j for _, j in matches)\n",
    "\n",
    "# --- Extract unmatched spike times from spikes_2 ---\n",
    "unmatched_spikes_2 = [spikes_2[j] for j in range(len(spikes_2)) if j not in matched_j]\n",
    "\n",
    "print(f\"Found {len(unmatched_spikes_2)} unmatched spikes in unit {unit_id_2}\")\n",
    "unmatched_spikes_2 = np.array(unmatched_spikes_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei,\n",
    "    positions=ei_positions,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,\n",
    "    box_width=50,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"EI\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, cluster in enumerate(clusters_pre):\n",
    "    ei = cluster['ei']\n",
    "    ref_ch = cluster['channels'][np.argmax(np.ptp(ei[cluster['channels'], :], axis=1))]\n",
    "    ei_p2p = np.ptp(ei[ref_ch, :])\n",
    "    n_spikes = len(cluster['inds'])\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plot_ei_waveforms(\n",
    "        ei=ei,\n",
    "        positions=ei_positions,\n",
    "        scale=70.0,\n",
    "        box_height=1.0,\n",
    "        box_width=50,\n",
    "        linewidth=0.5,\n",
    "        alpha=0.9,\n",
    "        colors='black'\n",
    "    )\n",
    "    plt.title(f\"Cluster {i} EI — Spikes: {n_spikes}, P2P on Ref Ch ({ref_ch}): {ei_p2p:.1f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips_baselined, ei, k_start=3,return_debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(cluster_eis_pre[2][148,:], color='black', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ei_a = clusters_pre[0]['ei']\n",
    "ei_b = clusters_pre[1]['ei']\n",
    "\n",
    "\n",
    "result = axolotl_utils_ram.compare_ei_subtraction(ei_a, ei_b, max_lag=3, p2p_thresh=30.0)\n",
    "\n",
    "res = np.array(result['per_channel_residuals'])\n",
    "cos_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "neg_inds = np.where(res < -10)[0]\n",
    "\n",
    "print(len(neg_inds))\n",
    "print(cos_sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ei_a = cluster_eis_pre[0]\n",
    "ei_b = cluster_eis_pre[2]\n",
    "\n",
    "\n",
    "result = axolotl_utils_ram.compare_ei_subtraction(ei_a, ei_b, max_lag=3, p2p_thresh=30.0)\n",
    "\n",
    "res = np.array(result['per_channel_residuals'])\n",
    "cos_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "neg_inds = np.where(res < -10)[0]\n",
    "\n",
    "print(len(neg_inds))\n",
    "print(cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_ei_waveforms(\n",
    "    ei=cluster_eis_pre[2],\n",
    "    positions=ei_positions,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,\n",
    "    box_width=50,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(f\"Cluster {i} EI — Spikes: {n_spikes}, P2P on Ref Ch ({ref_ch}): {ei_p2p:.1f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(ei_a[148,:], color='black', linewidth=1)\n",
    "plt.plot(ei_b[148,:], color='red', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_cosine_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "amp_threshold = -10\n",
    "cos_threshold = 0.9\n",
    "\n",
    "res = result['per_channel_residuals']\n",
    "neg_inds = np.where(np.array(res) < amp_threshold)[0]\n",
    "if global_cosine_sim < cos_threshold or len(neg_inds) > 0:\n",
    "    print('two units')\n",
    "else:\n",
    "    print('same unit')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(result['good_channels'], result['per_channel_residuals'], color='gray')\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Mean Residual (B - A)\")\n",
    "plt.title(\"Per-Channel Residuals (Masked Subtraction)\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(result['good_channels'], result['per_channel_cosine_sim'], color='gray')\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Mean Residual (B - A)\")\n",
    "plt.title(\"Per-Channel cosine_sim (Masked Subtraction)\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.bar(result['good_channels'], result['p2p'], color='gray')\n",
    "# plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "# plt.xlabel(\"Channel ID\")\n",
    "# plt.ylabel(\"Mean Residual (B - A)\")\n",
    "# plt.title(\"Per-Channel amplitude\")\n",
    "# plt.grid(True, axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.75× of mean waveform peak\n",
    "    threshold_ampl = 0.75 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "        dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        ref_channel=ref_channel,\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype='int16'\n",
    "    )\n",
    "\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "    final_spike_inds = np.where(keep_mask)[0]\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'window': (-20, 60),\n",
    "    'min_spikes': 100,\n",
    "    'ei_sim_threshold': 0.75,\n",
    "    'k_start': 4,\n",
    "    'k_refine': 2\n",
    "}\n",
    "\n",
    "from verify_cluster import verify_cluster\n",
    "\n",
    "spike_times = spikes\n",
    "clusters = verify_cluster(\n",
    "    spike_times=spike_times,\n",
    "    dat_path=snips_baselined,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "print(f\"Returned {len(clusters)} clean subclusters\")\n",
    "for i, cl in enumerate(clusters):\n",
    "    print(f\"  Cluster {i}: {len(cl['inds'])} spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import analyze_clusters\n",
    "import importlib\n",
    "importlib.reload(analyze_clusters)\n",
    "\n",
    "\n",
    "analyze_clusters.analyze_clusters(clusters,\n",
    "                 spike_times=spikes,\n",
    "                 sampling_rate=20000,\n",
    "                 dat_path=snips_baselined,\n",
    "                 h5_path='/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5',\n",
    "                 triggers_mat_path='/Volumes/Lab/Users/alexth/axolotl/trigger_in_samples_201703151.mat',\n",
    "                 cluster_ids=None,\n",
    "                 lut=None,\n",
    "                 sta_depth=30,\n",
    "                 sta_offset=0,\n",
    "                 sta_chunk_size=1000,\n",
    "                 sta_refresh=2,\n",
    "                 ei_scale=3,\n",
    "                 ei_cutoff=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = snips_baselined[ref_channel, 20, :].copy()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tmp)\n",
    "\n",
    "final_spike_inds = np.arange(len(spikes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(tmp, bins=50, color='gray', edgecolor='black')\n",
    "plt.title(\"Histogram of tmp values\")\n",
    "plt.xlabel(\"Amplitude\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(tmp > -500)[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(snips_baselined[39, :, inds].T, alpha=1)\n",
    "plt.plot(snips_baselined[39, :, :6], alpha=1)\n",
    "plt.title(f\"Overlay of {len(inds)} selected snippets on channel 39\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p_threshold = 30\n",
    "ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "#print(\"reclustered pursuit\\n\")\n",
    "\n",
    "# check for matching KS units\n",
    "results = []\n",
    "lag = 20\n",
    "ks_sim_threshold = 0.75\n",
    "\n",
    "# Run comparison\n",
    "sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "matches = [\n",
    "    {\n",
    "        \"unit_id\": ks_unit_ids[i],\n",
    "        \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "        \"similarity\": float(sim[i]),\n",
    "        \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "    }\n",
    "    for i in np.where(sim > ks_sim_threshold)[0]\n",
    "]\n",
    "\n",
    "pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "labels_post = np.array([0])                    # just one fake cluster label\n",
    "sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "selected_index_post = 0                        # only one cluster, so index is 0\n",
    "cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "spike_counts_post = [len(snips)]               # use actual number of spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    # if unit_id >= max_units:\n",
    "    #     print(\"Reached unit limit.\")\n",
    "    #     break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencoder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
