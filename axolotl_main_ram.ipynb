{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "from compare_eis import compare_eis\n",
    "\n",
    "\n",
    "# --- Path and recording setup ---\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat\"\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Get total number of samples ---\n",
    "file_size_bytes = os.path.getsize(dat_path)\n",
    "total_samples = file_size_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "\n",
    "# --- Load entire file into RAM as int16 ---\n",
    "raw_data = np.fromfile(dat_path, dtype=dtype, count=total_samples * n_channels)\n",
    "raw_data = raw_data.reshape((total_samples, n_channels))  # shape: [T, C]\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 1500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "do_pursuit = 0\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0528.h5' # where to save data\n",
    "\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug\"\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "    ks_vision_ids = f['/vision_ids'][:]  # shape: (N_units,)\n",
    "\n",
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "save_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_and_artifacts.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(save_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines = axolotl_utils_ram.compute_baselines_int16(raw_data, segment_len=100_000) # shape (512, 360)\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "        }, f)\n",
    "\n",
    "\n",
    "# get KS EIs\n",
    "ks_ei_path = '/Volumes/Lab/Users/alexth/axolotl/ks_eis_subset.h5'\n",
    "ks_templates = {}\n",
    "ks_n_spikes = {}\n",
    "\n",
    "with h5py.File(ks_ei_path, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        unit_id = int(k.split('_')[1])-1\n",
    "        ks_templates[unit_id] = f[k][:]\n",
    "        ks_n_spikes[unit_id] = f[k].attrs.get('n_spikes', -1)  # fallback if missing\n",
    "\n",
    "ks_unit_ids = list(ks_templates.keys())\n",
    "ks_ei_stack = np.stack([ks_templates[k] for k in ks_unit_ids], axis=0)  # [N x 512 x 81]\n",
    "\n",
    "unit_id = 0\n",
    "\n",
    "print(f\"\\n=== Starting unit {unit_id} ===\")\n",
    "\n",
    "while True:\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # could cache scores on channels to pre-identify next one\n",
    "    ref_channel = axolotl_utils_ram.find_dominant_channel_ram(\n",
    "            raw_data = raw_data,\n",
    "            segment_len = 100_000,\n",
    "            n_segments = 10,\n",
    "            peak_window = 30,\n",
    "            top_k_neg = 20,\n",
    "            top_k_events = 5,\n",
    "            seed = 42\n",
    "        )\n",
    "\n",
    "    threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "        raw_data=raw_data,\n",
    "        ref_channel=ref_channel,\n",
    "        window = 30,\n",
    "        total_samples_to_read = total_samples,\n",
    "        refractory = 30,\n",
    "        top_n = 100\n",
    "    )\n",
    "\n",
    "    print(f\"Channel: {ref_channel}, Threshold: {-threshold:.1f}, Initial spikes: {len(spike_times)}\")\n",
    "\n",
    "    snips, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spike_times,\n",
    "        window=window,\n",
    "        selected_channels=np.arange(n_channels)\n",
    "    )\n",
    "\n",
    "    ei = np.mean(snips, axis=2)\n",
    "    ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "    spikes_for_plot_pre = valid_spike_times\n",
    "\n",
    "    # Step 6–7: Cluster and select dominant unit\n",
    "    clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips, ei, k_start=3,return_debug=True)\n",
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.66× of mean waveform peak\n",
    "    threshold_ampl = 0.66 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "        dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        ref_channel=ref_channel,\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype='int16'\n",
    "    )\n",
    "\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    if unit_id >= max_units:\n",
    "        print(\"Reached unit limit.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25, 4))\n",
    "plt.plot(raw_data[:5000, 39])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Channel 39: First 5,000 samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parameters ---\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat'  # ← replace with actual path\n",
    "n_channels = 512\n",
    "channel = 39\n",
    "n_samples = 5000\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Read from disk ---\n",
    "with open(dat_path, 'rb') as f:\n",
    "    # Read first 5000 timepoints (i.e., 5000 * n_channels values)\n",
    "    raw = np.fromfile(f, dtype=dtype, count=n_samples * n_channels)\n",
    "    raw = raw.reshape((n_samples, n_channels))  # [time, channel]\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(25, 4))\n",
    "plt.plot(raw[:5000, 39])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Channel 39: First 5,000 samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test - development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading baselines\n",
      "subtracting baselines\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "from compare_eis import compare_eis\n",
    "\n",
    "\n",
    "# --- Path and recording setup ---\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat\"\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Get total number of samples ---\n",
    "file_size_bytes = os.path.getsize(dat_path)\n",
    "total_samples = file_size_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "\n",
    "# --- Load entire file into RAM as int16 ---\n",
    "raw_data = np.fromfile(dat_path, dtype=dtype, count=total_samples * n_channels)\n",
    "raw_data = raw_data.reshape((total_samples, n_channels))  # shape: [T, C]\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 1500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "do_pursuit = 0\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "    ks_vision_ids = f['/vision_ids'][:]  # shape: (N_units,)\n",
    "\n",
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "baseline_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_derivative_20k.json\"\n",
    "\n",
    "segment_len = 20_000\n",
    "if os.path.exists(baseline_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(baseline_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines = axolotl_utils_ram.compute_baselines_int16_deriv_robust(raw_data, segment_len=segment_len, diff_thresh=10, trim_fraction=0.15) # shape (512, 360)\n",
    "\n",
    "    with open(baseline_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "        }, f)\n",
    "\n",
    "print(\"subtracting baselines\")\n",
    "\n",
    "axolotl_utils_ram.subtract_segment_baselines_int16(raw_data=raw_data,\n",
    "                                     baselines_f32=baselines,\n",
    "                                     segment_len=segment_len) \n",
    "\n",
    "\n",
    "# get KS EIs\n",
    "ks_ei_path = '/Volumes/Lab/Users/alexth/axolotl/ks_eis_subset.h5'\n",
    "ks_templates = {}\n",
    "ks_n_spikes = {}\n",
    "\n",
    "with h5py.File(ks_ei_path, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        unit_id = int(k.split('_')[1])-1\n",
    "        ks_templates[unit_id] = f[k][:]\n",
    "        ks_n_spikes[unit_id] = f[k].attrs.get('n_spikes', -1)  # fallback if missing\n",
    "\n",
    "ks_unit_ids = list(ks_templates.keys())\n",
    "ks_ei_stack = np.stack([ks_templates[k] for k in ks_unit_ids], axis=0)  # [N x 512 x 81]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_id = 0\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug/0621\"\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0621.h5' # where to save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "from diptest import diptest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import chi2\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "# import sys\n",
    "\n",
    "# class Tee:\n",
    "#     def __init__(self, *files, flush_every=10):\n",
    "#         self.files = files\n",
    "#         self.line_count = 0\n",
    "#         self.flush_every = flush_every\n",
    "\n",
    "#     def write(self, obj):\n",
    "#         for f in self.files:\n",
    "#             f.write(obj)\n",
    "#         # Count lines\n",
    "#         self.line_count += obj.count('\\n')\n",
    "#         if self.line_count >= self.flush_every:\n",
    "#             self.flush()\n",
    "#             self.line_count = 0  # Reset counter\n",
    "\n",
    "#     def flush(self):\n",
    "#         for f in self.files:\n",
    "#             f.flush()\n",
    "\n",
    "\n",
    "\n",
    "# # Create the log file\n",
    "# log_file = open(\"/Volumes/Lab/Users/alexth/axolotl/debug/0616/processing_log.txt\", \"w\")\n",
    "\n",
    "# # Redirect stdout to both notebook and file\n",
    "# sys.stdout = Tee(sys.__stdout__, log_file, flush_every=10)\n",
    "\n",
    "# unit_id = 1004\n",
    "\n",
    "\n",
    "# last_ref_channel = None\n",
    "# remaining_candidates = []\n",
    "\n",
    "do_second_clustering = False\n",
    "\n",
    "if 'rejected_spike_log' not in globals():\n",
    "    rejected_spike_log = {\n",
    "        'times'   : [],   # absolute sample indices\n",
    "        'channel' : [],   # ref_channel for each rejected snip\n",
    "        'unit_id' : []    # which unit’s refinement produced the reject\n",
    "    }\n",
    "\n",
    "# ref_channel = 51\n",
    "\n",
    "max_units = 1500\n",
    "\n",
    "\n",
    "window = (-40, 80)\n",
    "\n",
    "window_post_lag = (-20,60)\n",
    "\n",
    "ax_ei_list = []\n",
    "\n",
    "thresholds = np.zeros(511, dtype=float)\n",
    "spike_length = np.zeros(511, dtype=int)\n",
    "for ref_channel in range(0,511):\n",
    "    threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "        raw_data=raw_data,\n",
    "        ref_channel=ref_channel,\n",
    "        window = 30,\n",
    "        total_samples_to_read = 5_000_000,\n",
    "        refractory = 10,\n",
    "        top_n = 100,\n",
    "        threshold_scale = 0.5\n",
    "    )\n",
    "    thresholds[ref_channel] = threshold\n",
    "    spike_length[ref_channel] = len(spike_times)\n",
    " \n",
    "   \n",
    "sorted_indices = np.argsort(-thresholds)  # ascending (min to max, most negative to least negative)\n",
    "\n",
    "# for idx in sorted_indices:\n",
    "#     print(f\"Channel {idx}, threshold {-thresholds[idx]:.2f}, spike N {spike_length[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "# if 1:\n",
    "while True:\n",
    "\n",
    "    print(f\"\\n=== Starting unit {unit_id} ===\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    ref_channel = np.argmax(thresholds)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 1.  Find all threshold crossings on ref_channel\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "        raw_data=raw_data,\n",
    "        ref_channel=ref_channel,\n",
    "        window = 30,\n",
    "        total_samples_to_read = total_samples,\n",
    "        refractory = 10,\n",
    "        top_n = 100,\n",
    "        threshold_scale = 0.5\n",
    "    )\n",
    "    \n",
    "\n",
    "    elapsed_thresholding = time.time() - start_time \n",
    "    print(f\"Threshold: {elapsed_thresholding:.1f} s\")\n",
    "\n",
    "    print(f\"Channel: {ref_channel}, Threshold: {-threshold:.1f}, Initial spikes: {len(spike_times)}\")\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 2.  Extract data snippets for each spike\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    snips_baselined, valid_spike_times = axolotl_utils_ram.extract_snippets_fast_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spike_times,\n",
    "        window=window,\n",
    "        selected_channels=np.arange(n_channels)\n",
    "    )\n",
    "    \n",
    "    elapsed_snippet = time.time() - start_time \n",
    "    print(f\"Snippet: {elapsed_snippet-elapsed_thresholding:.1f} s\")\n",
    "\n",
    "    # compute EI\n",
    "    ei = np.mean(snips_baselined, axis=2)\n",
    "\n",
    "    spikes_for_plot_pre = valid_spike_times\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 3.  Run PCA and k-means with adaptive k on extracted spikes\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    import axolotl_utils_ram\n",
    "    import importlib\n",
    "    importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "    k_start = min(5, 3 + (len(spike_times) - 1) // 3000)\n",
    "\n",
    "    clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, n_bad_channels_pre, cluster_eis_pre, cluster_to_merged_group_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips_baselined, ei, k_start=k_start,return_debug=True)\n",
    "\n",
    "    elapsed_clustering = time.time() - start_time \n",
    "    print(f\"Clustering: {elapsed_clustering-elapsed_snippet:.1f} s, with k={k_start}\")\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 4.  Select cluster with the largest negative amplitude for processing. Future: use all that are fully extracted?\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    # ei shape: [channels, time]\n",
    "    # Compute p2p for each channel\n",
    "    p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "\n",
    "    # Find channel with largest p2p - make sure it is the current ref_channel. If not, implement skip in the future - spikes should be sorted on Main\n",
    "    best_channel = np.argmax(p2p)\n",
    "    best_p2p = p2p[best_channel]\n",
    "    if best_channel != ref_channel:\n",
    "        print(f\"Largest p2p at channel {best_channel}, p2p = {best_p2p:.2f}, IS NOT ref channel {ref_channel}\")\n",
    "\n",
    "    # indices of all clusters that were merged from original k-means - for diagnostic plot\n",
    "    contributing_original_ids_pre = [\n",
    "        orig_id for orig_id, merged_idx in cluster_to_merged_group_pre.items()\n",
    "        if merged_idx == selected_cluster_index_pre\n",
    "    ]\n",
    "    contributing_original_ids_pre = np.array(contributing_original_ids_pre)\n",
    "\n",
    "    spikes = spike_times[spikes_idx]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 5.  Compute lags on the main channel to align the spikes (by somatic waveform)\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    snips_ref_channel = snips_baselined[ref_channel,:,spikes_idx]\n",
    "    snips_ref_channel = snips_ref_channel[:, np.newaxis, :]\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 6.  Re-extract the data snippets for aligned spikes. Works faster than shifting the snippets...\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_fast_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    elapsed_snippet_extraction = time.time() - start_time \n",
    "    print(f\"Snippet: {elapsed_snippet_extraction-elapsed_clustering:.1f} s\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 7.  Reanalyze ONLY the main channel for outliers - either randomly co-opted false spikes, or collisions/distorted WFs.\n",
    "    # --------------------------------------------------------------------\n",
    "    ref_snips     = snips_full[ref_channel, :, :].copy()\n",
    "    ref_snips     = ref_snips.T          # [N × 81]\n",
    "    wave_window   = slice(15, 40)                                 # focus on peak\n",
    "    waveforms     = ref_snips[:, wave_window]                     # [N × 25]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 7.1.  PCA, 2 components\n",
    "    # --------------------------------------------------------------------\n",
    "    pcs_raw = PCA(n_components=2, svd_solver='full').fit_transform(waveforms)  # [N × 2]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 7.2.  Hartigan’s dip test on rotated projections\n",
    "    # --------------------------------------------------------------------\n",
    "    angle_step = 10\n",
    "    angles = np.deg2rad(np.arange(0, 180, angle_step))\n",
    "\n",
    "    best_p = 1.0\n",
    "    best_proj = None\n",
    "    for theta in angles:\n",
    "        proj = pcs_raw[:, 0] * np.cos(theta) + pcs_raw[:, 1] * np.sin(theta)\n",
    "        _, p = diptest(proj)\n",
    "        if p < best_p:\n",
    "            best_p, best_proj = p, proj\n",
    "\n",
    "    discard_inds_bimodal = np.empty(0, dtype=int)\n",
    "\n",
    "    if best_p < 0.05:                                   # suspected bimodality\n",
    "        # histogram → first two peaks as initial centroids\n",
    "        hist, bin_edges = np.histogram(best_proj, bins=30)\n",
    "        peaks, _ = find_peaks(hist)\n",
    "        if len(peaks) >= 2:\n",
    "            centroids = bin_edges[peaks[:2]].reshape(-1, 1)\n",
    "            km = KMeans(n_clusters=2, init=centroids, n_init=1, random_state=42)\n",
    "            labels = km.fit_predict(best_proj[:, None])\n",
    "\n",
    "            # keep larger lobe, discard smaller one\n",
    "            counts = np.bincount(labels)\n",
    "            keep_label = counts.argmax()\n",
    "            discard_inds_bimodal = np.where(labels != keep_label)[0]\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 7.3.  Discard spikes that are too far or in a smaller split-up cluster. Currently, just zero them.\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    if discard_inds_bimodal.size:\n",
    "        keep_mask  = np.ones(len(pcs_raw), dtype=bool)\n",
    "        keep_mask[discard_inds_bimodal] = False\n",
    "        snips_full = snips_full[:, :, keep_mask]\n",
    "        valid_spike_times = valid_spike_times[keep_mask]\n",
    "        spikes = spikes[keep_mask]\n",
    "        waveforms     = waveforms[keep_mask, :]\n",
    "        pcs_raw   = PCA(n_components=2, svd_solver='full').fit_transform(waveforms)\n",
    "        print(f\"   Split the final cluster according to Hartigan test! Started with {len(keep_mask)} spikes, ended with {len(spikes)}\")\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # 7.4.  z-score PCs, Mahalanobis (actually equivalent to euclidean after z-scoring in 2D)\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    pcs_z = StandardScaler().fit_transform(pcs_raw)\n",
    "    pcs = pcs_z\n",
    "    \n",
    "    d2 = np.sum(pcs_z**2, axis=1)\n",
    "\n",
    "\n",
    "    thr_vis  = chi2.ppf(0.999,  df=2)     # illustration\n",
    "    thr_cut  = chi2.ppf(0.9999, df=2)     # discard\n",
    "\n",
    "    final_outlier_inds      = np.where(d2 > thr_vis)[0]\n",
    "    final_outlier_inds_max  = np.where(d2 > thr_cut)[0]\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # 7.5.  Local Outlier Factor\n",
    "    # --------------------------------------------------------------------\n",
    "        # ----------------- build robust inlier core -------------------------\n",
    "    core_mask = d2 < chi2.ppf(0.999, df=2)       # Mahalanobis core (~0.1 % trimmed)\n",
    "\n",
    "    # ----------------- semi-supervised LOF on the core ------------------\n",
    "    lof = LocalOutlierFactor(n_neighbors=20, novelty=True)\n",
    "    lof.fit(pcs[core_mask])                    # train ONLY on good spikes\n",
    "    lof_pred   = lof.predict(pcs)              # −1 = outlier w.r.t. core\n",
    "    lof_scores = lof.negative_outlier_factor_      # more negative = more outlying\n",
    "    lof_inds   = np.where(lof_pred == -1)[0]\n",
    "\n",
    "    final_outlier_inds_max  = np.union1d(final_outlier_inds_max, lof_inds)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 7.6.  Handle rejected spikes: find the spike region with signal on the ref_channel\n",
    "    # ------------------------------------------------------------------\n",
    "    pre, post        = window                     # snippet definition\n",
    "    n_channels, snip_len, n_spikes = snips_full.shape\n",
    "    accepted_mask    = np.ones(n_spikes, dtype=bool)\n",
    "    accepted_mask[final_outlier_inds_max] = False\n",
    "    accepted_inds    = np.where(accepted_mask)[0]\n",
    "\n",
    "\n",
    "    if accepted_inds.size:                         # normal case\n",
    "        mean_ref = snips_full[ref_channel, :, accepted_inds].copy()\n",
    "    else:                                          # degenerate (all rejected)\n",
    "        mean_ref = snips_full[ref_channel, :, :].copy()\n",
    "\n",
    "    mean_ref = mean_ref.T\n",
    "    mean_ref = np.mean(mean_ref, axis=1)\n",
    "    mean_ref = mean_ref-mean_ref[0]\n",
    "\n",
    "    abs_ref = np.abs(mean_ref)\n",
    "\n",
    "    # --- thresholds in μV ------------------------------------------------\n",
    "    max_abs = abs_ref.max()\n",
    "    low_thr = 0.01 * max_abs\n",
    "    high_thr = 0.1 * max_abs\n",
    "\n",
    "    # first index where |WF| > 5 µV  (fall back to 0)\n",
    "    try:\n",
    "        idx_start = int(np.where(abs_ref > low_thr)[0][0])\n",
    "    except IndexError:\n",
    "        idx_start = 0\n",
    "\n",
    "    # last index where |WF| > 20 µV  (fall back to end)\n",
    "    cands = np.where(abs_ref > high_thr)[0]\n",
    "    idx_end = int(cands[-1]) if cands.size else snip_len - 1\n",
    "\n",
    "    print(f\"Template masking for reject spikes: {idx_start} to {idx_end}\")\n",
    "\n",
    "\n",
    "    # convert waveform indices → sample offsets relative to spike centre\n",
    "    # global_sample = spike_time + (wave_idx + pre)\n",
    "    offset_start = idx_start + pre            # pre negative\n",
    "    offset_end   = idx_end   + pre            # inclusive\n",
    "    if offset_start > offset_end:             # safety\n",
    "        offset_start, offset_end = pre, post\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 7.7.  Stash rejects\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    reject_times = spikes[final_outlier_inds_max]\n",
    "    n_rej        = len(reject_times)\n",
    "\n",
    "    for t in reject_times:\n",
    "        rejected_spike_log['times']  .append(int(t))\n",
    "        rejected_spike_log['channel'].append(ref_channel)\n",
    "        rejected_spike_log['unit_id'].append(unit_id)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 7.8.  Collect data snippets of rejected spikes to display in diagnostics plot\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    bad_inds = final_outlier_inds_max #outlier_inds\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spikes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces_easy = snips_full[ref_channel, :, final_outlier_inds]  # shape: (n_bad, T)\n",
    "    bad_spike_traces = snips_full[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 8.  Collect surviving spikes and final spike times\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_full[ref_channel, :, :], axis=1)\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "    final_spike_inds = np.where(keep_mask)[0]\n",
    "\n",
    "    elapsed_bad = time.time() - start_time \n",
    "    print(f\"Distortion handling: {elapsed_bad - elapsed_snippet_extraction:.1f} s, stashed {len(bad_inds)} spikes\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 9.  Compute EI, selected_channels, cut snippets for records and for subtraction\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    final_ei = np.mean(snips_full, axis=2)\n",
    "\n",
    "    p2p_threshold = 30\n",
    "    ei_p2p = final_ei.max(axis=1) - final_ei.min(axis=1)\n",
    "    selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "    selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "    snips_full = snips_full[selected_channels, :, :]\n",
    "    \n",
    "    # ------------------------------------------------------------------\n",
    "    # 10.  Find KS and Axolotl matches by simple EI\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # check for matching KS units\n",
    "    results = []\n",
    "    lag = 20\n",
    "    ks_sim_threshold = 0.75\n",
    "\n",
    "    # Run comparison\n",
    "    sim = compare_eis(ks_ei_stack, final_ei[:,20:101], lag).squeeze() # shape: (num_KS_units,)\n",
    "    matches = [\n",
    "        {\n",
    "            \"unit_id\": ks_unit_ids[i],\n",
    "            \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "            \"similarity\": float(sim[i]),\n",
    "            \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "        }\n",
    "        for i in np.where(sim > ks_sim_threshold)[0]\n",
    "    ]\n",
    "    # Find and print best match regardless of threshold\n",
    "    best_idx = np.argmax(sim)\n",
    "    best_ks_unit_id = ks_unit_ids[best_idx]\n",
    "    best_ks_vision_id = int(ks_vision_ids[best_ks_unit_id].item())\n",
    "    best_sim = sim[best_idx]\n",
    "\n",
    "    print(f\"Closest Kilosort unit {best_ks_vision_id} with similarity {best_sim:.3f}\")\n",
    "\n",
    "    elapsed_post_clustering= time.time() - start_time \n",
    "    print(f\"Matches: {elapsed_post_clustering-elapsed_bad:.1f} s\")\n",
    "\n",
    "\n",
    "    if len(ax_ei_list) > 0:\n",
    "        ax_ei_stack = np.stack(ax_ei_list, axis=0)\n",
    "        sim = compare_eis(ax_ei_stack, final_ei, lag).reshape(-1) \n",
    "        best_idx = np.argmax(sim)\n",
    "        best_sim = sim[best_idx]\n",
    "\n",
    "        print(f\"Closest Axolotl unit {best_idx} with similarity {best_sim:.3f}\")\n",
    "\n",
    "    # Append current final_ei\n",
    "    ax_ei_list.append(final_ei)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 11.  DIAGNOSTIC PLOTS\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    import axolotl_utils_ram\n",
    "    import importlib\n",
    "    importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics_single_cluster(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "        n_bad_channels_pre = n_bad_channels_pre,\n",
    "        contributing_original_ids_pre = contributing_original_ids_pre,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        bad_spike_traces_easy=bad_spike_traces_easy,  # shape: (n_bad, T)\n",
    "        pcs = pcs,\n",
    "        outlier_inds_easy = final_outlier_inds,\n",
    "        outlier_inds = final_outlier_inds_max,\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        ref_channel=ref_channel,\n",
    "\n",
    "        final_ei = final_ei,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    elapsed_diagnostic= time.time() - start_time \n",
    "    print(f\"Diagnostics: {elapsed_diagnostic-elapsed_post_clustering:.1f} s\")\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 12.  Save unit metadata\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', final_ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n",
    "\n",
    "    elapsed_saving = time.time() - start_time \n",
    "    print(f\"Saving: {elapsed_saving-elapsed_diagnostic:.1f} s\")\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 13.  Compute residuals on selected channels\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        \n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]*0    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=segment_len,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        idx_in_selected = np.where(selected_channels == ref_channel)[0]\n",
    "        idx_in_selected = idx_in_selected[0]\n",
    "\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_full[idx_in_selected], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_full[idx_in_selected] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    elapsed_residual = time.time() - start_time \n",
    "    print(f\"Residual: {elapsed_residual-elapsed_saving:.1f} s\")\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 14.  Edit raw data - can do in RAM, file, or both. Usually RAM only for testing.\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 15.  Zero out rejected spikes - not ideal, maybe nan or masking would work better. \n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    for t in reject_times:\n",
    "        # ----   overwrite ref-channel samples with baseline --------\n",
    "        t0 = int(t + offset_start)\n",
    "        t1 = int(t + offset_end + 1)           # slice end is non-inclusive\n",
    "        # bounds check\n",
    "        if t0 < 0:            t0 = 0\n",
    "        if t1 > raw_data.shape[0]:\n",
    "            t1 = raw_data.shape[0]\n",
    "        raw_data[t0:t1, ref_channel] = 0\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # 16.  Re-estimate thresholds\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    cnt = 0\n",
    "    for new_ref_channel in range(0, 511):\n",
    "        neg_amp = -np.min(final_ei[new_ref_channel, :])\n",
    "        existing_threshold = thresholds[new_ref_channel]\n",
    "\n",
    "        if neg_amp >= 0.9 * abs(existing_threshold):\n",
    "            # EI amplitude close enough to threshold → recompute\n",
    "            threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "                raw_data=raw_data,\n",
    "                ref_channel=new_ref_channel,\n",
    "                window=30,\n",
    "                total_samples_to_read=5_000_000,\n",
    "                refractory=10,\n",
    "                top_n=100,\n",
    "                threshold_scale=0.5\n",
    "            )\n",
    "            thresholds[new_ref_channel] = threshold\n",
    "            cnt = cnt+1\n",
    "\n",
    "    print(f\"Recomputed threshold on {cnt} channels\")\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "\n",
    "    print(f\"Subtraction + threshold: {elapsed -elapsed_residual:.1f} s\")\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "    # print(f\"timing. threshold: {elapsed_thresholding:.1f}, cluster: {elapsed_clustering-elapsed_thresholding:.1f},lags: {elapsed_lags-elapsed_thresholding:.1f},post: {elapsed_post_clustering-elapsed_lags:.1f},diag: {elapsed_diagnostic-elapsed_post_clustering:.1f},.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    # if unit_id >= max_units:\n",
    "    #     print(\"Reached unit limit.\")\n",
    "    #     break\n",
    "\n",
    "# sys.stdout = sys.__stdout__\n",
    "# log_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative model test - DEVELOPMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "\n",
    "units = axolotl_utils_ram.load_units_from_h5(h5_out_path)\n",
    "\n",
    "unit_ids = sorted(units.keys())\n",
    "selected_channels_list = [units[uid]['selected_channels'] for uid in unit_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_channel = 413\n",
    "max_unit_id = 92\n",
    "\n",
    "units_with_413 = [\n",
    "    unit_id for unit_id, data in units.items()\n",
    "    if unit_id <= max_unit_id and target_channel in data['selected_channels']\n",
    "]\n",
    "\n",
    "print(f\"Units with channel {target_channel} and unit_id ≤ {max_unit_id}: {units_with_413}\")\n",
    "\n",
    "# Get all unit IDs (sorted, to ensure order is clear)\n",
    "unit_ids = sorted(units.keys())\n",
    "num_units = len(unit_ids)\n",
    "\n",
    "print(num_units)\n",
    "\n",
    "# Get shape info from first EI\n",
    "example_ei = units[unit_ids[0]]['ei']\n",
    "num_channels, num_timepoints = example_ei.shape\n",
    "\n",
    "# Allocate array\n",
    "ax_ei_stack = np.zeros((num_units, num_channels, num_timepoints), dtype=np.float32)\n",
    "\n",
    "# Fill it\n",
    "for i, unit_id in enumerate(unit_ids):\n",
    "    ax_ei_stack[i] = units[unit_id]['ei']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_units = [4, 54, 59, 9]\n",
    "\n",
    "# target_units = [4, 54, 59]\n",
    "\n",
    "window_start = 0#3003000\n",
    "window_end = 10_000_000#3003000+3000\n",
    "channel = 413\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Collect spikes per unit\n",
    "spikes_in_window = {}\n",
    "for uid in target_units:\n",
    "    spikes = units[uid]['spike_times']\n",
    "    spikes_window = spikes[(spikes >= window_start) & (spikes < window_end)]\n",
    "    spikes_in_window[uid] = spikes_window\n",
    "\n",
    "# Extract raw data for the window\n",
    "raw_snip = raw_data[window_start:window_end, channel].astype(np.float32)\n",
    "\n",
    "# plt.figure(figsize=(12, 4))\n",
    "# x = np.arange(window_start, window_end)\n",
    "# plt.plot(x, raw_snip, color='black', linewidth=0.5, label='Raw data')\n",
    "\n",
    "# # Colors for each unit\n",
    "# colors = {4: 'red', 54: 'blue', 59: 'green', 9: 'magenta'}\n",
    "\n",
    "# # Plot spike markers\n",
    "# for uid in target_units:\n",
    "#     spikes = spikes_in_window[uid]\n",
    "#     for s in spikes:\n",
    "#         plt.axvline(s, color=colors[uid], linestyle='--', alpha=0.7, label=f'Unit {uid}')\n",
    "        \n",
    "# # To avoid duplicate labels in legend\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "# unique = dict(zip(labels, handles))\n",
    "# plt.legend(unique.values(), unique.keys())\n",
    "\n",
    "# plt.xlabel(\"Sample\")\n",
    "# plt.ylabel(\"ADC units\")\n",
    "# plt.title(f\"Raw data on channel {channel} with spike times\")\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 413\n",
    "offsets = []\n",
    "\n",
    "for uid in target_units:\n",
    "    p2p = np.ptp(ax_ei_stack[uid], axis=1)  # [channels]\n",
    "    main_ch = np.argmax(p2p)\n",
    "    \n",
    "    T_main = ax_ei_stack[uid, main_ch, :]\n",
    "    idx_main = np.argmin(T_main) if np.min(T_main) < -np.max(T_main) else np.argmax(T_main)\n",
    "    \n",
    "    T_C = ax_ei_stack[uid, C, :]\n",
    "    idx_c = np.argmin(T_C) if np.min(T_C) < -np.max(T_C) else np.argmax(T_C)\n",
    "    \n",
    "    offsets.append(idx_c - idx_main)\n",
    "\n",
    "offsets_map = dict(zip(target_units, offsets))\n",
    "\n",
    "print(offsets_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates_413 = {}\n",
    "\n",
    "window_template = [-10,70]\n",
    "\n",
    "for uid in target_units:\n",
    "    T_C = ax_ei_stack[uid, C, :]  # template on 413\n",
    "    p2p = np.ptp(ax_ei_stack[uid], axis=1)\n",
    "    main_ch = np.argmax(p2p)\n",
    "    \n",
    "    T_main = ax_ei_stack[uid, main_ch, :]\n",
    "    idx_main = np.argmin(T_main) if np.min(T_main) < -np.max(T_main) else np.argmax(T_main)\n",
    "\n",
    "    center = idx_main + offsets_map[uid]\n",
    "\n",
    "    start = center + window_template[0]\n",
    "    end = center + window_template[1] + 1  # +1 because slicing is exclusive\n",
    "\n",
    "    if start < 0 or end > T_C.shape[0]:\n",
    "        print(f\"Skipping unit {uid}: template cut out of bounds [{start}:{end}]\")\n",
    "        continue\n",
    "\n",
    "    templates_413[uid] = T_C[start:end]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "x = np.arange(window_template[0], window_template[1]+1)  # sample offsets relative to center\n",
    "\n",
    "colors = {4: 'red', 54: 'blue', 59: 'green', 9: 'magenta'}\n",
    "\n",
    "for uid in templates_413:\n",
    "    T = templates_413[uid]\n",
    "    plt.plot(x[:len(T)], T, label=f'Unit {uid}', color=colors[uid], alpha=0.8)\n",
    "\n",
    "plt.xlabel(\"Samples relative to center\")\n",
    "plt.ylabel(\"ADC units\")\n",
    "plt.title(f\"Templates on channel {C} (aligned)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = window_end - window_start\n",
    "generated = np.zeros(chunk_len, dtype=np.float32)\n",
    "\n",
    "for uid in target_units:\n",
    "    spikes = units[uid]['spike_times']\n",
    "    template = templates_413[uid]\n",
    "    offset = offsets_map[uid]\n",
    "    \n",
    "    # Filter spikes in chunk\n",
    "    spikes_in_chunk = spikes[(spikes >= window_start) & (spikes < window_end)]\n",
    "    \n",
    "    for t_spike in spikes_in_chunk:\n",
    "        arrival = t_spike + offset\n",
    "        chunk_start_idx = int(arrival + window_template[0] - window_start)\n",
    "        chunk_end_idx = chunk_start_idx + len(template)\n",
    "\n",
    "        # Check bounds\n",
    "        if chunk_start_idx < 0 or chunk_end_idx > chunk_len:\n",
    "            print(f\"Skipping spike at {t_spike} (would go out of bounds)\")\n",
    "            continue\n",
    "\n",
    "        # Add template\n",
    "        generated[chunk_start_idx:chunk_end_idx] += template\n",
    "\n",
    "raw_snip = raw_data[window_start:window_end, C].astype(np.float32)\n",
    "\n",
    "residual = raw_snip - generated\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Invert residual to find negative peaks as positive peaks\n",
    "peaks, _ = find_peaks(-residual)\n",
    "\n",
    "# Get peak values\n",
    "neg_peak_values = residual[peaks]\n",
    "\n",
    "# Keep only the negative ones (should be all negative by construction, but safe check)\n",
    "neg_peak_values = neg_peak_values[neg_peak_values < 0]\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(neg_peak_values, bins=100, color='black', alpha=0.7)\n",
    "plt.xlabel(\"Negative peak residual (ADC units)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Distribution of negative residual peaks (channel {C})\")\n",
    "plt.ylim(0, 25)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.hist(residual, bins=100, color='black', alpha=0.7)\n",
    "# plt.xlabel(\"Residual (ADC units)\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.title(f\"Residual distribution over {window_end - window_start} samples on channel {C}\")\n",
    "# plt.ylim(0, 200)  # set y-axis limit to 10k\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = -100\n",
    "\n",
    "# peaks: indices of peaks in residual (relative to chunk)\n",
    "# residual[peaks]: their values\n",
    "\n",
    "# Find peaks that are below threshold\n",
    "mask = residual[peaks] < threshold\n",
    "peak_indices_below_thresh = peaks[mask]\n",
    "\n",
    "# Convert to original sample indices\n",
    "sample_indices_below_thresh = peak_indices_below_thresh + window_start\n",
    "\n",
    "print(f\"Found {len(sample_indices_below_thresh)} negative peaks below {threshold}\")\n",
    "\n",
    "\n",
    "# for s_idx, val in zip(sample_indices_below_thresh, residual[peak_indices_below_thresh]):\n",
    "#     print(f\"Sample {s_idx}: residual peak {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "win_pre = 100\n",
    "win_post = 100\n",
    "snippet_len = win_pre + win_post + 1\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, s_idx in enumerate(sample_indices_below_thresh):\n",
    "    start = s_idx - win_pre\n",
    "    end = s_idx + win_post + 1  # +1 because slice end is exclusive\n",
    "\n",
    "    # Check bounds\n",
    "    if start < 0 or end > raw_data.shape[0]:\n",
    "        print(f\"Skipping out-of-bounds snippet at sample {s_idx}\")\n",
    "        continue\n",
    "\n",
    "    snippet = raw_data[start:end, C].astype(np.float32)\n",
    "    x = np.arange(-win_pre, win_post + 1)\n",
    "\n",
    "    plt.plot(x, snippet, alpha=0.7, label=f\"Peak at {s_idx}\")\n",
    "\n",
    "plt.xlabel(\"Samples relative to peak\")\n",
    "plt.ylabel(\"ADC units\")\n",
    "plt.title(f\"Raw data snippets around negative residual peaks (channel {C})\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Optional: limit legend to avoid clutter if many peaks\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "if len(handles) > 10:\n",
    "    plt.legend(handles[:10], labels[:10], title=\"First 10 peaks\")\n",
    "else:\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for s_idx in sample_indices_below_thresh:\n",
    "    start = s_idx - win_pre\n",
    "    end = s_idx + win_post + 1\n",
    "\n",
    "    # Check bounds\n",
    "    if start < 0 or end > len(residual):\n",
    "        print(f\"Skipping out-of-bounds residual snippet at {s_idx}\")\n",
    "        continue\n",
    "\n",
    "    snippet = residual[start:end]\n",
    "    x = np.arange(-win_pre, win_post + 1)\n",
    "\n",
    "    plt.plot(x, snippet, alpha=0.7)\n",
    "\n",
    "plt.xlabel(\"Samples relative to residual peak\")\n",
    "plt.ylabel(\"Residual (ADC units)\")\n",
    "plt.title(f\"Residual snippets centered on negative residual peaks (channel {C})\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "win_pre_pca = 25\n",
    "win_post_pca = 25\n",
    "snip_len_pca = win_pre_pca + win_post_pca + 1\n",
    "\n",
    "snippets = []\n",
    "\n",
    "for s_idx in sample_indices_below_thresh:\n",
    "    start = s_idx - win_pre_pca\n",
    "    end = s_idx + win_post_pca + 1\n",
    "\n",
    "    if start < 0 or end > len(residual):\n",
    "        print(f\"Skipping out-of-bounds residual snippet at {s_idx}\")\n",
    "        continue\n",
    "\n",
    "    snippet = residual[start:end]\n",
    "    snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_snippets, 51]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pcs = pca.fit_transform(snippets)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(pcs[:, 0], pcs[:, 1], alpha=0.7, color='black')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA of residual snippets around negative peaks')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verify_cluster import verify_cluster\n",
    "\n",
    "# parameters for verify_cluster\n",
    "params = {\n",
    "    'window': (-20, 60),\n",
    "    'min_spikes': 100,\n",
    "    'ei_sim_threshold': 0.75,\n",
    "    'k_start': 4,\n",
    "    'k_refine': 2\n",
    "}\n",
    "\n",
    "# choose cell ID (this is index of the cell, not vision ID)\n",
    "spike_times = sample_indices_below_thresh\n",
    "\n",
    "# run recursive clustering\n",
    "clusters = verify_cluster(\n",
    "    spike_times=spike_times,\n",
    "    dat_path=dat_path,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "print(f\"Returned {len(clusters)} clean subclusters\")\n",
    "for i, cl in enumerate(clusters):\n",
    "    print(f\"  Cluster {i}: {len(cl['inds'])} spikes\")\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze_clusters\n",
    "import importlib\n",
    "importlib.reload(analyze_clusters)\n",
    "\n",
    "# plot EI (Vision style), ISI, firing rate, time course (from one pixel) and STA (single frame with strongest pixel)\n",
    "analyze_clusters.analyze_clusters(clusters,\n",
    "                 spike_times=spike_times,\n",
    "                 sampling_rate=20000,\n",
    "                 dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "                 h5_path='/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5',\n",
    "                 triggers_mat_path='/Volumes/Lab/Users/alexth/axolotl/trigger_in_samples_201703151.mat',\n",
    "                 cluster_ids=None,\n",
    "                 lut=None,\n",
    "                 sta_depth=30,\n",
    "                 sta_offset=0,\n",
    "                 sta_chunk_size=1000,\n",
    "                 sta_refresh=2,\n",
    "                 ei_scale=3,\n",
    "                 ei_cutoff=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plot_ei_waveforms\n",
    "import importlib\n",
    "importlib.reload(plot_ei_waveforms)\n",
    "\n",
    "snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_fast_ram(\n",
    "    raw_data=raw_data,\n",
    "    spike_times=sample_indices_below_thresh,\n",
    "    selected_channels=np.arange(n_channels),\n",
    "    window=window,\n",
    ")\n",
    "\n",
    "final_ei = np.mean(snips_full, axis=2)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_ei_waveforms.plot_ei_waveforms(final_ei, ei_positions, scale=70.0, box_height=1.0, box_width=50.0, colors='black')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim = compare_eis(ax_ei_stack, final_ei, 10).reshape(-1) \n",
    "best_idx = np.argmax(sim)\n",
    "best_sim = sim[best_idx]\n",
    "\n",
    "print(f\"Closest Axolotl unit {best_idx} with similarity {best_sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "from scipy.io import loadmat\n",
    "\n",
    "triggers_mat_path='/Volumes/Lab/Users/alexth/axolotl/trigger_in_samples_201703151.mat'\n",
    "triggers_sec = loadmat(triggers_mat_path)['triggers'].flatten()\n",
    "\n",
    "axolotl_utils_ram.analyze_single_unit(spike_samples = sample_indices_below_thresh,\n",
    "                        sampling_rate = 20000,\n",
    "                        triggers_sec = triggers_sec,\n",
    "                        ei = final_ei,\n",
    "                        ei_positions = ei_positions,\n",
    "                        lut=None,\n",
    "                        sta_depth=20,\n",
    "                        sta_offset=0,\n",
    "                        sta_chunk_size=1000,\n",
    "                        sta_refresh=2,\n",
    "                        ei_scale=3,\n",
    "                        ei_cutoff=0.05,\n",
    "                        isi_max_ms=200,\n",
    "                        sigma_ms=2500,\n",
    "                        dt_ms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_peaks = []\n",
    "\n",
    "for peak_sample in sample_indices_below_thresh:\n",
    "    found = False\n",
    "    for uid in target_units:\n",
    "        spikes = units[uid]['spike_times']\n",
    "        offset = offsets_map[uid]\n",
    "        arrivals = spikes + offset\n",
    "        \n",
    "        # Check if any arrival is within ±1 of peak_sample\n",
    "        if np.any(np.abs(arrivals - peak_sample) <= 2):\n",
    "            found = True\n",
    "            print(f\"Residual peak at {peak_sample} explained by unit {uid}\")\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        print(f\"Residual peak at {peak_sample} NOT explained by any target unit\")\n",
    "    \n",
    "    explained_peaks.append(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_len = window_end - window_start\n",
    "generated = np.zeros(chunk_len, dtype=np.float32)\n",
    "\n",
    "for uid in target_units:\n",
    "    spikes = units[uid]['spike_times']\n",
    "    template = templates_413[uid]\n",
    "    offset = offsets_map[uid]\n",
    "    \n",
    "    # Filter spikes in chunk\n",
    "    spikes_in_chunk = spikes[(spikes >= window_start) & (spikes < window_end)]\n",
    "    \n",
    "    for t_spike in spikes_in_chunk:\n",
    "        arrival = t_spike + offset\n",
    "        chunk_start_idx = int(arrival + window_template[0] - window_start)\n",
    "        chunk_end_idx = chunk_start_idx + len(template)\n",
    "\n",
    "        # Check bounds\n",
    "        if chunk_start_idx < 0 or chunk_end_idx > chunk_len:\n",
    "            print(f\"Skipping spike at {t_spike} (would go out of bounds)\")\n",
    "            continue\n",
    "\n",
    "        # Add template\n",
    "        generated[chunk_start_idx:chunk_end_idx] += template\n",
    "\n",
    "raw_snip = raw_data[window_start:window_end, C].astype(np.float32)\n",
    "x = np.arange(window_start, window_end)\n",
    "\n",
    "\n",
    "residual = raw_snip - generated\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(x, raw_snip, label='Original raw data', color='black', linewidth=0.5)\n",
    "plt.plot(x, residual, label='Original raw data', color='green', linewidth=0.5)\n",
    "plt.plot(x, generated, label='Generated from templates', color='red', linewidth=0.7)\n",
    "\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"ADC units\")\n",
    "plt.title(f\"Original vs generated signal on channel {C}\")\n",
    "plt.legend()\n",
    "plt.xlim(2462-100, 2462+100)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = raw_snip - generated\n",
    "plt.figure(figsize=(12, 4))\n",
    "x = np.arange(window_start, window_end)\n",
    "\n",
    "plt.plot(x, residual, label='Residual (raw - generated)', color='black', linewidth=0.7)\n",
    "\n",
    "# Colors for units\n",
    "colors = {4: 'red', 54: 'blue', 59: 'green', 9: 'magenta'}\n",
    "\n",
    "# Mark spikes on residual\n",
    "for uid in target_units:\n",
    "    spikes = units[uid]['spike_times']\n",
    "    spikes_in_chunk = spikes[(spikes >= window_start) & (spikes < window_end)]\n",
    "    for s in spikes_in_chunk:\n",
    "        plt.axvline(s, color=colors[uid], linestyle='--', alpha=0.5, label=f'Unit {uid}')\n",
    "\n",
    "# Remove duplicate labels\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "unique = dict(zip(labels, handles))\n",
    "plt.legend(unique.values(), unique.keys())\n",
    "\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Residual (ADC units)\")\n",
    "plt.title(f\"Residuals on channel {C} with spike locations marked\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "# Use 'valid' to avoid padding artifacts, or 'same' to keep same length\n",
    "residual_ma = np.convolve(residual, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "x = np.arange(window_start, window_end)\n",
    "\n",
    "plt.plot(x, residual_ma, label=f'Residual moving avg (win={window_size})', color='black', linewidth=1.0)\n",
    "\n",
    "# Mark spikes\n",
    "for uid in target_units:\n",
    "    spikes = units[uid]['spike_times']\n",
    "    spikes_in_chunk = spikes[(spikes >= window_start) & (spikes < window_end)]\n",
    "    for s in spikes_in_chunk:\n",
    "        plt.axvline(s, color=colors[uid], linestyle='--', alpha=0.5, label=f'Unit {uid}')\n",
    "\n",
    "# Remove duplicate labels\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "unique = dict(zip(labels, handles))\n",
    "plt.legend(unique.values(), unique.keys())\n",
    "\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Residual (ADC units)\")\n",
    "plt.title(f\"Residual + moving avg on channel {C}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_ei_stack = np.stack(ax_ei_list, axis=0)\n",
    "\n",
    "print(ax_ei_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "\n",
    "units = axolotl_utils_ram.load_units_from_h5(h5_out_path)\n",
    "\n",
    "# Get all unit IDs (sorted, to ensure order is clear)\n",
    "unit_ids = sorted(units.keys())\n",
    "num_units = len(unit_ids)\n",
    "\n",
    "# Get shape info from first EI\n",
    "example_ei = units[unit_ids[0]]['ei']\n",
    "num_channels, num_timepoints = example_ei.shape\n",
    "\n",
    "# Allocate array\n",
    "ax_ei_stack = np.zeros((num_units, num_channels, num_timepoints), dtype=np.float32)\n",
    "\n",
    "# Fill it\n",
    "for i, unit_id in enumerate(unit_ids):\n",
    "    ax_ei_stack[i] = units[unit_id]['ei']\n",
    "\n",
    "\n",
    "p2p = ax_ei_stack.max(axis=2) - ax_ei_stack.min(axis=2)  # [93, 512]\n",
    "\n",
    "pairs = axolotl_utils_ram.candidate_pairs_simple(p2p, thr=0.1)\n",
    "\n",
    "# print(pairs)\n",
    "\n",
    "# Find max unit ID (in case unit IDs are sparse or out of order)\n",
    "max_unit_id = max(units.keys())\n",
    "\n",
    "# Build list where spike_times_list[unit_id] = spike_times\n",
    "# Use None or empty array for missing units if any\n",
    "spike_times_list = [None] * (max_unit_id + 1)\n",
    "\n",
    "for unit_id, data in units.items():\n",
    "    spike_times_list[unit_id] = data['spike_times']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = 2\n",
    "\n",
    "A, B, shared = pairs[cell_id]\n",
    "C = shared[5]\n",
    "\n",
    "tA = spike_times_list[A]\n",
    "tB = spike_times_list[B]\n",
    "\n",
    "T_A_C = ax_ei_stack[A, C, :]\n",
    "T_B_C = ax_ei_stack[B, C, :]\n",
    "\n",
    "pkA = np.argmin(T_A_C) if np.min(T_A_C) < -np.max(T_A_C) else np.argmax(T_A_C)\n",
    "pkB = np.argmin(T_B_C) if np.min(T_B_C) < -np.max(T_B_C) else np.argmax(T_B_C)\n",
    "\n",
    "hitsA, hitsB = axolotl_utils_ram.find_collisions_1ch(tA, tB, pkA, pkB, delta=15)\n",
    "\n",
    "print(len(hitsA))\n",
    "# print(\"Collision spike indices in A:\", hitsA)\n",
    "# print(\"Collision spike indices in B:\", hitsB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soloA, soloB = axolotl_utils_ram.find_solo_1ch(tA, tB, pkA, pkB, delta=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p_A = np.ptp(ax_ei_stack[A], axis=1)  # [channels]\n",
    "main_ch_A = np.argmax(p2p_A)\n",
    "T_A_main = ax_ei_stack[A, main_ch_A, :]\n",
    "idx_main_A = np.argmin(T_A_main) if np.min(T_A_main) < -np.max(T_A_main) else np.argmax(T_A_main)\n",
    "T_A_C = ax_ei_stack[A, C, :]\n",
    "idx_c_A = np.argmin(T_A_C) if np.min(T_A_C) < -np.max(T_A_C) else np.argmax(T_A_C)\n",
    "offset_A = idx_c_A - idx_main_A\n",
    "\n",
    "print(offset_A)\n",
    "\n",
    "p2p_B = np.ptp(ax_ei_stack[B], axis=1)  # [channels]\n",
    "main_ch_B = np.argmax(p2p_B)\n",
    "T_B_main = ax_ei_stack[B, main_ch_B, :]\n",
    "idx_main_B = np.argmin(T_B_main) if np.min(T_B_main) < -np.max(T_B_main) else np.argmax(T_B_main)\n",
    "T_B_C = ax_ei_stack[B, C, :]\n",
    "idx_c_B = np.argmin(T_B_C) if np.min(T_B_C) < -np.max(T_B_C) else np.argmax(T_B_C)\n",
    "offset_B = idx_c_B - idx_main_B\n",
    "\n",
    "print(offset_B)\n",
    "\n",
    "win_pre = 40\n",
    "win_post = 40\n",
    "win_len = win_pre + win_post  # 80\n",
    "\n",
    "raw_snippets = []  # for storing y\n",
    "template_As = []   # for storing aligned A templates\n",
    "template_Bs = []   # for storing aligned B templates\n",
    "shifts = []        # for storing shift values\n",
    "\n",
    "# For A\n",
    "snipsA, _ = axolotl_utils_ram.extract_snippets_fast_ram(\n",
    "    raw_data=raw_data,\n",
    "    spike_times=tA + offset_A,\n",
    "    selected_channels=np.array([C]),\n",
    "    window=(-win_pre, win_post - 1),  # 80 samples\n",
    ")\n",
    "template_A = snipsA[0].mean(axis=1)  # [80]\n",
    "\n",
    "# For B — larger window for shifting\n",
    "snipsB, _ = axolotl_utils_ram.extract_snippets_fast_ram(\n",
    "    raw_data=raw_data,\n",
    "    spike_times=tB + offset_B,\n",
    "    selected_channels=np.array([C]),\n",
    "    window=(-win_pre - 30, win_post + 30 - 1),  # 80 + 60 = 140 samples\n",
    ")\n",
    "template_B_full = snipsB[0].mean(axis=1)  # [140]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example shift value: how much B's peak should move to align with A's\n",
    "# For example: let's align B's peak at sample 40\n",
    "desired_shift = 0  # no shift → peak at 40\n",
    "# desired_shift = 10  # shift B's peak to 40 + 10 = 50\n",
    "# desired_shift = -10  # shift B's peak to 40 - 10 = 30\n",
    "\n",
    "# Determine cut indices\n",
    "start_B = 30 - desired_shift\n",
    "end_B = start_B + 80\n",
    "\n",
    "if start_B < 0 or end_B > len(template_B_full):\n",
    "    raise ValueError(f\"Shift {desired_shift} causes out-of-bounds slice: [{start_B}:{end_B}]\")\n",
    "\n",
    "TB = template_B_full[start_B:end_B]\n",
    "\n",
    "x = np.arange(80)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(x, template_A, label='Template A (centered)', color='blue')\n",
    "plt.plot(x, TB, label=f'Template B (shifted by {desired_shift} samples)', color='red')\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"ADC units\")\n",
    "plt.title(\"Average templates overlay (aligned peak position)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for k in range(len(hitsA)):\n",
    "    # 1️⃣ Spike times\n",
    "    tA_hit = tA[hitsA[k]]\n",
    "    tB_hit = tB[hitsB[k]]\n",
    "\n",
    "    # 2️⃣ Compute arrival times at C\n",
    "    centerA = tA_hit + offset_A\n",
    "    centerB = tB_hit + offset_B\n",
    "\n",
    "    # 3️⃣ Compute shift\n",
    "    shift = centerB - centerA\n",
    "    shifts.append(shift)\n",
    "\n",
    "    # 4️⃣ Extract raw data snippet centered on centerA\n",
    "    start = centerA - win_pre\n",
    "    end = centerA + win_post\n",
    "\n",
    "    if start < 0 or end >= raw_data.shape[0]:\n",
    "        print(f\"Skipping out-of-bounds collision at k={k}\")\n",
    "        continue\n",
    "\n",
    "    y = raw_data[start:end, C].astype(np.float32)\n",
    "    raw_snippets.append(y)\n",
    "\n",
    "    # 5️⃣ Build template A (centered)\n",
    "    # A's template already has peak at 40\n",
    "    template_As.append(template_A)  # template_A = your 80-sample mean template for A\n",
    "\n",
    "    # 6️⃣ Build template B (shifted)\n",
    "    shift_int = int(round(shift))\n",
    "    start_B = 30 - shift_int\n",
    "    end_B = start_B + win_len\n",
    "\n",
    "    if start_B < 0 or end_B > template_B_full.shape[0]:\n",
    "        print(f\"Skipping B template out-of-bounds at k={k}\")\n",
    "        continue\n",
    "\n",
    "    TB_aligned = template_B_full[start_B:end_B]\n",
    "    template_Bs.append(TB_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "k_plot = 3  # index of the collision pair you want to plot\n",
    "\n",
    "TA = template_As[k_plot]\n",
    "TB = template_Bs[k_plot]\n",
    "\n",
    "x = np.arange(len(TA))  # sample positions (0..79)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(x, TA, label='Template A (centered)', color='red')\n",
    "plt.plot(x, TB, label='Template B (shifted)', color='blue')\n",
    "plt.plot(x, raw_snippets[k_plot], label='raw', color='black')\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"ADC units\")\n",
    "plt.title(f\"Collision template overlay at k={k_plot}\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "alphas = []\n",
    "betas = []\n",
    "residuals = []\n",
    "\n",
    "for y, TA, TB in zip(raw_snippets, template_As, template_Bs):\n",
    "    alpha, beta, rnorm = axolotl_utils_ram.fit_two_templates_bounded(y, TA, TB, lower=0.75, upper=1.25)\n",
    "    alphas.append(alpha)\n",
    "    betas.append(beta)\n",
    "    residuals.append(rnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_idx = 39\n",
    "\n",
    "y = raw_snippets[spike_idx]\n",
    "TA = template_As[spike_idx]\n",
    "TB = template_Bs[spike_idx]\n",
    "alpha = alphas[spike_idx]\n",
    "beta = betas[spike_idx]\n",
    "rnorm = residuals[spike_idx]\n",
    "\n",
    "fit = alpha * TA + beta * TB\n",
    "residual = y - fit\n",
    "\n",
    "x = np.arange(len(y))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(x, y, color='green', label='Raw data')\n",
    "plt.plot(x, alpha * TA, color='red', label=f'Template A (α={alpha:.2f})')\n",
    "plt.plot(x, beta * TB, color='blue', label=f'Template B (β={beta:.2f})')\n",
    "plt.plot(x, residual, color='black', label=f'Residual (norm={rnorm:.2f})')\n",
    "\n",
    "plt.title(f'Collision fit at spike index {spike_idx}')\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"ADC units\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Alpha histogram\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.hist(alphas, bins=50, color='red', alpha=0.7)\n",
    "plt.xlabel('Alpha (A amplitude)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of alpha')\n",
    "\n",
    "# Beta histogram\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.hist(betas, bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Beta (B amplitude)')\n",
    "plt.title('Distribution of beta')\n",
    "\n",
    "# Residual norm histogram\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.hist(residuals, bins=50, color='black', alpha=0.7)\n",
    "plt.title(f\"Residuals {len(residuals)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert to arrays if they aren't already\n",
    "alphas = np.array(alphas)\n",
    "betas = np.array(betas)\n",
    "residuals = np.array(residuals)\n",
    "\n",
    "# Identify fits where alpha and beta are not at bounds\n",
    "bound_tol = 1e-6  # tolerance to consider \"at bound\"\n",
    "alpha_not_bound = (alphas > (0.75 + bound_tol)) & (alphas < (1.25 - bound_tol))\n",
    "beta_not_bound = (betas > (0.75 + bound_tol)) & (betas < (1.25 - bound_tol))\n",
    "\n",
    "# Combined mask\n",
    "valid_mask = alpha_not_bound & beta_not_bound\n",
    "\n",
    "# Extract residuals for these cases\n",
    "residuals_valid = residuals[valid_mask]\n",
    "\n",
    "# Plot\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.hist(residuals_valid, bins=50, color='black', alpha=0.7)\n",
    "plt.xlabel('Residual norm')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f\"Residuals {len(residuals_valid)}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "\n",
    "for k in range(len(soloA)):\n",
    "    # 1️⃣ Spike times\n",
    "    tA_hit = tA[soloA[k]]\n",
    "\n",
    "    # 2️⃣ Compute arrival times at C\n",
    "    centerA = tA_hit + offset_A\n",
    "\n",
    "    # 4️⃣ Extract raw data snippet centered on centerA\n",
    "    start = centerA - win_pre\n",
    "    end = centerA + win_post\n",
    "\n",
    "    if start < 0 or end >= raw_data.shape[0]:\n",
    "        print(f\"Skipping out-of-bounds collision at k={k}\")\n",
    "        continue\n",
    "\n",
    "    y = raw_data[start:end, C].astype(np.float32)\n",
    "    raw_snippets.append(y)\n",
    "\n",
    "    # 5️⃣ Build template A (centered)\n",
    "    # A's template already has peak at 40\n",
    "    template_As.append(template_A)  # template_A = your 80-sample mean template for A\n",
    "\n",
    "\n",
    "template_Bs = [np.zeros_like(template_A) for _ in range(len(template_As))]\n",
    "\n",
    "alphas = []\n",
    "betas = []\n",
    "residuals = []\n",
    "\n",
    "for y, TA, TB in zip(raw_snippets, template_As, template_Bs):\n",
    "    alpha, beta, rnorm = axolotl_utils_ram.fit_two_templates_bounded(y, TA, TB, lower=0.75, upper=1.25)\n",
    "    alphas.append(alpha)\n",
    "    betas.append(beta)\n",
    "    residuals.append(rnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_idx = 12\n",
    "\n",
    "y = raw_snippets[spike_idx]\n",
    "TA = template_As[spike_idx]\n",
    "TB = template_Bs[spike_idx]\n",
    "alpha = alphas[spike_idx]\n",
    "beta = betas[spike_idx]\n",
    "rnorm = residuals[spike_idx]\n",
    "\n",
    "fit = alpha * TA + beta * TB\n",
    "residual = y - fit\n",
    "\n",
    "x = np.arange(len(y))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(x, y, color='green', label='Raw data')\n",
    "plt.plot(x, alpha * TA, color='red', label=f'Template A (α={alpha:.2f})')\n",
    "plt.plot(x, beta * TB, color='blue', label=f'Template B (β={beta:.2f})')\n",
    "plt.plot(x, residual, color='black', label=f'Residual (norm={rnorm:.2f})')\n",
    "\n",
    "plt.title(f'Collision fit at spike index {spike_idx}')\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"ADC units\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "template_B = template_B_full[30:110]\n",
    "\n",
    "raw_snippets = []\n",
    "template_Bs = []\n",
    "template_As = []\n",
    "\n",
    "for k in range(len(soloB)):\n",
    "    tB_hit = tB[soloB[k]]\n",
    "    centerB = tB_hit + offset_B\n",
    "\n",
    "    start = centerB - win_pre\n",
    "    end = centerB + win_post\n",
    "\n",
    "    if start < 0 or end >= raw_data.shape[0]:\n",
    "        print(f\"Skipping out-of-bounds solo B at k={k}\")\n",
    "        continue\n",
    "\n",
    "    y = raw_data[start:end, C].astype(np.float32)\n",
    "    raw_snippets.append(y)\n",
    "\n",
    "    template_Bs.append(template_B)  # your 80-sample mean template for B\n",
    "    template_As.append(np.zeros_like(template_B))  # zero A template\n",
    "\n",
    "alphas = []\n",
    "betas = []\n",
    "residuals = []\n",
    "\n",
    "for y, TA, TB in zip(raw_snippets, template_As, template_Bs):\n",
    "    alpha, beta, rnorm = axolotl_utils_ram.fit_two_templates_bounded(y, TA, TB, lower=0.75, upper=1.25)\n",
    "    alphas.append(alpha)\n",
    "    betas.append(beta)\n",
    "    residuals.append(rnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Alpha histogram\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.hist(alphas, bins=50, color='red', alpha=0.7)\n",
    "plt.xlabel('Alpha (A amplitude)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of alpha')\n",
    "\n",
    "# Beta histogram\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.hist(betas, bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Beta (B amplitude)')\n",
    "plt.title('Distribution of beta')\n",
    "\n",
    "# Residual norm histogram\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.hist(residuals, bins=50, color='black', alpha=0.7)\n",
    "plt.title(f\"Residuals {len(residuals)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert to arrays if they aren't already\n",
    "alphas = np.array(alphas)\n",
    "betas = np.array(betas)\n",
    "residuals = np.array(residuals)\n",
    "\n",
    "# Identify fits where alpha and beta are not at bounds\n",
    "bound_tol = 1e-6  # tolerance to consider \"at bound\"\n",
    "alpha_not_bound = (alphas > (0.75 + bound_tol)) & (alphas < (1.25 - bound_tol))\n",
    "beta_not_bound = (betas > (0.75 + bound_tol)) & (betas < (1.25 - bound_tol))\n",
    "\n",
    "# Combined mask\n",
    "valid_mask = alpha_not_bound & beta_not_bound\n",
    "\n",
    "# Extract residuals for these cases\n",
    "residuals_valid = residuals[valid_mask]\n",
    "\n",
    "# Plot\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.hist(residuals_valid, bins=50, color='black', alpha=0.7)\n",
    "plt.xlabel('Residual norm')\n",
    "plt.ylabel('Count')\n",
    "plt.title(f\"Residuals {len(residuals_valid)}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "win_pre = 50\n",
    "win_post = 50\n",
    "win_len = win_pre + win_post + 1\n",
    "\n",
    "# Pick a few hitsA indices to inspect (e.g. first 5)\n",
    "for idx in hitsA[:5]:\n",
    "    center = tA[idx]\n",
    "    \n",
    "    start = center - win_pre\n",
    "    end = center + win_post + 1  # +1 because Python slicing is exclusive\n",
    "    \n",
    "    if start < 0 or end > raw_data.shape[0]:\n",
    "        print(f\"Skipping spike at {center} (out of bounds)\")\n",
    "        continue\n",
    "    \n",
    "    trace = raw_data[start:end, C]\n",
    "    \n",
    "    plt.figure(figsize=(8, 2))\n",
    "    plt.plot(np.arange(-win_pre, win_post + 1), trace, color='black')\n",
    "    plt.title(f\"Unit A spike at {center} on channel {C}\")\n",
    "    plt.xlabel(\"Samples relative to center\")\n",
    "    plt.ylabel(\"ADC units\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random debug stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import chi2\n",
    "thr_vis  = chi2.ppf(0.9,  df=2)     # illustration\n",
    "thr_cut  = chi2.ppf(0.9, df=2)     # discard\n",
    "\n",
    "final_outlier_inds      = np.where(d2 > thr_vis)[0]\n",
    "final_outlier_inds_max  = np.where(d2 > thr_cut)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"LOF rejects: {len(lof_inds)}\")\n",
    "\n",
    "final_outlier_inds      = lof_inds\n",
    "\n",
    "\n",
    "# Assume labels_kmeans contains the 0/1 cluster assignments from k-means on best_proj\n",
    "# And pcs is your [N, 2] PC1/PC2 array (z-scored)\n",
    "pcs = pcs_z\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(pcs[:, 0], pcs[:, 1], s=5, alpha=0.7)\n",
    "plt.scatter(pcs[final_outlier_inds_max, 0], pcs[final_outlier_inds_max, 1], s=15, color=\"green\",alpha=1)\n",
    "plt.scatter(pcs[final_outlier_inds, 0], pcs[final_outlier_inds, 1], s=3, color=\"red\",alpha=1)\n",
    "plt.xlabel(\"PC1 (z-score)\")\n",
    "plt.ylabel(\"PC2 (z-score)\")\n",
    "plt.title(f\"PC1 vs PC2 scatter\\Hartigan p = {p:.3f}\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(pc1_z, pc2_z, s=2, alpha=0.5)\n",
    "plt.title(\"PC1 z-scores for Ref Channel Waveforms\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "thresh_pc = 6\n",
    "final_outlier_mask_max = (np.abs(pc1_z) > thresh_pc) | (np.abs(pc2_z) > thresh_pc) # or -2.5, tune this\n",
    "final_outlier_inds_max = np.where(final_outlier_mask_max)[0]\n",
    "\n",
    "print(len(final_outlier_inds_max))\n",
    "# print(final_outlier_inds_max)\n",
    "\n",
    "if final_outlier_inds_max.size > 0:\n",
    "    from scipy.spatial.distance import cdist\n",
    "\n",
    "    # Combine PC1 and PC2 z-scores\n",
    "    pcs_all = np.column_stack((pc1_z, pc2_z))\n",
    "\n",
    "    # Split into bad and good\n",
    "    bad_pcs = pcs_all[final_outlier_inds_max]\n",
    "    good_mask = np.ones(len(pcs_all), dtype=bool)\n",
    "    good_mask[final_outlier_inds_max] = False\n",
    "    good_pcs = pcs_all[good_mask]\n",
    "\n",
    "    N = min(20, len(good_pcs))\n",
    "\n",
    "    bad_good_means = []\n",
    "    good_good_means = []\n",
    "    ratios = []\n",
    "\n",
    "    for bad in bad_pcs:\n",
    "        # Distance to all good spikes\n",
    "        dists = np.linalg.norm(good_pcs - bad, axis=1)\n",
    "        nearest_inds = np.argsort(dists)[:N]\n",
    "        nearest_good = good_pcs[nearest_inds]\n",
    "\n",
    "        # Mean distance bad -> nearest N good\n",
    "        mean_bad_good = np.mean(dists[nearest_inds])\n",
    "        bad_good_means.append(mean_bad_good)\n",
    "\n",
    "        # Mean pairwise distance among those N good spikes\n",
    "        gg_dists = cdist(nearest_good, nearest_good)\n",
    "        mean_good_good = np.mean(gg_dists[np.triu_indices_from(gg_dists, k=1)])\n",
    "        good_good_means.append(mean_good_good)\n",
    "\n",
    "        # Ratio\n",
    "        ratios.append(mean_bad_good / (mean_good_good + 1e-8))\n",
    "\n",
    "    bad_good_means = np.array(bad_good_means)\n",
    "    good_good_means = np.array(good_good_means)\n",
    "    ratios = np.array(ratios)\n",
    "    # Summary\n",
    "    print(f\"Mean bad-good distance: {np.mean(bad_good_means):.4f}\")\n",
    "    print(f\"Mean good-good distance: {np.mean(good_good_means):.4f}\")\n",
    "    print(f\"Mean ratio (bad-good / good-good): {np.mean(ratios):.4f}\")\n",
    "\n",
    "    # Optional: print distribution\n",
    "    # for i, (bg, gg, r) in enumerate(zip(bad_good_means, good_good_means, ratios)):\n",
    "    #     print(f\"Bad spike {i}: bad-good = {bg:.4f}, good-good = {gg:.4f}, ratio = {r:.2f}\")\n",
    "    # final_outlier_inds_max = final_outlier_inds_max[ratios>3.5]\n",
    "\n",
    "N = snips_baselined.shape[2] # or however you define total spike count\n",
    "labels = np.zeros(N, dtype=int)\n",
    "thresh_pc = 2\n",
    "final_outlier_mask = (np.abs(pc1_z) > thresh_pc) | (np.abs(pc2_z) > thresh_pc) # or -2.5, tune this\n",
    "final_outlier_inds = np.where(final_outlier_mask)[0]\n",
    "\n",
    "labels[final_outlier_inds] = 1\n",
    "\n",
    "thresh_pc = 6\n",
    "final_outlier_mask_max = (np.abs(pc1_z) > thresh_pc) | (np.abs(pc2_z) > thresh_pc) # or -2.5, tune this\n",
    "final_outlier_inds_max = np.where(final_outlier_mask_max)[0]\n",
    "\n",
    "labels[final_outlier_inds_max] = 2\n",
    "\n",
    "merged_clusters, sim, n_bad_channels = axolotl_utils_ram.merge_similar_clusters(snips_baselined, labels, max_lag=3, p2p_thresh=30.0, amp_thresh=-20, cos_thresh=0.9)\n",
    "\n",
    "print(sim)\n",
    "print(n_bad_channels)\n",
    "print(N)\n",
    "print(len(merged_clusters))\n",
    "for i, arr in enumerate(merged_clusters):\n",
    "    print(f\"Array {i} length: {len(arr)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verify_cluster import verify_cluster\n",
    "\n",
    "# parameters for verify_cluster\n",
    "params = {\n",
    "    'window': (-20, 60),\n",
    "    'min_spikes': 100,\n",
    "    'ei_sim_threshold': 0.75,\n",
    "    'k_start': 10,\n",
    "    'k_refine': 4\n",
    "}\n",
    "\n",
    "# choose cell ID (this is index of the cell, not vision ID)\n",
    "spike_times = spikes\n",
    "\n",
    "# run recursive clustering\n",
    "clusters = verify_cluster(\n",
    "    spike_times=spike_times,\n",
    "    dat_path=dat_path,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "print(f\"Returned {len(clusters)} clean subclusters\")\n",
    "for i, cl in enumerate(clusters):\n",
    "    print(f\"  Cluster {i}: {len(cl['inds'])} spikes\")\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze_clusters\n",
    "import importlib\n",
    "importlib.reload(analyze_clusters)\n",
    "\n",
    "# plot EI (Vision style), ISI, firing rate, time course (from one pixel) and STA (single frame with strongest pixel)\n",
    "analyze_clusters.analyze_clusters(clusters,\n",
    "                 spike_times=spike_times,\n",
    "                 sampling_rate=20000,\n",
    "                 dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "                 h5_path='/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5',\n",
    "                 triggers_mat_path='/Volumes/Lab/Users/alexth/axolotl/trigger_in_samples_201703151.mat',\n",
    "                 cluster_ids=None,\n",
    "                 lut=None,\n",
    "                 sta_depth=30,\n",
    "                 sta_offset=0,\n",
    "                 sta_chunk_size=1000,\n",
    "                 sta_refresh=2,\n",
    "                 ei_scale=3,\n",
    "                 ei_cutoff=0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END of main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "\n",
    "# --- PC1 vs PC2 scatter plot ---\n",
    "plt.figure(figsize=(6, 5))\n",
    "unique_labels = np.unique(labels_pre)\n",
    "print(unique_labels)\n",
    "colors = plt.cm.tab10.colors  # or any colormap you like\n",
    "\n",
    "for i, label in enumerate(unique_labels):\n",
    "    mask = labels_pre == label\n",
    "    color = colors[i % len(colors)]\n",
    "    plt.scatter(pcs_pre[mask, 0], pcs_pre[mask, 1], s=10, color=color, alpha=0.7, label=f\"Cluster {label}\")\n",
    "\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Cluster PCA Scatter\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Plot EIs for each cluster ---\n",
    "for i, ei in enumerate(cluster_eis_pre):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plot_ei_waveforms(\n",
    "        ei=ei,                 # list of EIs\n",
    "        positions=ei_positions,\n",
    "        ref_channel=ref_channel,\n",
    "        scale=70.0,\n",
    "        box_height=1.5,\n",
    "        box_width=50,\n",
    "        linewidth=0.5,\n",
    "        alpha=0.9,\n",
    "        colors='black'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 20000\n",
    "\n",
    "# --- Load spike times and electrode positions from HDF5 ---\n",
    "all_spikes = {}\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    unit_ids = sorted(f['/spikes'].keys(), key=lambda x: int(x.split('_')[1]))\n",
    "    for uid in unit_ids:\n",
    "        unit_index = int(uid.split('_')[1])\n",
    "        raw = f[f'/spikes/{uid}'][:]\n",
    "        if raw.ndim == 1 and raw.shape[0] == 1:\n",
    "            spikes_sec = np.array(raw[0]).flatten()\n",
    "        else:\n",
    "            spikes_sec = np.array(raw).flatten()\n",
    "        spikes_samples = np.round(spikes_sec * sampling_rate).astype(np.int32)\n",
    "        all_spikes[unit_index] = spikes_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_spikes = all_spikes[56]\n",
    "print(len(ks_spikes))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ref_channel = 51\n",
    "\n",
    "# Define window around each spike\n",
    "pre, post = 20, 60\n",
    "snip_len = pre + post + 1\n",
    "\n",
    "# Allocate array to hold all snippets\n",
    "snippets = []\n",
    "\n",
    "for s in ss:#ks_spikes[:1000]:\n",
    "    if s - pre >= 0 and s + post < raw_data.shape[0]:\n",
    "        snippet = raw_data[s - pre : s + post + 1, ref_channel]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot all snippets\n",
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='black', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "plt.title(f\"Ref channel {ref_channel} waveforms at {len(snippets)} spikes\")\n",
    "plt.xlabel(\"Sample index (relative to spike)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_spikes = all_spikes[56]\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ref_channel = 51\n",
    "amps = raw_data[ks_spikes, ref_channel]\n",
    "plt.hist(amps, bins=100)\n",
    "plt.axvline(-50, color='red', linestyle='--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_spikes = all_spikes[56]\n",
    "# ks_spikes = ks_spikes[amps > -50]\n",
    "\n",
    "print(len(ks_spikes))\n",
    "\n",
    "ref_channel = 51\n",
    "fs = 20000  # sampling rate in Hz\n",
    "segment_len = fs  # 1 second = 20,000 samples\n",
    "\n",
    "trace = raw_data[:, ref_channel].astype(np.float32)\n",
    "n_samples = len(trace)\n",
    "\n",
    "# Create baseline-corrected version of the trace\n",
    "trace_corrected = np.empty_like(trace)\n",
    "\n",
    "n_segments = (n_samples + segment_len - 1) // segment_len  # ceil division\n",
    "for i in range(n_segments):\n",
    "    start = i * segment_len\n",
    "    end = min(start + segment_len, n_samples)\n",
    "    segment = trace[start:end]\n",
    "    trace_corrected[start:end] = segment - np.mean(segment)\n",
    "\n",
    "# Now extract snippets from corrected trace\n",
    "pre, post = 20, 60\n",
    "snip_len = pre + post + 1\n",
    "snippets = []\n",
    "\n",
    "for s in ks_spikes[:1000]:\n",
    "    if s - pre >= 0 and s + post < trace_corrected.shape[0]:\n",
    "        snippet = trace_corrected[s - pre : s + post + 1]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 9))\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='black', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "plt.title(f\"Ref channel {ref_channel} waveforms at {len(snippets)} spikes\\n(1s segment-wise baseline subtraction)\")\n",
    "plt.xlabel(\"Sample index (relative to spike)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amps = trace_corrected[ks_spikes]\n",
    "plt.hist(amps, bins=100)\n",
    "plt.axvline(-50, color='red', linestyle='--')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "unit_id_1 = 1011\n",
    "max_diff = 10 # in samples\n",
    "\n",
    "# --- Load spike times ---\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    spikes_1 = h5[f'unit_{unit_id_1}']['spike_times'][:]\n",
    "\n",
    "\n",
    "spikes_2 = ks_spikes\n",
    "# --- Sort spike times (for efficiency) ---\n",
    "spikes_1 = np.sort(spikes_1)\n",
    "spikes_2 = np.sort(spikes_2)\n",
    "\n",
    "# --- Match spikes within max_diff ---\n",
    "matches = []\n",
    "j_start = 0\n",
    "\n",
    "for i, t1 in enumerate(spikes_1):\n",
    "    while j_start < len(spikes_2) and spikes_2[j_start] < t1 - max_diff:\n",
    "        j_start += 1\n",
    "\n",
    "    j = j_start\n",
    "    while j < len(spikes_2) and spikes_2[j] <= t1 + max_diff:\n",
    "        if abs(t1 - spikes_2[j]) <= max_diff:\n",
    "            matches.append((i, j))\n",
    "        j += 1\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"Found {len(matches)} matched spikes between unit {unit_id_1} and ks\")\n",
    "for i, j in matches:\n",
    "    print(f\"Spike1: {spikes_1[i]}, Spike2: {spikes_2[j]}\")\n",
    "\n",
    "# --- Build set of matched indices in spikes_2 ---\n",
    "matched_j = set(j for _, j in matches)\n",
    "\n",
    "# --- Extract unmatched spike times from spikes_2 ---\n",
    "unmatched_spikes_2 = [spikes_2[j] for j in range(len(spikes_2)) if j not in matched_j]\n",
    "\n",
    "print(f\"Found {len(unmatched_spikes_2)} unmatched spikes in unit ks\")\n",
    "unmatched_spikes_2 = np.array(unmatched_spikes_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from h5 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "unit_id = 40 # or whatever unit you're checking\n",
    "\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    group = h5[f'unit_{unit_id}']\n",
    "    \n",
    "    spike_times = group['spike_times'][:]\n",
    "    ei = group['ei'][:]\n",
    "    selected_channels = group['selected_channels'][:]\n",
    "    \n",
    "    peak_channel = group.attrs['peak_channel']\n",
    "\n",
    "# Check shapes or values\n",
    "print(\"Spike times:\", spike_times.shape)\n",
    "#print(\"EI shape:\", ei.shape)\n",
    "# print(\"Selected channels:\", selected_channels)\n",
    "# print(179 in selected_channels)\n",
    "#print(\"Peak channel:\", peak_channel)\n",
    "print(spike_times)\n",
    "\n",
    "\n",
    "unit_id = 11 # or whatever unit you're checking\n",
    "\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    group = h5[f'unit_{unit_id}']\n",
    "    \n",
    "    spike_times1 = group['spike_times'][:]\n",
    "    ei = group['ei'][:]\n",
    "    selected_channels = group['selected_channels'][:]\n",
    "    \n",
    "    peak_channel = group.attrs['peak_channel']\n",
    "\n",
    "# Check shapes or values\n",
    "print(\"Spike times:\", spike_times1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot one channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# ei = np.mean(snips_baselined, axis=2)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(ei[125,:], color='black', linewidth=1)\n",
    "plt.plot(ei1[125,:], color='red', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "ref_channel = 51\n",
    "\n",
    "# Define window around each spike\n",
    "pre, post = 20, 60\n",
    "snip_len = pre + post + 1\n",
    "\n",
    "# Allocate array to hold all snippets\n",
    "snippets = []\n",
    "\n",
    "for s in spikes_1[:50]:\n",
    "    if s - pre >= 0 and s + post < raw_data.shape[0]:\n",
    "        snippet = raw_data[s - pre : s + post + 1, ref_channel]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot all snippets\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='black', alpha=0.1, linewidth=0.5)\n",
    "\n",
    "\n",
    "snippets = []\n",
    "\n",
    "for s in unmatched_spikes_2[:50]:\n",
    "    if s - pre >= 0 and s + post < raw_data.shape[0]:\n",
    "        snippet = raw_data[s - pre : s + post + 1, ref_channel]\n",
    "        snippets.append(snippet)\n",
    "\n",
    "snippets = np.array(snippets)  # shape: [n_spikes, snip_len]\n",
    "\n",
    "# Plot all snippets\n",
    "for i in range(snippets.shape[0]):\n",
    "    plt.plot(snippets[i], color='red', alpha=0.1, linewidth=1)\n",
    "\n",
    "plt.title(f\"Ref channel {ref_channel} waveforms at {len(snippets)} spikes\")\n",
    "plt.xlabel(\"Sample index (relative to spike)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snips, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "    raw_data=raw_data,\n",
    "    spike_times=ks_spikes[mean_scores_at_spikes<20000],\n",
    "    window=window,\n",
    "    selected_channels=np.arange(512)\n",
    ")\n",
    "\n",
    "segment_len = 100_000\n",
    "snips_baselined = snips.copy()  # shape (n_channels, 81, N)\n",
    "n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "# Determine segment index for each spike\n",
    "segment_indices = valid_spike_times // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "# Loop through channels and subtract baseline per spike\n",
    "for ch in range(n_channels):\n",
    "    snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "ei = np.mean(snips_baselined, axis=2)\n",
    "#ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei,\n",
    "    positions=ei_positions,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,\n",
    "    box_width=50,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"EI\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "(\n",
    "spikes,\n",
    "mean_score,\n",
    "valid_score,\n",
    "mean_scores_at_spikes,\n",
    "valid_scores_at_spikes,\n",
    "mean_thresh,\n",
    "valid_thresh\n",
    ") = axolotl_utils_ram.ei_pursuit_ram(\n",
    "    raw_data=raw_data,\n",
    "    spikes=ks_spikes,                     # absolute sample times\n",
    "    ei_template=ei,                    # EI from selected cluster\n",
    "    save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "    alignment_offset = -window[0],\n",
    "    fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "    sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "    return_debug=True, \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(spikes))\n",
    "ss=ks_spikes[mean_scores_at_spikes<20000]\n",
    "print(len(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(mean_scores_at_spikes, bins=200, alpha=0.5, label='KS spike scores', color='red')\n",
    "plt.xlabel(\"Mean EI Match Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Mean EI Scores: Global vs. KS-aligned\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NaNs in scores: {np.isnan(scores).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "fit_percentile = 40\n",
    "sigma_thresh = 5.0\n",
    "\n",
    "scores = mean_scores_at_spikes  # KS spike scores\n",
    "\n",
    "clean_scores = mean_scores_at_spikes[~np.isnan(mean_scores_at_spikes)]\n",
    "\n",
    "# 1. Determine percentile cutoff\n",
    "cutoff = np.percentile(clean_scores, fit_percentile, method='nearest')\n",
    "print(cutoff)\n",
    "\n",
    "# 2. Select left tail\n",
    "left_tail = clean_scores[clean_scores <= cutoff]\n",
    "\n",
    "# 3. Fit normal distribution to tail\n",
    "mu, sigma = norm.fit(left_tail)\n",
    "\n",
    "# 4. Compute final threshold\n",
    "threshold = mu - sigma_thresh * sigma\n",
    "\n",
    "print(f\"Fitted mu = {mu:.3f}, sigma = {sigma:.3f}\")\n",
    "print(f\"Computed threshold = {threshold:.3f}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(left_tail, bins=100, density=True, alpha=0.5, label=\"Left tail\")\n",
    "\n",
    "# Overlay fitted Gaussian\n",
    "x = np.linspace(left_tail.min(), left_tail.max(), 200)\n",
    "pdf = norm.pdf(x, mu, sigma)\n",
    "plt.plot(x, pdf, 'r-', label=f\"Fit: μ={mu:.2f}, σ={sigma:.2f}\")\n",
    "\n",
    "plt.axvline(threshold, color='red', linestyle='--', label=f\"Threshold = {threshold:.2f}\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Fit to Left Tail of Scores (KS Spikes)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "unit_id_1 = 1000\n",
    "unit_id_2 = 11\n",
    "max_diff = 10 # in samples\n",
    "\n",
    "# --- Load spike times ---\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    spikes_1 = h5[f'unit_{unit_id_1}']['spike_times'][:]\n",
    "    spikes_2 = h5[f'unit_{unit_id_2}']['spike_times'][:]\n",
    "\n",
    "# --- Sort spike times (for efficiency) ---\n",
    "spikes_1 = np.sort(spikes_1)\n",
    "spikes_2 = np.sort(spikes_2)\n",
    "\n",
    "# --- Match spikes within max_diff ---\n",
    "matches = []\n",
    "j_start = 0\n",
    "\n",
    "for i, t1 in enumerate(spikes_1):\n",
    "    while j_start < len(spikes_2) and spikes_2[j_start] < t1 - max_diff:\n",
    "        j_start += 1\n",
    "\n",
    "    j = j_start\n",
    "    while j < len(spikes_2) and spikes_2[j] <= t1 + max_diff:\n",
    "        if abs(t1 - spikes_2[j]) <= max_diff:\n",
    "            matches.append((i, j))\n",
    "        j += 1\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"Found {len(matches)} matched spikes between unit {unit_id_1} and {unit_id_2}\")\n",
    "for i, j in matches:\n",
    "    print(f\"Spike1: {spikes_1[i]}, Spike2: {spikes_2[j]}\")\n",
    "\n",
    "# --- Build set of matched indices in spikes_2 ---\n",
    "matched_j = set(j for _, j in matches)\n",
    "\n",
    "# --- Extract unmatched spike times from spikes_2 ---\n",
    "unmatched_spikes_2 = [spikes_2[j] for j in range(len(spikes_2)) if j not in matched_j]\n",
    "\n",
    "print(f\"Found {len(unmatched_spikes_2)} unmatched spikes in unit {unit_id_2}\")\n",
    "unmatched_spikes_2 = np.array(unmatched_spikes_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei,\n",
    "    positions=ei_positions,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,\n",
    "    box_width=50,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"EI\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, cluster in enumerate(clusters_pre):\n",
    "    ei = cluster['ei']\n",
    "    ref_ch = cluster['channels'][np.argmax(np.ptp(ei[cluster['channels'], :], axis=1))]\n",
    "    ei_p2p = np.ptp(ei[ref_ch, :])\n",
    "    n_spikes = len(cluster['inds'])\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plot_ei_waveforms(\n",
    "        ei=ei,\n",
    "        positions=ei_positions,\n",
    "        scale=70.0,\n",
    "        box_height=1.0,\n",
    "        box_width=50,\n",
    "        linewidth=0.5,\n",
    "        alpha=0.9,\n",
    "        colors='black'\n",
    "    )\n",
    "    plt.title(f\"Cluster {i} EI — Spikes: {n_spikes}, P2P on Ref Ch ({ref_ch}): {ei_p2p:.1f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips_baselined, ei, k_start=3,return_debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(cluster_eis_pre[2][148,:], color='black', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ei_a = clusters_pre[0]['ei']\n",
    "ei_b = clusters_pre[1]['ei']\n",
    "\n",
    "\n",
    "result = axolotl_utils_ram.compare_ei_subtraction(ei_a, ei_b, max_lag=3, p2p_thresh=30.0)\n",
    "\n",
    "res = np.array(result['per_channel_residuals'])\n",
    "cos_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "neg_inds = np.where(res < -10)[0]\n",
    "\n",
    "print(len(neg_inds))\n",
    "print(cos_sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ei_a = cluster_eis_pre[0]\n",
    "ei_b = cluster_eis_pre[2]\n",
    "\n",
    "\n",
    "result = axolotl_utils_ram.compare_ei_subtraction(ei_a, ei_b, max_lag=3, p2p_thresh=30.0)\n",
    "\n",
    "res = np.array(result['per_channel_residuals'])\n",
    "cos_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "neg_inds = np.where(res < -10)[0]\n",
    "\n",
    "print(len(neg_inds))\n",
    "print(cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_ei_waveforms(\n",
    "    ei=cluster_eis_pre[2],\n",
    "    positions=ei_positions,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,\n",
    "    box_width=50,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(f\"Cluster {i} EI — Spikes: {n_spikes}, P2P on Ref Ch ({ref_ch}): {ei_p2p:.1f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(ei_a[148,:], color='black', linewidth=1)\n",
    "plt.plot(ei_b[148,:], color='red', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_cosine_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "amp_threshold = -10\n",
    "cos_threshold = 0.9\n",
    "\n",
    "res = result['per_channel_residuals']\n",
    "neg_inds = np.where(np.array(res) < amp_threshold)[0]\n",
    "if global_cosine_sim < cos_threshold or len(neg_inds) > 0:\n",
    "    print('two units')\n",
    "else:\n",
    "    print('same unit')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(result['good_channels'], result['per_channel_residuals'], color='gray')\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Mean Residual (B - A)\")\n",
    "plt.title(\"Per-Channel Residuals (Masked Subtraction)\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(result['good_channels'], result['per_channel_cosine_sim'], color='gray')\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Mean Residual (B - A)\")\n",
    "plt.title(\"Per-Channel cosine_sim (Masked Subtraction)\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.bar(result['good_channels'], result['p2p'], color='gray')\n",
    "# plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "# plt.xlabel(\"Channel ID\")\n",
    "# plt.ylabel(\"Mean Residual (B - A)\")\n",
    "# plt.title(\"Per-Channel amplitude\")\n",
    "# plt.grid(True, axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.75× of mean waveform peak\n",
    "    threshold_ampl = 0.75 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "        dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        ref_channel=ref_channel,\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype='int16'\n",
    "    )\n",
    "\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "    final_spike_inds = np.where(keep_mask)[0]\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'window': (-20, 60),\n",
    "    'min_spikes': 100,\n",
    "    'ei_sim_threshold': 0.75,\n",
    "    'k_start': 4,\n",
    "    'k_refine': 2\n",
    "}\n",
    "\n",
    "from verify_cluster import verify_cluster\n",
    "\n",
    "spike_times = spikes\n",
    "clusters = verify_cluster(\n",
    "    spike_times=spike_times,\n",
    "    dat_path=snips_baselined,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "print(f\"Returned {len(clusters)} clean subclusters\")\n",
    "for i, cl in enumerate(clusters):\n",
    "    print(f\"  Cluster {i}: {len(cl['inds'])} spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import analyze_clusters\n",
    "import importlib\n",
    "importlib.reload(analyze_clusters)\n",
    "\n",
    "\n",
    "analyze_clusters.analyze_clusters(clusters,\n",
    "                 spike_times=spikes,\n",
    "                 sampling_rate=20000,\n",
    "                 dat_path=snips_baselined,\n",
    "                 h5_path='/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5',\n",
    "                 triggers_mat_path='/Volumes/Lab/Users/alexth/axolotl/trigger_in_samples_201703151.mat',\n",
    "                 cluster_ids=None,\n",
    "                 lut=None,\n",
    "                 sta_depth=30,\n",
    "                 sta_offset=0,\n",
    "                 sta_chunk_size=1000,\n",
    "                 sta_refresh=2,\n",
    "                 ei_scale=3,\n",
    "                 ei_cutoff=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = snips_baselined[ref_channel, 20, :].copy()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tmp)\n",
    "\n",
    "final_spike_inds = np.arange(len(spikes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(tmp, bins=50, color='gray', edgecolor='black')\n",
    "plt.title(\"Histogram of tmp values\")\n",
    "plt.xlabel(\"Amplitude\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(tmp > -500)[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(snips_baselined[39, :, inds].T, alpha=1)\n",
    "plt.plot(snips_baselined[39, :, :6], alpha=1)\n",
    "plt.title(f\"Overlay of {len(inds)} selected snippets on channel 39\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p_threshold = 30\n",
    "ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "#print(\"reclustered pursuit\\n\")\n",
    "\n",
    "# check for matching KS units\n",
    "results = []\n",
    "lag = 20\n",
    "ks_sim_threshold = 0.75\n",
    "\n",
    "# Run comparison\n",
    "sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "matches = [\n",
    "    {\n",
    "        \"unit_id\": ks_unit_ids[i],\n",
    "        \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "        \"similarity\": float(sim[i]),\n",
    "        \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "    }\n",
    "    for i in np.where(sim > ks_sim_threshold)[0]\n",
    "]\n",
    "\n",
    "pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "labels_post = np.array([0])                    # just one fake cluster label\n",
    "sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "selected_index_post = 0                        # only one cluster, so index is 0\n",
    "cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "spike_counts_post = [len(snips)]               # use actual number of spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    # if unit_id >= max_units:\n",
    "    #     print(\"Reached unit limit.\")\n",
    "    #     break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencoder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
