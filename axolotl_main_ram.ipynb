{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "from compare_eis import compare_eis\n",
    "\n",
    "\n",
    "# --- Path and recording setup ---\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat\"\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Get total number of samples ---\n",
    "file_size_bytes = os.path.getsize(dat_path)\n",
    "total_samples = file_size_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "\n",
    "# --- Load entire file into RAM as int16 ---\n",
    "raw_data = np.fromfile(dat_path, dtype=dtype, count=total_samples * n_channels)\n",
    "raw_data = raw_data.reshape((total_samples, n_channels))  # shape: [T, C]\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 1500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "do_pursuit = 0\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0528.h5' # where to save data\n",
    "\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug\"\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "    ks_vision_ids = f['/vision_ids'][:]  # shape: (N_units,)\n",
    "\n",
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "save_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_and_artifacts.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(save_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines = axolotl_utils_ram.compute_baselines_int16(raw_data, segment_len=100_000) # shape (512, 360)\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "        }, f)\n",
    "\n",
    "\n",
    "# get KS EIs\n",
    "ks_ei_path = '/Volumes/Lab/Users/alexth/axolotl/ks_eis_subset.h5'\n",
    "ks_templates = {}\n",
    "ks_n_spikes = {}\n",
    "\n",
    "with h5py.File(ks_ei_path, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        unit_id = int(k.split('_')[1])-1\n",
    "        ks_templates[unit_id] = f[k][:]\n",
    "        ks_n_spikes[unit_id] = f[k].attrs.get('n_spikes', -1)  # fallback if missing\n",
    "\n",
    "ks_unit_ids = list(ks_templates.keys())\n",
    "ks_ei_stack = np.stack([ks_templates[k] for k in ks_unit_ids], axis=0)  # [N x 512 x 81]\n",
    "\n",
    "unit_id = 0\n",
    "\n",
    "print(f\"\\n=== Starting unit {unit_id} ===\")\n",
    "\n",
    "while True:\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # could cache scores on channels to pre-identify next one\n",
    "    ref_channel = axolotl_utils_ram.find_dominant_channel_ram(\n",
    "            raw_data = raw_data,\n",
    "            segment_len = 100_000,\n",
    "            n_segments = 10,\n",
    "            peak_window = 30,\n",
    "            top_k_neg = 20,\n",
    "            top_k_events = 5,\n",
    "            seed = 42\n",
    "        )\n",
    "\n",
    "    threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "        raw_data=raw_data,\n",
    "        ref_channel=ref_channel,\n",
    "        window = 30,\n",
    "        total_samples_to_read = total_samples,\n",
    "        refractory = 30,\n",
    "        top_n = 100\n",
    "    )\n",
    "\n",
    "    print(f\"Channel: {ref_channel}, Threshold: {-threshold:.1f}, Initial spikes: {len(spike_times)}\")\n",
    "\n",
    "    snips, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spike_times,\n",
    "        window=window,\n",
    "        selected_channels=np.arange(n_channels)\n",
    "    )\n",
    "\n",
    "    ei = np.mean(snips, axis=2)\n",
    "    ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "    spikes_for_plot_pre = valid_spike_times\n",
    "\n",
    "    # Step 6–7: Cluster and select dominant unit\n",
    "    clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips, ei, k_start=3,return_debug=True)\n",
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.66× of mean waveform peak\n",
    "    threshold_ampl = 0.66 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "        dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        ref_channel=ref_channel,\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype='int16'\n",
    "    )\n",
    "\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    if unit_id >= max_units:\n",
    "        print(\"Reached unit limit.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(25, 4))\n",
    "plt.plot(raw_data[:5000, 39])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Channel 39: First 5,000 samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Parameters ---\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat'  # ← replace with actual path\n",
    "n_channels = 512\n",
    "channel = 39\n",
    "n_samples = 5000\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Read from disk ---\n",
    "with open(dat_path, 'rb') as f:\n",
    "    # Read first 5000 timepoints (i.e., 5000 * n_channels values)\n",
    "    raw = np.fromfile(f, dtype=dtype, count=n_samples * n_channels)\n",
    "    raw = raw.reshape((n_samples, n_channels))  # [time, channel]\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(25, 4))\n",
    "plt.plot(raw[:5000, 39])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Channel 39: First 5,000 samples')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test verify_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "from compare_eis import compare_eis\n",
    "\n",
    "\n",
    "# --- Path and recording setup ---\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat\"\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# --- Get total number of samples ---\n",
    "file_size_bytes = os.path.getsize(dat_path)\n",
    "total_samples = file_size_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "\n",
    "# --- Load entire file into RAM as int16 ---\n",
    "raw_data = np.fromfile(dat_path, dtype=dtype, count=total_samples * n_channels)\n",
    "raw_data = raw_data.reshape((total_samples, n_channels))  # shape: [T, C]\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 1500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "do_pursuit = 0\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0528.h5' # where to save data\n",
    "\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug\"\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "    ks_vision_ids = f['/vision_ids'][:]  # shape: (N_units,)\n",
    "\n",
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "save_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_and_artifacts.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(save_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines = axolotl_utils_ram.compute_baselines_int16(raw_data, segment_len=100_000) # shape (512, 360)\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "        }, f)\n",
    "\n",
    "\n",
    "# get KS EIs\n",
    "ks_ei_path = '/Volumes/Lab/Users/alexth/axolotl/ks_eis_subset.h5'\n",
    "ks_templates = {}\n",
    "ks_n_spikes = {}\n",
    "\n",
    "with h5py.File(ks_ei_path, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        unit_id = int(k.split('_')[1])-1\n",
    "        ks_templates[unit_id] = f[k][:]\n",
    "        ks_n_spikes[unit_id] = f[k].attrs.get('n_spikes', -1)  # fallback if missing\n",
    "\n",
    "ks_unit_ids = list(ks_templates.keys())\n",
    "ks_ei_stack = np.stack([ks_templates[k] for k in ks_unit_ids], axis=0)  # [N x 512 x 81]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "unit_id = 0\n",
    "\n",
    "print(f\"\\n=== Starting unit {unit_id} ===\")\n",
    "\n",
    "last_ref_channel = None\n",
    "remaining_candidates = []\n",
    "\n",
    "\n",
    "while True:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not remaining_candidates:\n",
    "        remaining_candidates = axolotl_utils_ram.find_dominant_channel_ram(\n",
    "            raw_data=raw_data,\n",
    "            segment_len=100_000,\n",
    "            n_segments=10,\n",
    "            peak_window=30,\n",
    "            top_k_neg=20,\n",
    "            top_k_events=5,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "        # Try candidates until one is far enough from previous\n",
    "\n",
    "    while remaining_candidates:\n",
    "        candidate = remaining_candidates.pop(0)\n",
    "        if last_ref_channel is None:\n",
    "            ref_channel = candidate\n",
    "            channel_source='fresh'\n",
    "            break\n",
    "        dist = np.linalg.norm(ei_positions[candidate] - ei_positions[last_ref_channel])\n",
    "        if dist >= 150:\n",
    "            ref_channel = candidate\n",
    "            channel_source='cache'\n",
    "            break\n",
    "    else:\n",
    "        # All candidates were too close → recompute and restart this block\n",
    "        remaining_candidates = axolotl_utils_ram.find_dominant_channel_ram(\n",
    "            raw_data=raw_data,\n",
    "            segment_len=100_000,\n",
    "            n_segments=10,\n",
    "            peak_window=30,\n",
    "            top_k_neg=20,\n",
    "            top_k_events=5,\n",
    "            seed=42\n",
    "        )\n",
    "        # Re-run selection from top of block\n",
    "        candidate = remaining_candidates.pop(0)\n",
    "        ref_channel = candidate\n",
    "        channel_source='fresh'\n",
    "\n",
    "    # Update tracker\n",
    "    last_ref_channel = ref_channel\n",
    "\n",
    "    # # could cache scores on channels to pre-identify next one\n",
    "    # ref_channel = axolotl_utils_ram.find_dominant_channel_ram(\n",
    "    #         raw_data = raw_data,\n",
    "    #         segment_len = 100_000,\n",
    "    #         n_segments = 10,\n",
    "    #         peak_window = 30,\n",
    "    #         top_k_neg = 20,\n",
    "    #         top_k_events = 5,\n",
    "    #         seed = 42\n",
    "    #     )\n",
    "\n",
    "    threshold, spike_times = axolotl_utils_ram.estimate_spike_threshold_ram(\n",
    "        raw_data=raw_data,\n",
    "        ref_channel=ref_channel,\n",
    "        window = 30,\n",
    "        total_samples_to_read = total_samples,\n",
    "        refractory = 30,\n",
    "        top_n = 100\n",
    "    )\n",
    "\n",
    "    print(f\"Channel: {ref_channel} (from {channel_source}), Threshold: {-threshold:.1f}, Initial spikes: {len(spike_times)}\")\n",
    "\n",
    "    snips, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spike_times,\n",
    "        window=window,\n",
    "        selected_channels=np.arange(n_channels)\n",
    "    )\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = valid_spike_times // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    ei = np.mean(snips_baselined, axis=2)\n",
    "    #ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "    spikes_for_plot_pre = valid_spike_times\n",
    "\n",
    "    # Step 6–7: Cluster and select dominant unit\n",
    "    clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, n_bad_channels_pre, cluster_eis_pre, cluster_to_merged_group_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips_baselined, ei, k_start=3,return_debug=True)\n",
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    contributing_original_ids_pre = [\n",
    "        orig_id for orig_id, merged_idx in cluster_to_merged_group_pre.items()\n",
    "        if merged_idx == selected_cluster_index_pre\n",
    "    ]\n",
    "    contributing_original_ids_pre = np.array(contributing_original_ids_pre)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.75× of mean waveform peak\n",
    "    threshold_ampl = 0.75 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "        dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        ref_channel=ref_channel,\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype='int16'\n",
    "    )\n",
    "\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "    final_spike_inds = np.where(keep_mask)[0]\n",
    "\n",
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, n_bad_channels_post, cluster_eis_post, cluster_to_merged_group_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "        n_bad_channels_pre = n_bad_channels_pre,\n",
    "        contributing_original_ids_pre = contributing_original_ids_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    if unit_id >= max_units:\n",
    "        print(\"Reached unit limit.\")\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    # if unit_id >= max_units:\n",
    "    #     print(\"Reached unit limit.\")\n",
    "    #     break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "ei = np.mean(snips_baselined, axis=2)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(ei[148,:], color='black', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, cluster in enumerate(clusters_pre):\n",
    "    ei = cluster['ei']\n",
    "    ref_ch = cluster['channels'][np.argmax(np.ptp(ei[cluster['channels'], :], axis=1))]\n",
    "    ei_p2p = np.ptp(ei[ref_ch, :])\n",
    "    n_spikes = len(cluster['inds'])\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plot_ei_waveforms(\n",
    "        ei=ei,\n",
    "        positions=ei_positions,\n",
    "        scale=70.0,\n",
    "        box_height=1.0,\n",
    "        box_width=50,\n",
    "        linewidth=0.5,\n",
    "        alpha=0.9,\n",
    "        colors='black'\n",
    "    )\n",
    "    plt.title(f\"Cluster {i} EI — Spikes: {n_spikes}, P2P on Ref Ch ({ref_ch}): {ei_p2p:.1f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_utils_ram\n",
    "import importlib\n",
    "importlib.reload(axolotl_utils_ram)\n",
    "\n",
    "clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = axolotl_utils_ram.cluster_spike_waveforms(snips_baselined, ei, k_start=3,return_debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(cluster_eis_pre[2][148,:], color='black', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ei_a = clusters_pre[0]['ei']\n",
    "ei_b = clusters_pre[1]['ei']\n",
    "\n",
    "\n",
    "result = axolotl_utils_ram.compare_ei_subtraction(ei_a, ei_b, max_lag=3, p2p_thresh=30.0)\n",
    "\n",
    "res = np.array(result['per_channel_residuals'])\n",
    "cos_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "neg_inds = np.where(res < -10)[0]\n",
    "\n",
    "print(len(neg_inds))\n",
    "print(cos_sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ei_a = cluster_eis_pre[0]\n",
    "ei_b = cluster_eis_pre[2]\n",
    "\n",
    "\n",
    "result = axolotl_utils_ram.compare_ei_subtraction(ei_a, ei_b, max_lag=3, p2p_thresh=30.0)\n",
    "\n",
    "res = np.array(result['per_channel_residuals'])\n",
    "cos_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "neg_inds = np.where(res < -10)[0]\n",
    "\n",
    "print(len(neg_inds))\n",
    "print(cos_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plot_ei_waveforms(\n",
    "    ei=cluster_eis_pre[2],\n",
    "    positions=ei_positions,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,\n",
    "    box_width=50,\n",
    "    linewidth=0.5,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(f\"Cluster {i} EI — Spikes: {n_spikes}, P2P on Ref Ch ({ref_ch}): {ei_p2p:.1f}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(ei_a[148,:], color='black', linewidth=1)\n",
    "plt.plot(ei_b[148,:], color='red', linewidth=1)\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_cosine_sim = np.mean(result['per_channel_cosine_sim'])\n",
    "\n",
    "amp_threshold = -10\n",
    "cos_threshold = 0.9\n",
    "\n",
    "res = result['per_channel_residuals']\n",
    "neg_inds = np.where(np.array(res) < amp_threshold)[0]\n",
    "if global_cosine_sim < cos_threshold or len(neg_inds) > 0:\n",
    "    print('two units')\n",
    "else:\n",
    "    print('same unit')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(result['good_channels'], result['per_channel_residuals'], color='gray')\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Mean Residual (B - A)\")\n",
    "plt.title(\"Per-Channel Residuals (Masked Subtraction)\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(result['good_channels'], result['per_channel_cosine_sim'], color='gray')\n",
    "plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "plt.xlabel(\"Channel ID\")\n",
    "plt.ylabel(\"Mean Residual (B - A)\")\n",
    "plt.title(\"Per-Channel cosine_sim (Masked Subtraction)\")\n",
    "plt.grid(True, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# plt.bar(result['good_channels'], result['p2p'], color='gray')\n",
    "# plt.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "# plt.xlabel(\"Channel ID\")\n",
    "# plt.ylabel(\"Mean Residual (B - A)\")\n",
    "# plt.title(\"Per-Channel amplitude\")\n",
    "# plt.grid(True, axis='y')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = axolotl_utils_ram.select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "    if do_pursuit:\n",
    "        (\n",
    "        spikes,\n",
    "        mean_score,\n",
    "        valid_score,\n",
    "        mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes,\n",
    "        mean_thresh,\n",
    "        valid_thresh\n",
    "        ) = axolotl_utils_ram.ei_pursuit_ram(\n",
    "            raw_data=raw_data,\n",
    "            spikes=spikes_init,                     # absolute sample times\n",
    "            ei_template=ei,                    # EI from selected cluster\n",
    "            save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "            alignment_offset = -window[0],\n",
    "            fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "            sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "            return_debug=True, \n",
    "\n",
    "        )\n",
    "    else:\n",
    "        spikes = spikes_init\n",
    "        mean_score=None\n",
    "        valid_score=None\n",
    "        mean_scores_at_spikes=spikes\n",
    "        valid_scores_at_spikes=None\n",
    "        mean_thresh=None\n",
    "        valid_thresh=None\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = axolotl_utils_ram.estimate_lags_by_xcorr_ram(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = axolotl_utils_ram.extract_snippets_ram(\n",
    "        raw_data=raw_data,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "    )\n",
    "\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.75× of mean waveform peak\n",
    "    threshold_ampl = 0.75 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad = axolotl_utils_ram.extract_snippets_single_channel(\n",
    "        dat_path='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        ref_channel=ref_channel,\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype='int16'\n",
    "    )\n",
    "\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "\n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "    final_spike_inds = np.where(keep_mask)[0]\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'window': (-20, 60),\n",
    "    'min_spikes': 100,\n",
    "    'ei_sim_threshold': 0.75,\n",
    "    'k_start': 4,\n",
    "    'k_refine': 2\n",
    "}\n",
    "\n",
    "from verify_cluster import verify_cluster\n",
    "\n",
    "spike_times = spikes\n",
    "clusters = verify_cluster(\n",
    "    spike_times=spike_times,\n",
    "    dat_path=snips_baselined,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "print(f\"Returned {len(clusters)} clean subclusters\")\n",
    "for i, cl in enumerate(clusters):\n",
    "    print(f\"  Cluster {i}: {len(cl['inds'])} spikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import analyze_clusters\n",
    "import importlib\n",
    "importlib.reload(analyze_clusters)\n",
    "\n",
    "\n",
    "analyze_clusters.analyze_clusters(clusters,\n",
    "                 spike_times=spikes,\n",
    "                 sampling_rate=20000,\n",
    "                 dat_path=snips_baselined,\n",
    "                 h5_path='/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5',\n",
    "                 triggers_mat_path='/Volumes/Lab/Users/alexth/axolotl/trigger_in_samples_201703151.mat',\n",
    "                 cluster_ids=None,\n",
    "                 lut=None,\n",
    "                 sta_depth=30,\n",
    "                 sta_offset=0,\n",
    "                 sta_chunk_size=1000,\n",
    "                 sta_refresh=2,\n",
    "                 ei_scale=3,\n",
    "                 ei_cutoff=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = snips_baselined[ref_channel, 20, :].copy()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tmp)\n",
    "\n",
    "final_spike_inds = np.arange(len(spikes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(tmp, bins=50, color='gray', edgecolor='black')\n",
    "plt.title(\"Histogram of tmp values\")\n",
    "plt.xlabel(\"Amplitude\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = np.where(tmp > -500)[0]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(snips_baselined[39, :, inds].T, alpha=1)\n",
    "plt.plot(snips_baselined[39, :, :6], alpha=1)\n",
    "plt.title(f\"Overlay of {len(inds)} selected snippets on channel 39\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = axolotl_utils_ram.cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = axolotl_utils_ram.select_cluster_by_ei_similarity_ram(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2p_threshold = 30\n",
    "ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "#print(\"reclustered pursuit\\n\")\n",
    "\n",
    "# check for matching KS units\n",
    "results = []\n",
    "lag = 20\n",
    "ks_sim_threshold = 0.75\n",
    "\n",
    "# Run comparison\n",
    "sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "matches = [\n",
    "    {\n",
    "        \"unit_id\": ks_unit_ids[i],\n",
    "        \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "        \"similarity\": float(sim[i]),\n",
    "        \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "    }\n",
    "    for i in np.where(sim > ks_sim_threshold)[0]\n",
    "]\n",
    "\n",
    "pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "labels_post = np.array([0])                    # just one fake cluster label\n",
    "sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "selected_index_post = 0                        # only one cluster, so index is 0\n",
    "cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "spike_counts_post = [len(snips)]               # use actual number of spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    axolotl_utils_ram.plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = axolotl_utils_ram.subtract_pca_cluster_means_ram(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    axolotl_utils_ram.apply_residuals(\n",
    "        raw_data=raw_data,\n",
    "        dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat',\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=raw_data.shape[0],\n",
    "        dtype = np.int16,\n",
    "        n_channels = n_channels,\n",
    "        is_ram=True,\n",
    "        is_disk=False\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    # if unit_id >= max_units:\n",
    "    #     print(\"Reached unit limit.\")\n",
    "    #     break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencoder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
