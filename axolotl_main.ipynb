{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WHOLE DAMN PIPELINE FOR AXOLOTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert time-major to row-major"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convert_time_to_channel_major\n",
    "import importlib\n",
    "importlib.reload(convert_time_to_channel_major)\n",
    "\n",
    "convert_time_to_channel_major.convert_time_to_channel_major(\n",
    "    dat_path_in='/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat',\n",
    "    dat_path_out='/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat',\n",
    "    n_channels=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from extract_snippets_channel_major import extract_snippets_channel_major\n",
    "from extract_snippets_channel_major import compute_channel_baselines_artifact\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "import os\n",
    "from compare_eis import compare_eis\n",
    "\n",
    "\n",
    "# Placeholder functions to be implemented\n",
    "# These imports will fail until you define them in your script/module\n",
    "from axolotl_unit_loop_utils import (\n",
    "    find_dominant_channel,\n",
    "    estimate_spike_threshold,\n",
    "    cluster_spike_waveforms,\n",
    "    select_cluster_with_largest_waveform,\n",
    "    ei_pursuit,\n",
    "    estimate_lags_by_xcorr,\n",
    "    select_cluster_by_ei_similarity,\n",
    "    subtract_pca_cluster_means,\n",
    "    apply_residuals_to_channel_major,\n",
    "    suppress_artifacts_in_dat,\n",
    "    plot_unit_diagnostics\n",
    ")\n",
    "\n",
    "# --- Parameters ---\n",
    "#dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat\"\n",
    "dat_path_chmajor = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat\"\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 1500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "ei_window_len = window[1] - window[0] + 1\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0528.h5' # where to save data\n",
    "\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug\"\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "    ks_vision_ids = f['/vision_ids'][:]  # shape: (N_units,)\n",
    "\n",
    "save_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_and_artifacts.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(save_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "    artifact_locs = {int(k): v for k, v in data['artifact_locs'].items()}\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines, artifact_locs = compute_channel_baselines_artifact(\n",
    "        dat_path_chmajor=dat_path_chmajor,\n",
    "        n_channels=n_channels,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype,\n",
    "        segment_len=100_000,\n",
    "        artifact_threshold=100.0,\n",
    "        artifact_padding=1000,\n",
    "        max_artifacts=5\n",
    "    )\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "            'artifact_locs': {str(k): v.tolist() if isinstance(v, np.ndarray) else v for k, v in artifact_locs.items()}\n",
    "        }, f)\n",
    "\n",
    "suppress_artifacts_in_dat(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    artifact_locs=artifact_locs,\n",
    "    n_channels=n_channels,\n",
    "    total_samples=total_samples\n",
    ")\n",
    "\n",
    "# get KS EIs\n",
    "ks_ei_path = '/Volumes/Lab/Users/alexth/axolotl/ks_eis_subset.h5'\n",
    "ks_templates = {}\n",
    "ks_n_spikes = {}\n",
    "\n",
    "with h5py.File(ks_ei_path, 'r') as f:\n",
    "    for k in f.keys():\n",
    "        unit_id = int(k.split('_')[1])-1\n",
    "        ks_templates[unit_id] = f[k][:]\n",
    "        ks_n_spikes[unit_id] = f[k].attrs.get('n_spikes', -1)  # fallback if missing\n",
    "\n",
    "ks_unit_ids = list(ks_templates.keys())\n",
    "ks_ei_stack = np.stack([ks_templates[k] for k in ks_unit_ids], axis=0)  # [N x 512 x 81]\n",
    "\n",
    "\n",
    "\n",
    "unit_id = 43\n",
    "\n",
    "print(f\"\\n=== Starting unit {unit_id} ===\")\n",
    "\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Step 1: Find dominant channel\n",
    "    ref_channel = find_dominant_channel(\n",
    "        dat_path=dat_path_chmajor,\n",
    "        n_channels=n_channels,\n",
    "        dtype=dtype,          # optional if you're happy with 'int16'\n",
    "        segment_len=100000,   # 5s at 20kHz\n",
    "        n_segments=10,\n",
    "        peak_window=peak_window, # look for post-peak maxima within ±30 samples\n",
    "        top_k_neg=20,   # find this many negative peaks per channel per segment\n",
    "        top_k_events=5, # use top K events per channel across all segments\n",
    "        seed=42\n",
    "    )\n",
    "    #print(f\"Channel: {ref_channel}\")\n",
    "\n",
    "    # Step 2–3: Estimate threshold and detect events\n",
    "    threshold, spike_times = estimate_spike_threshold(\n",
    "        dat_path=dat_path_chmajor,\n",
    "        ref_channel=ref_channel,\n",
    "        dtype=dtype,\n",
    "        window=peak_window,                  # ±window around neg peak to find post-peak\n",
    "        n_channels=n_channels,\n",
    "        total_samples_to_read=total_samples,    # could be changed to full file length\n",
    "        block_size=100_000,                  # Number of timepoints per block\n",
    "        refractory=30,                       # Enforce 60-sample exclusion window\n",
    "        top_n=100                            # N events to estimate spike threshold\n",
    "    )\n",
    "\n",
    "    print(f\"Channel: {ref_channel}, Threshold: {threshold:.1f}, Initial spikes: {len(spike_times)}\")\n",
    "    #print(len(spike_times))\n",
    "\n",
    "    #if len(spike_times) < 100:\n",
    "    #    print(\"Too few suprathreshold events — stopping.\")\n",
    "    #    break\n",
    "\n",
    "    # Step 4: Extract snippets\n",
    "    #snips = extract_snippets(dat_path, spike_times, window, n_channels, dtype)\n",
    "\n",
    "    snips, valid_spike_times = extract_snippets_channel_major(\n",
    "        dat_path_chmajor=dat_path_chmajor,\n",
    "        spike_times=spike_times,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    # Step 5: Compute EI\n",
    "    ei = np.mean(snips, axis=2)\n",
    "    ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "   \n",
    "    spikes_for_plot_pre = valid_spike_times\n",
    "    # Step 6–7: Cluster and select dominant unit\n",
    "    clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = cluster_spike_waveforms(snips, ei, k_start=3,return_debug=True)\n",
    "\n",
    "    ei, spikes_idx, selected_channels, selected_cluster_index_pre = select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "    spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "\n",
    "\n",
    "    # Step 8: EI pursuit\n",
    "    (\n",
    "    spikes,\n",
    "    mean_score,\n",
    "    valid_score,\n",
    "    mean_scores_at_spikes,\n",
    "    valid_scores_at_spikes,\n",
    "    mean_thresh,\n",
    "    valid_thresh\n",
    "    ) = ei_pursuit(\n",
    "        dat_path=dat_path_chmajor,\n",
    "        spikes=spikes_init,                     # absolute sample times\n",
    "        ei_template=ei,                    # EI from selected cluster\n",
    "        dtype=dtype,\n",
    "        total_samples=total_samples,          # or set to actual full recording length\n",
    "        save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "        block_size=None,                    # if None, GPU will handle this automatically\n",
    "        baseline_start_sample=0,            # revisit for baseline subtraction!!! should be inside GPU\n",
    "        alignment_offset = -window[0],\n",
    "        fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "        sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "        channel_major = True,\n",
    "        return_debug=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed = end_time - start_time \n",
    "    # print(f\"Finished pursuit. Elapsed: {elapsed:.1f} seconds.\")\n",
    "    #print(\"done with pursuit\\n\")\n",
    "\n",
    "    #if len(spikes) < 100:\n",
    "    #    print(\"Too few matched spikes — skipping subtraction.\")\n",
    "    #    break\n",
    "\n",
    "    # Step 9a: Extract full snippets from final spike times\n",
    "\n",
    "    snips_ref_channel, valid_spike_times = extract_snippets_channel_major(\n",
    "        dat_path_chmajor=dat_path_chmajor,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.array([ref_channel]),\n",
    "        window=window,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype\n",
    "    )\n",
    "    \n",
    "    snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "    lags = estimate_lags_by_xcorr(\n",
    "        snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "        peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "        window=(-5, 10),                  # optional, relative to peak\n",
    "        max_lag=6,                        # optional, max xcorr shift\n",
    "    )\n",
    "\n",
    "    lags = lags \n",
    "    spikes = spikes+lags\n",
    "\n",
    "    snips_full, valid_spike_times = extract_snippets_channel_major(\n",
    "        dat_path_chmajor=dat_path_chmajor,\n",
    "        spike_times=spikes,\n",
    "        selected_channels=np.arange(n_channels),\n",
    "        window=window,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "    segment_len = 100_000\n",
    "    snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "    n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "    # Determine segment index for each spike\n",
    "    segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "    # Loop through channels and subtract baseline per spike\n",
    "    for ch in range(n_channels):\n",
    "        snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "    # Extract baseline-subtracted waveforms for ref_channel\n",
    "    ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "    # Mean waveform over all spikes\n",
    "    ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "    # Negative peak (should be near index 20)\n",
    "    ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "    # Threshold at 0.5× of mean waveform peak\n",
    "    threshold_ampl = 0.5 * ref_peak_amp\n",
    "\n",
    "    # Get all actual spike values at sample 20\n",
    "    spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "    # Flag bad spikes: too small\n",
    "    bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "    # Create mask to keep only good spikes\n",
    "    keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "    keep_mask[bad_inds] = False\n",
    "\n",
    "    # --- Extract bad spike traces for plotting\n",
    "    bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "    # Get original traces for bad_spike_traces\n",
    "    snips_bad, disregard = extract_snippets_channel_major(\n",
    "        dat_path_chmajor='/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor_orig.dat',\n",
    "        spike_times=spikes[bad_inds],\n",
    "        selected_channels=[ref_channel],\n",
    "        window=window,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype\n",
    "    )\n",
    "    segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "    snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "    \n",
    "    # Apply to real data and snips_baselined\n",
    "    snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "    good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "    snips_full = snips_full[:, :, keep_mask]\n",
    "    valid_spike_times = valid_spike_times[keep_mask]\n",
    "    spikes = spikes[keep_mask]\n",
    "\n",
    "    spikes_for_plot_post = spikes\n",
    "\n",
    "\n",
    "    # Step 9b: Recluster - choose k. snips_full is all channels, baselined - relevant cahnnels will be subselected in the function.\n",
    "\n",
    "    if len(spikes)<100:\n",
    "        pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "        labels_post = np.array([0])                    # just one fake cluster label\n",
    "        sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "        ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "        selected_index_post = 0                        # only one cluster, so index is 0\n",
    "        cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "        spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "        spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "        matches = []                                # no matches\n",
    "        # `snips_baselined` is [C x T x N]\n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True)\n",
    "\n",
    "        # Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "        ei, final_spike_inds, selected_channels, selected_cluster_index_post = select_cluster_by_ei_similarity(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "        spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "        snips_baselined = snips_baselined[:,:,final_spike_inds] # cut only the ones that survived\n",
    "\n",
    "        p2p_threshold = 30\n",
    "        ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "        selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "        selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "\n",
    "        #print(\"reclustered pursuit\\n\")\n",
    "\n",
    "        # check for matching KS units\n",
    "        results = []\n",
    "        lag = 20\n",
    "        ks_sim_threshold = 0.75\n",
    "\n",
    "        # Run comparison\n",
    "        sim = compare_eis(ks_ei_stack, ei, lag).squeeze() # shape: (num_KS_units,)\n",
    "        matches = [\n",
    "            {\n",
    "                \"unit_id\": ks_unit_ids[i],\n",
    "                \"vision_id\": int(ks_vision_ids[ks_unit_ids[i]].item()),\n",
    "                \"similarity\": float(sim[i]),\n",
    "                \"n_spikes\": int(ks_n_spikes[ks_unit_ids[i]])\n",
    "            }\n",
    "            for i in np.where(sim > ks_sim_threshold)[0]\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "    # DIAGNOSTIC PLOTS\n",
    "\n",
    "    plot_unit_diagnostics(\n",
    "        output_path=debug_folder,\n",
    "        unit_id=unit_id,\n",
    "\n",
    "        # --- From first call to cluster_spike_waveforms\n",
    "        pcs_pre=pcs_pre,\n",
    "        labels_pre=labels_pre,\n",
    "        sim_matrix_pre=sim_matrix_pre,\n",
    "        cluster_eis_pre = cluster_eis_pre,\n",
    "        spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "        # --- From ei_pursuit\n",
    "        mean_score=mean_score,\n",
    "        valid_score=valid_score,\n",
    "        mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "        valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "        mean_thresh=mean_thresh,\n",
    "        valid_thresh=valid_thresh,\n",
    "\n",
    "        # --- Lag estimation and bad spike filtering\n",
    "        lags=lags,\n",
    "        bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "        good_mean_trace=good_mean_trace,\n",
    "        threshold_ampl=-threshold_ampl,\n",
    "        ref_channel=ref_channel,\n",
    "        snips_bad=snips_bad,\n",
    "\n",
    "        # --- From second clustering\n",
    "        pcs_post=pcs_post,\n",
    "        labels_post=labels_post,\n",
    "        sim_matrix_post=sim_matrix_post,\n",
    "        cluster_eis_post = cluster_eis_post,\n",
    "        spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "        # --- For axis labels etc.\n",
    "        window=(-20, 60),\n",
    "\n",
    "        ei_positions=ei_positions,\n",
    "        selected_channels_count=len(selected_channels),\n",
    "\n",
    "        spikes = spikes, \n",
    "        orig_threshold = threshold,\n",
    "        ks_matches = matches\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    # Step 10: Save unit metadata\n",
    "    try:\n",
    "        with h5py.File(h5_out_path, 'a') as h5:\n",
    "            group = h5.require_group(f'unit_{unit_id}')\n",
    "\n",
    "            for name, data in [\n",
    "                ('spike_times', spikes.astype(np.int32)),\n",
    "                ('ei', ei.astype(np.float32)), # EI is already baselined\n",
    "                ('selected_channels', selected_channels.astype(np.int32))\n",
    "            ]:\n",
    "                if name in group:\n",
    "                    del group[name]\n",
    "                group.create_dataset(name, data=data)\n",
    "\n",
    "            group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "            # group.create_dataset('spike_times', data=spikes.astype(np.int32))\n",
    "            # group.create_dataset('ei', data=ei.astype(np.float32))\n",
    "            # group.create_dataset('selected_channels', data=selected_channels.astype(np.int32))\n",
    "            # group.attrs['peak_channel'] = int(np.argmax(np.ptp(ei, axis=1)))\n",
    "\n",
    "        #print(f\"Exported unit_{unit_id} with {len(spikes)} spikes.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nKeyboard interrupt detected — exiting safely before write completes.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nUnexpected error while saving unit_{unit_id}: {e}\")\n",
    "\n",
    "\n",
    "    # Step 11: Prepare for subtraction\n",
    "\n",
    "    # snippets, valid_spike_times = extract_snippets_channel_major(\n",
    "    #     dat_path_chmajor=dat_path_chmajor,\n",
    "    #     spike_times=spikes,\n",
    "    #     selected_channels=selected_channels,\n",
    "    #     window=window,\n",
    "    #     total_samples=total_samples,\n",
    "    #     dtype=dtype\n",
    "    # )\n",
    "    # here we use non-baselined snippets... and rebaseline them later in subtract_pca_cluster_means - could be optimized. \n",
    "    if len(spikes)>=100:\n",
    "        snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "        snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n",
    "\n",
    "            # --- Setup ---\n",
    "        residuals_per_channel = {}\n",
    "        cluster_ids_per_channel = {}\n",
    "        scale_factors_per_channel = {}\n",
    "\n",
    "        for ch_idx, ch in enumerate(selected_channels):\n",
    "            # Slice data for this channel\n",
    "            ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "            ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "            # Subtract PCA cluster means\n",
    "            residuals, cluster_ids, scale_factors = subtract_pca_cluster_means(\n",
    "                snippets=ch_snips,\n",
    "                baselines=ch_baselines,\n",
    "                spike_times=spikes,\n",
    "                segment_len=100_000,  # must match what was used to generate baselines\n",
    "                n_clusters=5,\n",
    "                offset_window=(-10,40)\n",
    "            )\n",
    "\n",
    "            # Store results\n",
    "            residuals_per_channel[ch] = residuals\n",
    "            cluster_ids_per_channel[ch] = cluster_ids\n",
    "            scale_factors_per_channel[ch] = scale_factors\n",
    "    else:\n",
    "        \n",
    "        # We only subtract on the referent channel to avoid distortion\n",
    "        template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "        residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "        # Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "        # Transpose to match expected shape: (n_spikes, snip_len)\n",
    "        # force key and lookup to match normal case: np.int64\n",
    "        ref_channel = np.int64(ref_channel)\n",
    "        selected_channels = np.array([ref_channel], dtype=np.int64)\n",
    "        residuals_per_channel = {\n",
    "            ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "        }\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time \n",
    "    # print(f\"Finished preprocessing, starting edits. Elapsed: {elapsed:.1f} seconds.\")\n",
    "\n",
    "    print(len(spikes))\n",
    "    print(residuals_per_channel[ref_channel].shape)\n",
    "\n",
    "    # Step 12: edit raw data\n",
    "    write_locs = spikes + window[0]\n",
    "    apply_residuals_to_channel_major(\n",
    "        dat_path_chmajor=dat_path_chmajor,\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype,\n",
    "        n_channels=n_channels\n",
    "    )\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    print(f\"Processed unit {unit_id} with {len(spikes)} final spikes in {elapsed:.1f} seconds.\\n\")\n",
    "\n",
    "\n",
    "    # Step 13: Repeat until done\n",
    "    unit_id += 1\n",
    "    if unit_id >= max_units:\n",
    "        print(\"Reached unit limit.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(len(spikes))\n",
    "    tmp = snips_baselined[:,:,final_spike_inds]\n",
    "    print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(spikes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "traces = snips_baselined[ref_channel, :, :]  # shape (T, N)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i in range(traces.shape[1]):\n",
    "    plt.plot(traces[:, i], color='gray', alpha=0.7)\n",
    "\n",
    "plt.title(f\"Ref channel {ref_channel} | N={traces.shape[1]} spikes\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tmp_ei = np.mean(snips_baselined, axis=2)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_ei_waveforms(\n",
    "    ei=tmp_ei,\n",
    "    positions=ei_positions,\n",
    "    ref_channel = ref_channel,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,   # can adjust for visibility\n",
    "    box_width=50,\n",
    "    linewidth=1,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"Initial EI from Detected Spikes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step-by-step tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from extract_snippets_channel_major import extract_snippets_channel_major\n",
    "from extract_snippets_channel_major import compute_channel_baselines_artifact\n",
    "import h5py\n",
    "import time\n",
    "from scipy.spatial import KDTree\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Placeholder functions to be implemented\n",
    "# These imports will fail until you define them in your script/module\n",
    "from axolotl_unit_loop_utils import (\n",
    "    find_dominant_channel,\n",
    "    estimate_spike_threshold,\n",
    "    cluster_spike_waveforms,\n",
    "    select_cluster_with_largest_waveform,\n",
    "    ei_pursuit,\n",
    "    estimate_lags_by_xcorr,\n",
    "    select_cluster_by_ei_similarity,\n",
    "    subtract_pca_cluster_means,\n",
    "    apply_residuals_to_channel_major,\n",
    "    suppress_artifacts_in_dat,\n",
    "    plot_unit_diagnostics\n",
    ")\n",
    "\n",
    "# --- Parameters ---\n",
    "#dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat\"\n",
    "dat_path_chmajor = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat\"\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 35\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "ei_window_len = window[1] - window[0] + 1\n",
    "peak_window = 30\n",
    "total_samples=36_000_000\n",
    "fit_offsets = (-5, 10)\n",
    "\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export, to get EI positions\n",
    "h5_out_path = '/Volumes/Lab/Users/alexth/axolotl/results_pipeline_0528.h5' # where to save data\n",
    "\n",
    "debug_folder = \"/Volumes/Lab/Users/alexth/axolotl/debug\"\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "save_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_baseline_and_artifacts.json\"\n",
    "\n",
    "if os.path.exists(save_path):\n",
    "    print(f\"Loading baselines\")\n",
    "    with open(save_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    baselines = np.array(data['baselines'], dtype=np.float32)\n",
    "    artifact_locs = {int(k): v for k, v in data['artifact_locs'].items()}\n",
    "else:\n",
    "    print(f\"Computing baselines\")\n",
    "    baselines, artifact_locs = compute_channel_baselines_artifact(\n",
    "        dat_path_chmajor=dat_path_chmajor,\n",
    "        n_channels=n_channels,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype,\n",
    "        segment_len=100_000,\n",
    "        artifact_threshold=100.0,\n",
    "        artifact_padding=1000,\n",
    "        max_artifacts=5\n",
    "    )\n",
    "\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'baselines': baselines.tolist(),\n",
    "            'artifact_locs': {str(k): v.tolist() if isinstance(v, np.ndarray) else v for k, v in artifact_locs.items()}\n",
    "        }, f)\n",
    "\n",
    "suppress_artifacts_in_dat(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    artifact_locs=artifact_locs,\n",
    "    n_channels=n_channels,\n",
    "    total_samples=total_samples\n",
    ")\n",
    "\n",
    "unit_id = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Find dominant channel\n",
    "ref_channel = find_dominant_channel(\n",
    "    dat_path=dat_path_chmajor,\n",
    "    n_channels=n_channels,\n",
    "    dtype=dtype,          # optional if you're happy with 'int16'\n",
    "    segment_len=100000,   # 5s at 20kHz\n",
    "    n_segments=10,\n",
    "    peak_window=peak_window, # look for post-peak maxima within ±30 samples\n",
    "    top_k_neg=20,   # find this many negative peaks per channel per segment\n",
    "    top_k_events=5, # use top K events per channel across all segments\n",
    "    seed=42\n",
    ")\n",
    "print(f\"Channel: {ref_channel}\\n\")\n",
    "\n",
    "\n",
    "# Step 2–3: Estimate threshold and detect events\n",
    "threshold, spike_times = estimate_spike_threshold(\n",
    "    dat_path=dat_path_chmajor,\n",
    "    ref_channel=ref_channel,\n",
    "    dtype=dtype,\n",
    "    window=peak_window,                  # ±window around neg peak to find post-peak\n",
    "    n_channels=n_channels,\n",
    "    total_samples_to_read=total_samples,    # could be changed to full file length\n",
    "    block_size=100_000,                  # Number of timepoints per block\n",
    "    refractory=30,                       # Enforce 60-sample exclusion window\n",
    "    top_n=100                            # N events to estimate spike threshold\n",
    ")\n",
    "\n",
    "print(f\"Threshold: {threshold}\\n\")\n",
    "print(len(spike_times))\n",
    "\n",
    "snips, valid_spike_times = extract_snippets_channel_major(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    spike_times=spike_times,\n",
    "    selected_channels=np.arange(n_channels),\n",
    "    window=window,\n",
    "    total_samples=total_samples,\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "# Step 5: Compute EI\n",
    "ei = np.mean(snips, axis=2)\n",
    "ei -= ei[:, :5].mean(axis=1, keepdims=True)\n",
    "\n",
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei,\n",
    "    positions=ei_positions,\n",
    "    ref_channel = ref_channel,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,   # can adjust for visibility\n",
    "    box_width=50,\n",
    "    linewidth=1,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"Initial EI from Detected Spikes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "# Step 6–7: Cluster and select dominant unit\n",
    "clusters_pre, pcs_pre, labels_pre, sim_matrix_pre, cluster_eis_pre  = axolotl_unit_loop_utils.cluster_spike_waveforms(snips, ei, k_start=3,return_debug=True, plot_diagnostic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "amplitudes = []\n",
    "for cl in clusters_pre:\n",
    "    ei = cl['ei']\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plot_ei_waveforms(\n",
    "        ei=ei,\n",
    "        positions=ei_positions,\n",
    "        scale=70.0,\n",
    "        box_height=1.0,   # can adjust for visibility\n",
    "        box_width=50,\n",
    "        linewidth=1,\n",
    "        alpha=0.9,\n",
    "        colors='black'\n",
    "    )\n",
    "    plt.title(\"Initial EI from Detected Spikes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    sp = cl['inds']\n",
    "    print(len(sp))\n",
    "    p2p = ei[ref_channel, :].max() - ei[ref_channel, :].min()\n",
    "    amplitudes.append(p2p)\n",
    "print(amplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ei, spikes_idx, selected_channels, selected_cluster_index_pre = select_cluster_with_largest_waveform(clusters_pre, ref_channel)\n",
    "\n",
    "spikes_init = spike_times[spikes_idx]\n",
    "\n",
    "print(len(spikes_init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 8: EI pursuit\n",
    "(\n",
    "spikes,\n",
    "mean_score,\n",
    "valid_score,\n",
    "mean_scores_at_spikes,\n",
    "valid_scores_at_spikes,\n",
    "mean_thresh,\n",
    "valid_thresh\n",
    ") = ei_pursuit(\n",
    "    dat_path=dat_path_chmajor,\n",
    "    spikes=spikes_init,                     # absolute sample times\n",
    "    ei_template=ei,                    # EI from selected cluster\n",
    "    dtype=dtype,\n",
    "    total_samples=total_samples,          # or set to actual full recording length\n",
    "    save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "    block_size=None,                    # if None, GPU will handle this automatically\n",
    "    baseline_start_sample=0,            # revisit for baseline subtraction!!! should be inside GPU\n",
    "    alignment_offset = -window[0],\n",
    "    fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "    sigma_thresh = 5.0,                  # how many Gaussian sigmas to take for threshold\n",
    "    channel_major = True,\n",
    "    return_debug=True\n",
    ")\n",
    "\n",
    "print(len(spikes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snips_ref_channel, valid_spike_times = extract_snippets_channel_major(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    spike_times=spikes,\n",
    "    selected_channels=np.array([ref_channel]),\n",
    "    window=window,\n",
    "    total_samples=total_samples,\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "snips_ref_channel = snips_ref_channel.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "lags = estimate_lags_by_xcorr(\n",
    "    snippets=snips_ref_channel,                # shape [N x C x T]\n",
    "    peak_channel_idx=0,                 # 0 because the only channel that gets passed is the referent channel\n",
    "    window=(-5, 10),                  # optional, relative to peak\n",
    "    max_lag=6,                        # optional, max xcorr shift\n",
    ")\n",
    "\n",
    "lags = lags \n",
    "spikes = spikes+lags\n",
    "\n",
    "snips_full, valid_spike_times = extract_snippets_channel_major(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    spike_times=spikes,\n",
    "    selected_channels=np.arange(n_channels),\n",
    "    window=window,\n",
    "    total_samples=total_samples,\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "segment_len = 100_000\n",
    "snips_baselined = snips_full.copy()  # shape (n_channels, 81, N)\n",
    "n_channels, snip_len, n_spikes = snips_baselined.shape\n",
    "\n",
    "# Determine segment index for each spike\n",
    "segment_indices = spikes // segment_len  # shape: (n_spikes,)\n",
    "\n",
    "# Loop through channels and subtract baseline per spike\n",
    "for ch in range(n_channels):\n",
    "    snips_baselined[ch, :, :] -= baselines[ch, segment_indices][None, :]\n",
    "\n",
    "# Extract baseline-subtracted waveforms for ref_channel\n",
    "ref_snips = snips_baselined[ref_channel, :, :]  # shape: (81, N)\n",
    "\n",
    "# Mean waveform over all spikes\n",
    "ref_mean = ref_snips.mean(axis=1)  # shape: (81,)\n",
    "# Negative peak (should be near index 20)\n",
    "ref_peak_amp = np.abs(ref_mean[-window[0]])  # scalar\n",
    "\n",
    "# Threshold at 0.5× of mean waveform peak\n",
    "threshold_ampl = 0.5 * ref_peak_amp\n",
    "\n",
    "# Get all actual spike values at sample 20\n",
    "spike_amplitudes = np.abs(ref_snips[20, :])  # shape: (N,)\n",
    "\n",
    "# Flag bad spikes: too small\n",
    "bad_inds = np.where(spike_amplitudes < threshold_ampl)[0]\n",
    "\n",
    "print(bad_inds)\n",
    "print(valid_spike_times[bad_inds])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create mask to keep only good spikes\n",
    "keep_mask = np.ones(spike_amplitudes.shape[0], dtype=bool)\n",
    "keep_mask[bad_inds] = False\n",
    "\n",
    "# --- Extract bad spike traces for plotting\n",
    "bad_spike_traces = snips_baselined[ref_channel, :, bad_inds]  # shape: (n_bad, T)\n",
    "\n",
    "# Get original traces for bad_spike_traces\n",
    "snips_bad, disregard = extract_snippets_channel_major(\n",
    "    dat_path_chmajor='/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor_orig.dat',\n",
    "    spike_times=spikes[bad_inds],\n",
    "    selected_channels=[ref_channel],\n",
    "    window=window,\n",
    "    total_samples=total_samples,\n",
    "    dtype=dtype\n",
    ")\n",
    "segment_indices = spikes[bad_inds] // segment_len  # shape: (n_spikes,)\n",
    "snips_bad[0, :, :] -= baselines[ref_channel, segment_indices][None, :]\n",
    "\n",
    "# Apply to real data and snips_baselined\n",
    "snips_baselined = snips_baselined[:, :, keep_mask]\n",
    "good_mean_trace = np.mean(snips_baselined[ref_channel, :, :], axis=1)\n",
    "snips_full = snips_full[:, :, keep_mask]\n",
    "valid_spike_times = valid_spike_times[keep_mask]\n",
    "spikes = spikes[keep_mask]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters_post, pcs_post, labels_post, sim_matrix_post, cluster_eis_post  = cluster_spike_waveforms(snips=snips_baselined, ei=ei, k_start=2,return_debug=True, plot_diagnostic=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 9c: choose the best cluster - choose similarity threshold. EI is all channels, baselined\n",
    "ei, final_spike_inds, selected_channels, selected_cluster_index_post = select_cluster_by_ei_similarity(clusters=clusters_post,reference_ei=ei,similarity_threshold=0.95)\n",
    "\n",
    "\n",
    "spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "\n",
    "p2p_threshold = 30\n",
    "ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spikes)\n",
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "# DIAGNOSTIC PLOTS\n",
    "\n",
    "\n",
    "pcs_post = np.zeros((1, 2))                    # shape: (N_spikes, 2 PCs)\n",
    "labels_post = np.array([0])                    # just one fake cluster label\n",
    "sim_matrix_post = np.zeros((1, 1))             # fake 1×1 similarity matrix\n",
    "ei_clusters_post = [np.zeros((512, 81))]       # fake EI for the “post” cluster\n",
    "selected_index_post = 0                        # only one cluster, so index is 0\n",
    "cluster_eis_post = [np.zeros((512, 81))]       # same dummy EI\n",
    "spikes_for_plot_post = np.array([0])           # placeholder spike time\n",
    "spike_counts_post = [len(snips)]               # use actual number of spikes\n",
    "ks_matches = []                                # no matches\n",
    "\n",
    "\n",
    "axolotl_unit_loop_utils.plot_unit_diagnostics(\n",
    "    output_path=debug_folder,\n",
    "    unit_id=unit_id,\n",
    "\n",
    "    # --- From first call to cluster_spike_waveforms\n",
    "    pcs_pre=pcs_pre,\n",
    "    labels_pre=labels_pre,\n",
    "    sim_matrix_pre=sim_matrix_pre,\n",
    "    ei_clusters_pre=[cl['ei'] for cl in clusters_pre],\n",
    "    selected_index_pre=selected_cluster_index_pre,  # index into clusters_pre\n",
    "    cluster_eis_pre = cluster_eis_pre,\n",
    "    spikes_for_plot_pre = spikes_for_plot_pre,\n",
    "\n",
    "    # --- From ei_pursuit\n",
    "    mean_score=mean_score,\n",
    "    valid_score=valid_score,\n",
    "    mean_scores_at_spikes=mean_scores_at_spikes,\n",
    "    valid_scores_at_spikes=valid_scores_at_spikes,\n",
    "    mean_thresh=mean_thresh,\n",
    "    valid_thresh=valid_thresh,\n",
    "\n",
    "    # --- Lag estimation and bad spike filtering\n",
    "    lags=lags,\n",
    "    bad_spike_traces=bad_spike_traces,  # shape: (n_bad, T)\n",
    "    good_mean_trace=good_mean_trace,\n",
    "    threshold_ampl=-threshold_ampl,\n",
    "    ref_channel=ref_channel,\n",
    "    snips_bad=snips_bad,\n",
    "\n",
    "    # --- From second clustering\n",
    "    pcs_post=pcs_post,\n",
    "    labels_post=labels_post,\n",
    "    sim_matrix_post=sim_matrix_post,\n",
    "    ei_clusters_post=[cl['ei'] for cl in clusters_post],\n",
    "    selected_index_post=selected_cluster_index_post,\n",
    "    cluster_eis_post = cluster_eis_post,\n",
    "    spikes_for_plot_post = spikes_for_plot_post,\n",
    "\n",
    "    # --- For axis labels etc.\n",
    "    window=(-20, 60),\n",
    "\n",
    "    ei_positions=ei_positions,\n",
    "    selected_channels_count=len(selected_channels),\n",
    "\n",
    "    spike_counts_pre=[len(cl['inds']) for cl in clusters_pre],\n",
    "    spike_counts_post = [len(cl['inds']) for cl in clusters_post],\n",
    "    spikes = spikes, \n",
    "    orig_threshold = threshold,\n",
    "    ks_matches = matches\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(residuals_fallback[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `snips_baselined` is [C x T x N]\n",
    "# We only subtract on the referent channel to avoid distortion\n",
    "template_fallback = np.mean(snips_baselined[ref_channel], axis=1)  # shape: (T,)\n",
    "residuals_fallback = snips_baselined[ref_channel] - template_fallback[:, None]  # shape: (T, N)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for i in range(residuals_fallback.shape[1]):\n",
    "    plt.plot(residuals_fallback[:,i], color='black', alpha=0.5, linewidth=0.5)\n",
    "\n",
    "plt.plot(template_fallback, color='red', linewidth=2)\n",
    "plt.title(f\"Residuals from Channel {ref_channel}\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Assume residuals_fallback is (T, N) from previous step (template-subtracted waveforms)\n",
    "# Transpose to match expected shape: (n_spikes, snip_len)\n",
    "residual_snips_per_channel = {\n",
    "    ref_channel: residuals_fallback.T.astype(np.int16)\n",
    "}\n",
    "selected_channels = [ref_channel]\n",
    "\n",
    "write_locs = spikes + window[0]\n",
    "apply_residuals_to_channel_major(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    residual_snips_per_channel=residuals_per_channel,\n",
    "    write_locs=write_locs,\n",
    "    selected_channels=selected_channels,\n",
    "    total_samples=total_samples,\n",
    "    dtype=dtype,\n",
    "    n_channels=n_channels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "snips_full = snips_full[np.ix_(selected_channels, np.arange(snips_full.shape[1]), final_spike_inds)]\n",
    "snips_full = snips_full.transpose(2, 0, 1) # [C × T × N] → [N × C × T]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[ 9091998 22675586 30549802 34674503]\n",
    "tmp_spikes = np.array(spikes) \n",
    "targets = np.array([9091998, 22675586, 30549802, 34674503])\n",
    "tolerance = 30\n",
    "\n",
    "# Create a boolean mask for each target\n",
    "matches = [np.where(np.abs(tmp_spikes - t) <= tolerance)[0] for t in targets]\n",
    "\n",
    "# Flatten and get the actual matched spike times if needed\n",
    "matched_indices = np.concatenate(matches)\n",
    "matched_spikes = tmp_spikes[matched_indices]\n",
    "\n",
    "print(\"Matched spike indices:\", matched_indices)\n",
    "print(\"Matched spike times:\", matched_spikes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "    # --- Setup ---\n",
    "residuals_per_channel = {}\n",
    "cluster_ids_per_channel = {}\n",
    "scale_factors_per_channel = {}\n",
    "\n",
    "print(len(spikes))\n",
    "\n",
    "for ch_idx, ch in enumerate(selected_channels):\n",
    "    if ch==39:\n",
    "        # Slice data for this channel\n",
    "        ch_snips = snips_full[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "        ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "        # Subtract PCA cluster means\n",
    "        residuals, scale_factors, cluster_ids = axolotl_unit_loop_utils.subtract_pca_cluster_means(\n",
    "            snippets=ch_snips,\n",
    "            baselines=ch_baselines,\n",
    "            spike_times=spikes,\n",
    "            segment_len=100_000,  # must match what was used to generate baselines\n",
    "            n_clusters=5, \n",
    "            offset_window=(-10,40)\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        residuals_per_channel[ch] = residuals\n",
    "        cluster_ids_per_channel[ch] = cluster_ids\n",
    "        scale_factors_per_channel[ch] = scale_factors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factors_per_channel[179][matched_indices]\n",
    "scale_factors_per_channel[39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale =scale_factors_per_channel[125]\n",
    "\n",
    "print(scale[:10])\n",
    "\n",
    "\n",
    "[ 9091998 22675586 30549802 34674503]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ch = 179\n",
    "\n",
    "# Assume residuals_per_channel[126] is a 2D array of shape (n_samples, n_spikes)\n",
    "residuals = residuals_per_channel[ch]  # shape: (T, N), e.g., (81, 9000)\n",
    "\n",
    "# --- Define range to plot ---\n",
    "start = 0\n",
    "end = 1000  # or 1000-2000, etc.\n",
    "\n",
    "residuals_chunk = residuals[start:end, :]  # shape: (1000, 81)\n",
    "\n",
    "# Find index of channel 126 in selected_channels\n",
    "ch_idx = np.where(selected_channels == ch)[0][0]  # assumes channel is present\n",
    "# Slice desired spike range\n",
    "snips_chunk = snips_full[start:end, ch_idx, :]  # shape: (1000, 81)\n",
    "mean_waveform = snips_full[:, ch_idx, :].mean(axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "for i in range(residuals_chunk.shape[0]):\n",
    "    plt.plot(residuals_chunk[i], color='black', alpha=0.5, linewidth=0.5)\n",
    "\n",
    "plt.plot(mean_waveform, color='red', linewidth=2)\n",
    "plt.title(f\"Residuals from Channel {ch}, Spikes {start}-{end}\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "for i in range(snips_chunk.shape[0]):\n",
    "    plt.plot(snips_chunk[i], color='black', alpha=0.5, linewidth=0.5)\n",
    "plt.plot(mean_waveform, color='red', linewidth=2)\n",
    "plt.title(f\"Original from Channel {ch}, Spikes {start}-{end}\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_spikes = valid_spike_times[bad_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_data_snippets import extract_snippets\n",
    "\n",
    "snips_test, valid_spike_times_test = extract_snippets_channel_major(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    spike_times=bad_spikes,\n",
    "    selected_channels=np.arange(n_channels),\n",
    "    window=window,\n",
    "    total_samples=total_samples,\n",
    "    dtype=dtype\n",
    ")\n",
    "print(snips_test.shape)\n",
    "\n",
    "\n",
    "snips_orig = extract_snippets(\"/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat\", bad_spikes, window=(-20, 60), n_channels=512, dtype='int16')\n",
    "\n",
    "print(snips_orig.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_test = snips_orig[:, :, 0]\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei_test,\n",
    "    positions=ei_positions,\n",
    "    ref_channel = ref_channel,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,   # can adjust for visibility\n",
    "    box_width=50,\n",
    "    linewidth=1,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"Original\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ei_test = snips_test[:, :, 0]\n",
    "\n",
    "plt.figure(figsize=(15, 4))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei_test,\n",
    "    positions=ei_positions,\n",
    "    ref_channel = ref_channel,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,   # can adjust for visibility\n",
    "    box_width=50,\n",
    "    linewidth=1,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"Subtracted\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#[ 9091998 22675586 30549802 34674503]\n",
    "# --- Parameters ---\n",
    "dat_path_chmajor = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat'\n",
    "dtype = np.int16\n",
    "n_channels = 512\n",
    "channel_index = 179          # Channel to extract\n",
    "start_sample = 9091998-60            # Start time index (in samples)\n",
    "n_samples = 120            # Number of time samples to read\n",
    "\n",
    "# channel_index = 39          # Channel to extract\n",
    "# start_sample = 1700            # Start time index (in samples)\n",
    "# n_samples = 1500            # Number of time samples to read\n",
    "\n",
    "# --- Determine total timepoints ---\n",
    "file_size_bytes = np.memmap(dat_path_chmajor, dtype=dtype, mode='r').nbytes\n",
    "bytes_per_sample = np.dtype(dtype).itemsize\n",
    "total_samples_in_file = file_size_bytes // bytes_per_sample\n",
    "total_timepoints = total_samples_in_file // n_channels\n",
    "\n",
    "# --- Memory map full file with known shape ---\n",
    "data = np.memmap(dat_path_chmajor, dtype=dtype, mode='r', shape=(n_channels, total_timepoints))\n",
    "\n",
    "# --- Extract desired trace ---\n",
    "trace = data[channel_index, start_sample:start_sample + n_samples]\n",
    "\n",
    "# --- (Optional) Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(trace, color='black')\n",
    "plt.title(f\"Channel {channel_index}, Samples {start_sample} to {start_sample + n_samples}\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# --- Parameters ---\n",
    "channel_index = 179          # Channel to extract\n",
    "start_sample = 9091998-60            # Start time index (in samples)\n",
    "segment_length = 120            # Number of time samples to read\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat'\n",
    "\n",
    "\n",
    "# --- Step 1: Read unmodified segment ---\n",
    "segment_offset_bytes = start_sample * n_channels * np.dtype(dtype).itemsize\n",
    "segment_shape = (segment_length, n_channels)\n",
    "\n",
    "with open(dat_path, 'rb') as f:\n",
    "    f.seek(0, 2)\n",
    "    file_len_bytes = f.tell()\n",
    "n_total_samples = file_len_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "assert start_sample + segment_length <= n_total_samples, \"Segment overflows file.\"\n",
    "\n",
    "raw_mem = np.memmap(dat_path, dtype=dtype, mode='r', offset=segment_offset_bytes, shape=segment_shape)\n",
    "raw_trace_original = raw_mem[:, ref_channel].astype(np.float32).copy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(raw_trace_original, label=\"Original\", color='black')\n",
    "plt.xlabel(\"Global sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read from h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "unit_id = 0 # or whatever unit you're checking\n",
    "\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    group = h5[f'unit_{unit_id}']\n",
    "    \n",
    "    spike_times = group['spike_times'][:]\n",
    "    ei = group['ei'][:]\n",
    "    selected_channels = group['selected_channels'][:]\n",
    "    \n",
    "    peak_channel = group.attrs['peak_channel']\n",
    "\n",
    "# Check shapes or values\n",
    "#print(\"Spike times:\", spike_times.shape)\n",
    "#print(\"EI shape:\", ei.shape)\n",
    "print(\"Selected channels:\", selected_channels)\n",
    "print(179 in selected_channels)\n",
    "#print(\"Peak channel:\", peak_channel)\n",
    "#print(spike_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find matching spike pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "unit_id_1 = 3\n",
    "unit_id_2 = 5\n",
    "max_diff = 5  # in samples\n",
    "\n",
    "# --- Load spike times ---\n",
    "with h5py.File(h5_out_path, 'r') as h5:\n",
    "    spikes_1 = h5[f'unit_{unit_id_1}']['spike_times'][:]\n",
    "    spikes_2 = h5[f'unit_{unit_id_2}']['spike_times'][:]\n",
    "\n",
    "# --- Sort spike times (for efficiency) ---\n",
    "spikes_1 = np.sort(spikes_1)\n",
    "spikes_2 = np.sort(spikes_2)\n",
    "\n",
    "# --- Match spikes within max_diff ---\n",
    "matches = []\n",
    "j_start = 0\n",
    "\n",
    "for i, t1 in enumerate(spikes_1):\n",
    "    while j_start < len(spikes_2) and spikes_2[j_start] < t1 - max_diff:\n",
    "        j_start += 1\n",
    "\n",
    "    j = j_start\n",
    "    while j < len(spikes_2) and spikes_2[j] <= t1 + max_diff:\n",
    "        if abs(t1 - spikes_2[j]) <= max_diff:\n",
    "            matches.append((i, j))\n",
    "        j += 1\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"Found {len(matches)} matched spikes between unit {unit_id_1} and {unit_id_2}\")\n",
    "for i, j in matches:\n",
    "    print(f\"Spike1: {spikes_1[i]}, Spike2: {spikes_2[j]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(ei[179], color='black')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot EI from simple EI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_ei_waveforms(\n",
    "    ei=ei,\n",
    "    positions=ei_positions,\n",
    "    ref_channel=180,\n",
    "    scale=70.0,\n",
    "    box_height=1.0,   # can adjust for visibility\n",
    "    box_width=50,\n",
    "    linewidth=1,\n",
    "    alpha=0.9,\n",
    "    colors='black'\n",
    ")\n",
    "plt.title(\"Initial EI from Detected Spikes\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot EIs from multiple clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    ei = cluster['ei']\n",
    "    ref_ch = cluster['channels'][np.argmax(np.ptp(ei[cluster['channels'], :], axis=1))]\n",
    "    ei_p2p = np.ptp(ei[ref_ch, :])\n",
    "    n_spikes = len(cluster['inds'])\n",
    "\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plot_ei_waveforms(\n",
    "        ei=ei,\n",
    "        positions=ei_positions,\n",
    "        scale=70.0,\n",
    "        box_height=1.0,\n",
    "        box_width=50,\n",
    "        linewidth=1,\n",
    "        alpha=0.9,\n",
    "        colors='black'\n",
    "    )\n",
    "    plt.title(f\"Cluster {i} EI — Spikes: {n_spikes}, P2P on Ref Ch ({ref_ch}): {ei_p2p:.1f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and plot samples from channel-major file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Parameters ---\n",
    "dat_path_chmajor = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat'\n",
    "dtype = np.int16\n",
    "n_channels = 512\n",
    "channel_index = 39          # Channel to extract\n",
    "start_sample = 0            # Start time index (in samples)\n",
    "n_samples = 5000            # Number of time samples to read\n",
    "\n",
    "# --- Determine total timepoints ---\n",
    "file_size_bytes = np.memmap(dat_path_chmajor, dtype=dtype, mode='r').nbytes\n",
    "bytes_per_sample = np.dtype(dtype).itemsize\n",
    "total_samples = file_size_bytes // bytes_per_sample\n",
    "total_timepoints = total_samples // n_channels\n",
    "\n",
    "# --- Memory map full file with known shape ---\n",
    "data = np.memmap(dat_path_chmajor, dtype=dtype, mode='r', shape=(n_channels, total_timepoints))\n",
    "\n",
    "# --- Extract desired trace ---\n",
    "trace = data[channel_index, start_sample:start_sample + n_samples]\n",
    "\n",
    "# --- (Optional) Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(trace, color='black')\n",
    "plt.title(f\"Channel {channel_index}, Samples {start_sample} to {start_sample + n_samples}\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "window = (-30, 50)\n",
    "win_left, win_right = window\n",
    "n_channels = 512  # or however many you have\n",
    "\n",
    "data = np.memmap(dat_path_chmajor, dtype=np.int16, mode='r', shape=(n_channels, total_samples))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "win_left, win_right = window\n",
    "window_size = win_right - win_left\n",
    "ncols = 5\n",
    "snips_to_plot = [(ch, s) for ch, locs in artifact_locs.items() for s in locs]\n",
    "n = len(snips_to_plot)\n",
    "nrows = (n + ncols - 1) // ncols\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 4, nrows * 2))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ch, s) in enumerate(snips_to_plot):\n",
    "    ax = axes[i]\n",
    "    start = s + win_left\n",
    "    end = s + win_right\n",
    "    if start < 0 or end > data.shape[1]:\n",
    "        ax.set_title(f\"Channel {ch}, Sample {s} (OOB)\")\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "    snippet = data[ch, start:end]\n",
    "    ax.plot(np.arange(window_size), snippet)\n",
    "    ax.set_title(f\"Ch {ch}, Samp {s}\", fontsize=8)\n",
    "    ax.tick_params(labelsize=6)\n",
    "\n",
    "# Hide unused axes\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# --- Parameters ---\n",
    "ref_channel = 39\n",
    "start_sample = 0       # start of segment\n",
    "segment_length = 5000        # number of samples\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat'\n",
    "\n",
    "\n",
    "# --- Step 1: Read unmodified segment ---\n",
    "segment_offset_bytes = start_sample * n_channels * np.dtype(dtype).itemsize\n",
    "segment_shape = (segment_length, n_channels)\n",
    "\n",
    "with open(dat_path, 'rb') as f:\n",
    "    f.seek(0, 2)\n",
    "    file_len_bytes = f.tell()\n",
    "n_total_samples = file_len_bytes // (np.dtype(dtype).itemsize * n_channels)\n",
    "assert start_sample + segment_length <= n_total_samples, \"Segment overflows file.\"\n",
    "\n",
    "raw_mem = np.memmap(dat_path, dtype=dtype, mode='r', offset=segment_offset_bytes, shape=segment_shape)\n",
    "raw_trace_original = raw_mem[:, ref_channel].astype(np.float32).copy()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(raw_trace_original, label=\"Original\", color='black')\n",
    "plt.xlabel(\"Global sample index\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from axolotl_unit_loop_utils import find_dominant_channel\n",
    "\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat\"\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "\n",
    "ref_channel = find_dominant_channel(\n",
    "    dat_path=dat_path,\n",
    "    n_channels=n_channels,\n",
    "    dtype=dtype,          # optional if you're happy with 'int16'\n",
    "    segment_len=100000,   # 5s at 20kHz\n",
    "    n_segments=10,\n",
    "    peak_window=30, # look for post-peak maxima within ±30 samples\n",
    "    top_k_neg=20,   # find this many negative peaks per channel per segment\n",
    "    top_k_events=5, # use top K events per channel across all segments\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Dominant channel: {ref_channel}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from axolotl_unit_loop_utils import estimate_spike_threshold\n",
    "\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat'\n",
    "ref_channel = 39  # or use the result from find_dominant_channel\n",
    "dtype = 'int16'\n",
    "window = 30 \n",
    "\n",
    "threshold, spike_times = estimate_spike_threshold(\n",
    "    dat_path=dat_path,\n",
    "    ref_channel=ref_channel,\n",
    "    dtype=dtype,\n",
    "    window=window,\n",
    "    n_channels=512,\n",
    "    total_samples_to_read=10_000_000,\n",
    "    block_size=100_000,\n",
    "    refractory=30,\n",
    "    top_n=100\n",
    ")\n",
    "\n",
    "print(f\"Estimated threshold: {threshold:.2f}\")\n",
    "print(f\"Number of suprathreshold events: {len(spike_times)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from extract_data_snippets import extract_snippets\n",
    "from plot_ei_waveforms import plot_ei_waveforms\n",
    "import h5py\n",
    "\n",
    "h5_in_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_kilosort_data001_spike_times.h5'  # from MATLAB export\n",
    "\n",
    "with h5py.File(h5_in_path, 'r') as f:\n",
    "    # Load electrode positions\n",
    "    ei_positions = f['/ei_positions'][:].T  # shape becomes [512 x 2]\n",
    "\n",
    "# --- Parameters ---\n",
    "dat_path = \"/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat\"\n",
    "n_channels = 512\n",
    "dtype = 'int16'\n",
    "max_units = 500\n",
    "amplitude_threshold = 15\n",
    "window = (-20, 60)\n",
    "ei_window_len = window[1] - window[0] + 1\n",
    "peak_window = 30\n",
    "window = (-20, 60)\n",
    "\n",
    "snips = extract_snippets(dat_path, spike_times, window, n_channels, dtype)\n",
    "\n",
    "    # Step 5: Compute EI\n",
    "ei = np.mean(snips, axis=2)\n",
    "ei -= ei[:, :5].mean(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "ref_channel = 39\n",
    "clusters = axolotl_unit_loop_utils.cluster_spike_waveforms(snips, ei, k_start=3)\n",
    "ei, spikes, selected_channels = axolotl_unit_loop_utils.select_cluster_with_largest_waveform(clusters, ref_channel)\n",
    "\n",
    "len(spikes)\n",
    "spikes = spike_times[spikes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "\n",
    "spikes = axolotl_unit_loop_utils.ei_pursuit(\n",
    "    dat_path=dat_path,\n",
    "    spikes=spikes,                     # absolute sample times\n",
    "    ei_template=ei,                    # EI from selected cluster\n",
    "    dtype=dtype,\n",
    "    total_samples=10_000_000,          # or set to actual full recording length\n",
    "    save_prefix='/Volumes/Lab/Users/alexth/axolotl/ei_scan_unit0',  # set uniquely per unit\n",
    "    block_size=None,\n",
    "    baseline_start_sample=0,            # revisit for baseline subtraction!!! should be inside GPU\n",
    "    alignment_offset = -window[0],\n",
    "    fit_percentile = 40,                # how many (percentile) spikes to take to fit Gaussian for threshold determination (left-hand side of already found spikes)\n",
    "    sigma_thresh = 5.0                  # how many Gaussian sigmas to take for threshold\n",
    ")\n",
    "\n",
    "    \n",
    "print(f\"Detected {len(spikes)} spikes after EI pursuit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_data_snippets import extract_snippets\n",
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "    # Step 1: Extract full snippets from final spike times\n",
    "snips_full = extract_snippets(\n",
    "        dat_path=dat_path,\n",
    "        spike_times=spikes,          # these are real spike times (sample indices)\n",
    "        window=window,\n",
    "        n_channels=512,\n",
    "        dtype=dtype\n",
    ")\n",
    "\n",
    "    # Step 2: Recluster\n",
    "clusters = axolotl_unit_loop_utils.cluster_spike_waveforms(\n",
    "        snips=snips_full,\n",
    "        ei=ei,                       # from earlier template cluster\n",
    "        k_start=2                    # or however many clusters you want to try\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "ei, final_spike_inds, selected_channels = axolotl_unit_loop_utils.select_cluster_by_ei_similarity(\n",
    "    clusters=clusters,\n",
    "    reference_ei=ei,\n",
    "    similarity_threshold=0.9\n",
    ")\n",
    "\n",
    "spikes = spikes[final_spike_inds]  # convert to absolute spike times\n",
    "snips = snips_full[:, :, final_spike_inds]  # update snippet block\n",
    "\n",
    "print(spikes)\n",
    "print(len(spikes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, cl in enumerate(clusters):\n",
    "    print(f\"Cluster {i}: {len(cl['inds'])} spikes, EI shape = {cl['ei'].shape}, channels = {len(cl['channels'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "channel_idx = 39  # channel you want to process\n",
    "dtype = np.int16\n",
    "n_channels = 512\n",
    "window = (-20, 60)\n",
    "fit_offsets = (-5, 10)\n",
    "total_samples = 36000000  # update based on your .dat file\n",
    "dat_path_chmajor = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat'\n",
    "\n",
    "# Optional\n",
    "diagnostic_spikes = spikes[[0,1,5]] if len(spikes) > 2 else None\n",
    "\n",
    "# Run\n",
    "fit_params, residual_snippets, write_locs = axolotl_unit_loop_utils.run_template_subtraction_on_channel_accumulate(\n",
    "    channel_idx=channel_idx,\n",
    "    final_spike_times=spikes,\n",
    "    ei_waveform=ei[channel_idx],\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    total_samples=total_samples,\n",
    "    dtype=dtype,\n",
    "    window=window,\n",
    "    fit_offsets=fit_offsets,\n",
    "    max_chunk_len=100_000,  # 5 seconds @ 20 kHz\n",
    "    fallback_fit_params=None,  # or supply dict if desired\n",
    "    diagnostic_plot_spikes=diagnostic_spikes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example spike indices into residual_snippets\n",
    "plot_spike_indices = list(range(50))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "for idx in plot_spike_indices:\n",
    "    plt.plot(residual_snippets[idx], label=f\"Spike {idx}\", alpha=0.7)\n",
    "\n",
    "plt.title(f\"Residual snippets for channel {channel_idx}\")\n",
    "plt.xlabel(\"Sample index (relative to spike)\")\n",
    "plt.ylabel(\"Amplitude (int16)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "channel = 43  # replace with actual channel index\n",
    "start_sample = 1700\n",
    "end_sample = 3000\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# File paths\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat'\n",
    "sub_dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat'\n",
    "\n",
    "def read_channel_segment(path, ch, start, end, n_channels, dtype):\n",
    "    num_samples = end - start\n",
    "    offset = start * n_channels * np.dtype(dtype).itemsize\n",
    "    with open(path, 'rb') as f:\n",
    "        f.seek(offset)\n",
    "        raw = np.fromfile(f, dtype=dtype, count=num_samples * n_channels)\n",
    "    data = raw.reshape((num_samples, n_channels))\n",
    "    return data[:, ch]\n",
    "\n",
    "# Read both versions\n",
    "raw_segment = read_channel_segment(dat_path, channel, start_sample, end_sample, n_channels, dtype)\n",
    "sub_segment = read_channel_segment(sub_dat_path, channel, start_sample, end_sample, n_channels, dtype)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(raw_segment, label='Original', alpha=0.6)\n",
    "plt.plot(sub_segment, label='Subtracted', alpha=0.6)\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Raw value')\n",
    "plt.title(f'Channel {channel}, Samples {start_sample}–{end_sample}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters\n",
    "channel = 43  # replace with actual channel index\n",
    "start_sample = 1700\n",
    "end_sample = 3000\n",
    "n_channels = 512\n",
    "dtype = np.int16\n",
    "\n",
    "# File paths\n",
    "dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001.dat'\n",
    "sub_dat_path = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_sub.dat'\n",
    "\n",
    "def read_channel_segment(path, ch, start, end, n_channels, dtype):\n",
    "    num_samples = end - start\n",
    "    offset = start * n_channels * np.dtype(dtype).itemsize\n",
    "    with open(path, 'rb') as f:\n",
    "        f.seek(offset)\n",
    "        raw = np.fromfile(f, dtype=dtype, count=num_samples * n_channels)\n",
    "    data = raw.reshape((num_samples, n_channels))\n",
    "    return data[:, ch]\n",
    "\n",
    "# Read both versions\n",
    "raw_segment = read_channel_segment(dat_path, channel, start_sample, end_sample, n_channels, dtype)\n",
    "sub_segment = read_channel_segment(sub_dat_path, channel, start_sample, end_sample, n_channels, dtype)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(raw_segment, label='Original', alpha=0.6)\n",
    "plt.plot(sub_segment, label='Subtracted', alpha=0.6)\n",
    "plt.xlabel('Sample index')\n",
    "plt.ylabel('Raw value')\n",
    "plt.title(f'Channel {channel}, Samples {start_sample}–{end_sample}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "\n",
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "\n",
    "if 1:\n",
    "    # Step 11: accumulate edits (subtracted spike residuals)\n",
    "    # --- Setup ---\n",
    "    p2p_threshold = 15\n",
    "    ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "    selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "    selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "    tree = KDTree(ei_positions)\n",
    "\n",
    "    #spikes = spikes[:5]\n",
    "\n",
    "    # Precompute valid spikes (for fallback and range checks)\n",
    "    valid_spikes = [s for s in spikes if (s + window[0] >= 0) and (s + window[1] < total_samples)]\n",
    "    valid_spikes = np.array(valid_spikes)\n",
    "\n",
    "    #selected_channels = [39]\n",
    "\n",
    "    # Accumulators\n",
    "    all_channel_fit_params = {ch: None for ch in selected_channels}\n",
    "    all_residual_snips = {}\n",
    "    all_write_locs = {}\n",
    "\n",
    "    for ch_idx in selected_channels:\n",
    "\n",
    "        # Neighbor search\n",
    "        ch_coord = ei_positions[ch_idx]\n",
    "        neighbor_dists, neighbor_idxs = tree.query(ch_coord, k=6)\n",
    "        neighbor_idxs = neighbor_idxs[neighbor_dists > 0]\n",
    "\n",
    "        # Collect fallback parameters if available\n",
    "        valid_neighbors = [idx for idx in neighbor_idxs if all_channel_fit_params.get(idx) is not None]\n",
    "        if valid_neighbors:\n",
    "            fallback_params = {}\n",
    "            for i, spike_time in enumerate(valid_spikes):\n",
    "                per_spike_vals = [all_channel_fit_params[idx][i] for idx in valid_neighbors]\n",
    "                fallback_params[spike_time] = np.mean(per_spike_vals, axis=0)\n",
    "        else:\n",
    "            fallback_params = None\n",
    "\n",
    "        # Run subtraction\n",
    "        fit_params, residual_snippets, write_locs = axolotl_unit_loop_utils.run_template_subtraction_on_channel_accumulate(\n",
    "            channel_idx=ch_idx,\n",
    "            final_spike_times=spikes,\n",
    "            ei_waveform=ei[ch_idx],\n",
    "            dat_path_chmajor=dat_path_chmajor,\n",
    "            total_samples=total_samples,\n",
    "            dtype=dtype,\n",
    "            window=window,\n",
    "            fit_offsets=fit_offsets,\n",
    "            max_chunk_len=100_000,  # 5s @ 20kHz\n",
    "            fallback_fit_params=fallback_params,\n",
    "            diagnostic_plot_spikes=None\n",
    "        )\n",
    "        \n",
    "\n",
    "        # Store results\n",
    "        all_channel_fit_params[ch_idx] = fit_params\n",
    "        all_residual_snips[ch_idx] = residual_snippets\n",
    "        all_write_locs[ch_idx] = write_locs\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"Processed channel {ch_idx} with {len(spikes)} spikes in {elapsed:.1f} seconds.\")\n",
    "\n",
    "    print(\"Template subtraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import extract_snippets_channel_major\n",
    "import importlib\n",
    "importlib.reload(extract_snippets_channel_major)\n",
    "\n",
    "p2p_threshold = 25\n",
    "ei_p2p = ei.max(axis=1) - ei.min(axis=1)\n",
    "selected_channels = np.where(ei_p2p > p2p_threshold)[0]\n",
    "selected_channels = selected_channels[np.argsort(ei_p2p[selected_channels])[::-1]]\n",
    "print(len(selected_channels))\n",
    "\n",
    "snippets, valid_spike_times = extract_snippets_channel_major.extract_snippets_channel_major(\n",
    "    dat_path_chmajor='/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat',\n",
    "    spike_times=spikes,\n",
    "    selected_channels=selected_channels,\n",
    "    window=window,\n",
    "    total_samples=36_000_000,\n",
    "    dtype=dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = extract_snippets_channel_major.compute_channel_baselines(\n",
    "    dat_path_chmajor=dat_path_chmajor,\n",
    "    n_channels=512,\n",
    "    total_samples=36_000_000,\n",
    "    dtype=dtype,\n",
    "    segment_len=100_000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "\n",
    "# --- Setup ---\n",
    "residuals_per_channel = {}\n",
    "cluster_ids_per_channel = {}\n",
    "scale_factors_per_channel = {}\n",
    "\n",
    "for ch_idx, ch in enumerate(selected_channels):\n",
    "    # Slice data for this channel\n",
    "    ch_snips = snippets[:, ch_idx, :]  # shape: (n_spikes, snip_len)\n",
    "    ch_baselines = baselines[ch, :]    # shape: (n_segments,)\n",
    "\n",
    "    # Subtract PCA cluster means\n",
    "    residuals, cluster_ids, scale_factors = axolotl_unit_loop_utils.subtract_pca_cluster_means(\n",
    "        snippets=ch_snips,\n",
    "        baselines=ch_baselines,\n",
    "        spike_times=valid_spike_times,\n",
    "        segment_len=100_000,  # must match what was used to generate baselines\n",
    "        n_clusters=5\n",
    "    )\n",
    "\n",
    "    # Store results\n",
    "    residuals_per_channel[ch] = residuals\n",
    "    cluster_ids_per_channel[ch] = cluster_ids\n",
    "    scale_factors_per_channel[ch] = scale_factors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# --- Setup ---\n",
    "channel = 166\n",
    "segment_len = 100_000  # 5 seconds\n",
    "assert channel in selected_channels, f\"Channel {channel} not in selected_channels\"\n",
    "ch_idx = selected_channels.tolist().index(channel) # index into snippets array\n",
    "\n",
    "# --- Compute baseline index for each spike ---\n",
    "baseline_indices = valid_spike_times // segment_len  # shape: (9383,)\n",
    "channel_baselines = baselines[channel, baseline_indices]  # shape: (9383,)\n",
    "\n",
    "# --- Subtract and plot ---\n",
    "baseline_subtracted_snips = snippets[:, ch_idx, :] - channel_baselines[:, None]\n",
    "\n",
    "# Compute mean waveform\n",
    "mean_waveform = np.mean(baseline_subtracted_snips, axis=0)\n",
    "\n",
    "# Example: plot first 5\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(baseline_subtracted_snips[:500].T, linewidth=0.5)\n",
    "plt.plot(mean_waveform, color=\"black\", linewidth=3)\n",
    "plt.title(f\"Baseline-subtracted snippets for channel {channel}\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Voltage (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = mean_waveform  # ensure same preprocessing as in EI prep\n",
    "template_power = np.dot(template, template)  # scalar\n",
    "\n",
    "# Compute scaling factors (one per spike)\n",
    "scales = baseline_subtracted_snips @ template / template_power  # shape: (n_spikes,)\n",
    "\n",
    "# Optionally reconstruct scaled fits:\n",
    "scaled_snips = baseline_subtracted_snips / scales[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(scaled_snips[:500].T, linewidth=0.5)\n",
    "plt.plot(mean_waveform, color=\"black\", linewidth=3)\n",
    "plt.title(f\"Baseline-subtracted snippets for channel {channel}\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Voltage (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assumes the following are already defined:\n",
    "# - baseline_subtracted_snips: shape (n_spikes, snip_len)\n",
    "# - template: 1D array of shape (snip_len,)\n",
    "\n",
    "# Amplitude fits\n",
    "A_fits = np.sum(baseline_subtracted_snips * template, axis=1) / np.sum(template ** 2)\n",
    "\n",
    "# Unscaled residuals\n",
    "residuals_unscaled = baseline_subtracted_snips - template[None, :]\n",
    "\n",
    "# Scaled template per spike\n",
    "scaled_templates = A_fits[:, None] * template[None, :]\n",
    "residuals_scaled = baseline_subtracted_snips - scaled_templates\n",
    "\n",
    "# RMS per spike\n",
    "rms_unscaled = np.sqrt(np.mean(residuals_unscaled ** 2, axis=1))\n",
    "rms_scaled = np.sqrt(np.mean(residuals_scaled ** 2, axis=1))\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(rms_unscaled, bins=50, alpha=0.5, label='Unscaled Template Residual')\n",
    "plt.hist(rms_scaled, bins=50, alpha=0.5, label='Amplitude-Scaled Template Residual')\n",
    "plt.xlabel('RMS')\n",
    "plt.ylabel('Spike Count')\n",
    "plt.legend()\n",
    "plt.title('RMS Before vs. After Amplitude Scaling')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import correlate\n",
    "\n",
    "peak_sample = 20\n",
    "shift_range = (-5, 10)  # relative to peak\n",
    "\n",
    "# Define window for xcorr comparison\n",
    "xcorr_window = np.arange(peak_sample + shift_range[0], peak_sample + shift_range[1])\n",
    "template_seg = template[xcorr_window]\n",
    "template_seg = (template_seg - np.mean(template_seg)) / (np.std(template_seg) + 1e-9)\n",
    "\n",
    "n_spikes, snip_len = scaled_snips.shape\n",
    "aligned_snips = np.zeros_like(scaled_snips)\n",
    "shifts = np.zeros(n_spikes, dtype=int)\n",
    "\n",
    "for i in range(n_spikes):\n",
    "    snip = scaled_snips[i]\n",
    "    snip_seg = snip[xcorr_window]\n",
    "    snip_seg_norm = (snip_seg - np.mean(snip_seg)) / (np.std(snip_seg) + 1e-9)\n",
    "\n",
    "    # Compute cross-correlation only within the window\n",
    "    xcorr = correlate(snip_seg_norm, template_seg, mode='full')\n",
    "    center = len(xcorr) // 2\n",
    "    lag_range = np.arange(shift_range[0], shift_range[1])\n",
    "\n",
    "    # Get subset of xcorr within allowed lags\n",
    "    allowed_lags = lag_range[(center + lag_range >= 0) & (center + lag_range < len(xcorr))]\n",
    "    lag_values = xcorr[center + allowed_lags]\n",
    "\n",
    "    best_rel_shift = allowed_lags[np.argmax(lag_values)]\n",
    "    shifts[i] = best_rel_shift\n",
    "\n",
    "    # Apply shift to full snippet\n",
    "    if best_rel_shift < 0:\n",
    "        aligned_snips[i, :best_rel_shift] = snip[-best_rel_shift:]\n",
    "    elif best_rel_shift > 0:\n",
    "        aligned_snips[i, best_rel_shift:] = snip[:-best_rel_shift]\n",
    "    else:\n",
    "        aligned_snips[i] = snip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_scaled = baseline_subtracted_snips - scaled_templates\n",
    "residuals_shifted = baseline_subtracted_snips - aligned_snips\n",
    "# RMS per spike\n",
    "rms_unscaled = np.sqrt(np.mean(residuals_unscaled ** 2, axis=1))\n",
    "rms_scaled = np.sqrt(np.mean(residuals_scaled ** 2, axis=1))\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(rms_unscaled, bins=50, alpha=0.5, label='Unscaled Template Residual')\n",
    "plt.hist(rms_scaled, bins=50, alpha=0.5, label='Amplitude-Scaled Template Residual')\n",
    "plt.xlabel('RMS')\n",
    "plt.ylabel('Spike Count')\n",
    "plt.legend()\n",
    "plt.title('RMS Before vs. After Amplitude Scaling')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(aligned_snips[:500].T, linewidth=0.5)\n",
    "plt.plot(mean_waveform, color=\"black\", linewidth=3)\n",
    "plt.title(f\"Baseline-subtracted snippets for channel {channel}\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Voltage (a.u.)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Inputs:\n",
    "# - baseline_subtracted_snips: shape (n_spikes, snip_len)\n",
    "# - template: shape (snip_len,)\n",
    "# Assumes: template has already been baseline corrected\n",
    "\n",
    "# Step 1: amplitude scale each snippet\n",
    "template = template - np.mean(template[:5])\n",
    "A_fits = np.sum(baseline_subtracted_snips * template, axis=1) / np.sum(template ** 2)\n",
    "#scaled_snips = A_fits[:, None] * template[None, :]\n",
    "scaled_snips = baseline_subtracted_snips  # leave unscaled and cluster raw residual shape\n",
    "\n",
    "\n",
    "# Step 2: PCA\n",
    "pca = PCA(n_components=10)\n",
    "proj = pca.fit_transform(scaled_snips)\n",
    "\n",
    "n_clusters = 5\n",
    "# Step 3: KMeans clustering\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(proj)\n",
    "\n",
    "# Step 4: compute residuals from cluster means\n",
    "residuals = np.zeros_like(scaled_snips)\n",
    "rms_residuals = np.zeros(scaled_snips.shape[0])\n",
    "\n",
    "for k in range(n_clusters):\n",
    "    count = np.sum(cluster_labels == k)\n",
    "    print(f\"Cluster {k}: {count} spikes\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "for k in range(n_clusters):\n",
    "    cluster_mask = (cluster_labels == k)\n",
    "    cluster_snips = scaled_snips[cluster_mask]\n",
    "    if len(cluster_snips) == 0:\n",
    "        continue\n",
    "    cluster_mean = np.mean(cluster_snips, axis=0)\n",
    "        # Plot mean waveform for this cluster\n",
    "    plt.plot(cluster_mean, label=f'Cluster {k} (N={len(cluster_snips)})')\n",
    "    residuals[cluster_mask] = scaled_snips[cluster_mask] - cluster_mean\n",
    "    rms_residuals[cluster_mask] = np.sqrt(np.mean((scaled_snips[cluster_mask] - cluster_mean[None, :]) ** 2, axis=1))\n",
    "\n",
    "plt.title(\"Cluster Mean Templates\")\n",
    "plt.axvline(20, linestyle='--', color='gray', label='Peak')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(np.mean(rms_residuals))\n",
    "# Step 5: visualize RMS\n",
    "plt.hist(rms_residuals, bins=200, color='gray', alpha=0.5, label='PCA Template Residual')\n",
    "plt.hist(rms_scaled, bins=200, alpha=0.5, label='Amplitude-Scaled Template Residual')\n",
    "plt.xlabel('RMS of residual (post-clustering)')\n",
    "plt.ylabel('Spike count')\n",
    "plt.title('Cluster-based Template Residual RMS')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_recon = baseline_subtracted_snips - residuals\n",
    "\n",
    "i = 2  # or any other spike index\n",
    "\n",
    "k = cluster_labels[i]\n",
    "cluster_mask = (cluster_labels == k)\n",
    "cluster_snips = scaled_snips[cluster_mask]\n",
    "cluster_mean = np.mean(cluster_snips, axis=0)\n",
    "\n",
    "plt.plot(baseline_subtracted_snips[i,:], label='Original', color='black')\n",
    "plt.plot(template_recon[i,:], label='Template used', color='red')\n",
    "plt.plot(residuals[i,:], label='Residual', color='green')\n",
    "plt.plot(cluster_mean, label='template', color='blue')\n",
    "plt.plot(baseline_subtracted_snips[i,:]-cluster_mean, label='residual', color='gray')\n",
    "plt.axvline(20, linestyle='--', color='gray')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(f'Spike {i}, Channel {channel}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# --- Prep ---\n",
    "snip_len = baseline_subtracted_snips.shape[1]\n",
    "t_orig = np.arange(snip_len)\n",
    "template = template.astype(np.float32)\n",
    "\n",
    "# --- Define transforms ---\n",
    "warp_factors = [0.9, 0.95, 1, 1.05, 1.1]\n",
    "shifts = [-1, -0.5, 0, 0.5, 1]\n",
    "template_options = []\n",
    "\n",
    "for warp in warp_factors:\n",
    "    for shift in shifts:\n",
    "        t_warped = (t_orig - snip_len // 2 - shift) / warp + snip_len // 2\n",
    "        t_warped = np.clip(t_warped, 0, snip_len - 1)\n",
    "        interp_func = interp1d(t_orig, template, kind='cubic', bounds_error=False, fill_value=\"extrapolate\")\n",
    "        warped_template = interp_func(t_warped)\n",
    "        template_options.append(warped_template)\n",
    "\n",
    "template_options = np.stack(template_options)  # shape: (8, snip_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute residuals for each template option ---\n",
    "# baseline_subtracted_snips: (n_spikes, snip_len)\n",
    "# template_options: (8, snip_len)\n",
    "\n",
    "precom_residuals = baseline_subtracted_snips[:, None, :] - template_options[None, :, :]\n",
    "rms_errors = np.sqrt(np.mean(precom_residuals**2, axis=-1))  # (n_spikes, 8)\n",
    "best_idx = np.argmin(rms_errors, axis=1)\n",
    "best_residuals = precom_residuals[np.arange(len(baseline_subtracted_snips)), best_idx, :]\n",
    "\n",
    "print(\"Residuals shape:\", best_residuals.shape)  # (n_spikes, snip_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "template_recon = baseline_subtracted_snips - residuals\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    rms = np.sqrt(np.mean(best_residuals[i] ** 2))\n",
    "    k = cluster_labels[i]\n",
    "    cluster_mask = (cluster_labels == k)\n",
    "    cluster_snips = scaled_snips[cluster_mask]\n",
    "    cluster_mean = np.mean(cluster_snips, axis=0)\n",
    "    pca_res = baseline_subtracted_snips[i,:]-cluster_mean\n",
    "    rms_pca = np.sqrt(np.mean(pca_res ** 2))\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    plt.plot(baseline_subtracted_snips[i], label='Original', color='black')\n",
    "    plt.plot(template_options[best_idx[i]], label='Precomp', color='red')\n",
    "    plt.plot(best_residuals[i], label=f'Precom Residual (RMS={rms:.2f})', color='green')\n",
    "    plt.plot(cluster_mean, label='PCA template', color='blue')\n",
    "    plt.plot(pca_res, label=f'PCA residual (RMS={rms_pca:.2f})', color='gray')\n",
    "    plt.title(f\"Spike {i}, Best template #{best_idx[i]}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get channel index in the output array (not global index)\n",
    "channel_i = 0  # if it's the first in selected_channels\n",
    "mean_waveform = snippets[:, channel_i, :].mean(axis=0)\n",
    "\n",
    "plt.plot(mean_waveform)\n",
    "plt.title(f\"Mean waveform for selected channel {selected_channels[channel_i]}\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get channel index in the output array (not global index)\n",
    "channel_i = 0  # if it's the first in selected_channels\n",
    "\n",
    "for i in range(8000,8010):\n",
    "    plt.plot(snippets[i, channel_i, :], label=f\"Spike {i}\")\n",
    "\n",
    "plt.plot(mean_waveform, color=\"black\", label=\"Mean\")\n",
    "plt.title(f\"Waveforms for channel {selected_channels[channel_i]}\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Choose the channel and spikes to visualize ---\n",
    "channel_to_plot = 39\n",
    "spike_indices = [0, 1, 2]  # Replace with desired spike indices\n",
    "\n",
    "# --- Extract residuals ---\n",
    "residuals = all_residual_snips[channel_to_plot]  # shape: (n_spikes, snip_len)\n",
    "\n",
    "print(all_write_locs[channel_to_plot])\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in spike_indices:\n",
    "    plt.plot(residuals[i], alpha=0.7, label=f\"Spike {i}\")\n",
    "plt.axhline(0, linestyle='--', color='gray')\n",
    "plt.title(f\"Residual waveforms on channel {channel_to_plot}\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "plt.ylabel(\"Amplitude (int16)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(residuals.dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_write_locs = spikes + window[0]\n",
    "print(all_write_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import axolotl_unit_loop_utils\n",
    "import importlib\n",
    "importlib.reload(axolotl_unit_loop_utils)\n",
    "\n",
    "    # Step 12: edit raw data\n",
    "write_locs = spikes + window[0]\n",
    "if 1:\n",
    "    axolotl_unit_loop_utils.apply_residuals_to_channel_major(\n",
    "        dat_path_chmajor=dat_path_chmajor,\n",
    "        residual_snips_per_channel=residuals_per_channel,\n",
    "        write_locs=write_locs,\n",
    "        selected_channels=selected_channels,\n",
    "        total_samples=total_samples,\n",
    "        dtype=dtype,\n",
    "        n_channels=n_channels\n",
    "    )\n",
    "\n",
    "    with open(dat_path_chmajor, 'rb') as f:\n",
    "        offset = (39 * total_samples + write_locs[0]) * np.dtype(np.int16).itemsize\n",
    "        f.seek(offset)\n",
    "        post_edit_snip = np.fromfile(f, dtype=np.int16, count=81)\n",
    "\n",
    "    plt.plot(post_edit_snip)\n",
    "    plt.title(\"Post-subtraction, from disk\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #end_time = time.time()\n",
    "    #elapsed = end_time - start_time\n",
    "    #print(f\"Processed unit {unit_id} with {len(spikes)} spikes in {elapsed:.1f} seconds.\")\n",
    "\n",
    "    # # Step 12: Repeat until done\n",
    "    # unit_id += 1\n",
    "    # if unit_id >= max_units:\n",
    "    #     print(\"Reached unit limit.\")\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = 39\n",
    "n_to_plot = 15\n",
    "\n",
    "# Extract the residuals for this channel\n",
    "residuals = residuals_per_channel[channel]\n",
    "\n",
    "# Plot the first few\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(min(n_to_plot, residuals.shape[0])):\n",
    "    plt.plot(residuals[i], label=f\"Spike {i}\", alpha=0.6)\n",
    "\n",
    "plt.title(f\"First {n_to_plot} residuals for channel {channel}\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# --- Parameters ---\n",
    "dat_path_chmajor = '/Volumes/Lab/Users/alexth/axolotl/201703151_data001_chmajor.dat'\n",
    "dtype = np.int16\n",
    "n_channels = 512\n",
    "channel_index = 39          # Channel to extract\n",
    "start_sample = 1700            # Start time index (in samples)\n",
    "n_samples = 1200            # Number of time samples to read\n",
    "\n",
    "# --- Determine total timepoints ---\n",
    "file_size_bytes = np.memmap(dat_path_chmajor, dtype=dtype, mode='r').nbytes\n",
    "bytes_per_sample = np.dtype(dtype).itemsize\n",
    "total_samples = file_size_bytes // bytes_per_sample\n",
    "total_timepoints = total_samples // n_channels\n",
    "\n",
    "# --- Memory map full file with known shape ---\n",
    "data = np.memmap(dat_path_chmajor, dtype=dtype, mode='r', shape=(n_channels, total_timepoints))\n",
    "\n",
    "# --- Extract desired trace ---\n",
    "trace = data[channel_index, start_sample:start_sample + n_samples]\n",
    "\n",
    "# --- (Optional) Plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(trace, color='black')\n",
    "plt.title(f\"Channel {channel_index}, Samples {start_sample} to {start_sample + n_samples}\")\n",
    "plt.grid(True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencoder_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
